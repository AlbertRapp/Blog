geom_curve(
data = slice(tib, 1),
col = 'dodgerblue4',
size = tib$size_col,
curvature = -0.3
)
ggplot(mapping = aes(x = x, xend = xend, y = y, yend = yend)) +
geom_curve(
data = slice(tib, 1),
col = 'dodgerblue4',
size = tib$size_col[1],
curvature = -0.3
)
ggplot(mapping = aes(x = x, xend = xend, y = y, yend = yend)) +
geom_curve(
data = slice(tib, 1),
col = 'dodgerblue4',
size = tib$size_col[1],
curvature = -0.3
) +
geom_curve(
data = slice(tib, 2),
col = 'dodgerblue4',
size = tib$size_col[2],
curvature = 0.6
)
tib <- tib %>% mutate(curvature = c(-0.3, 0.6))
tib
tib %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6
))
ggplot() +
curve_layers[[1]]
curve_layers <- tib %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6
))
curve_layers
ggplot() +
curve_layers[[1]]
curve_layers <- tib %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6,
col = 'dodgerblue4'
))
curve_layers
ggplot() +
curve_layers[[1]]
ggplot() +
curve_layers[1] +
curve_layers[[2]]
ggplot() +
curve_layers[1] +
curve_layers[2]
ggplot() +
curve_layers
ggplot() +
curve_layers
ggplot() +
curve_layers +
annotate('point', x = 1, y = 1)
ggplot() +
curve_layers
n_curves <- 10
curve_layers <- tibble(
x = runif(N),
y = runif(N),
xend = runif(N),
yend = runif(N),
size = runif(N, 0, 2),
curvature = runif(N, -1, 1)
) %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6,
col = 'dodgerblue4'
))
ggplot() + curve_layers
n_curves <- 10
curve_layers <- tibble(
x = runif(n_curves),
y = runif(n_curves),
xend = runif(n_curves),
yend = runif(n_curves),
size = runif(n_curves, 0, 2),
curvature = runif(n_curves, -1, 1)
) %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6,
col = 'dodgerblue4'
))
ggplot() + curve_layers
n_curves <- 25
curve_layers <- tibble(
x = runif(n_curves),
y = runif(n_curves),
xend = runif(n_curves),
yend = runif(n_curves),
size = runif(n_curves, 0, 2),
curvature = runif(n_curves, -1, 1)
) %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6,
col = 'dodgerblue4'
))
ggplot() + curve_layers
n_curves <- 50
curve_layers <- tibble(
x = runif(n_curves),
y = runif(n_curves),
xend = runif(n_curves),
yend = runif(n_curves),
size = runif(n_curves, 0, 2),
curvature = runif(n_curves, -1, 1)
) %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6,
col = 'dodgerblue4'
))
ggplot() + curve_layers
combine_gg_elements(ggplot(), curve_layers)
combine_gg_elements <- function(...) {
Reduce(`+`, list(...))
}
combine_gg_elements(ggplot(), curve_layers)
# Enter your code in this chunk.
# Never overwrite the first line (here {r "1_a"}) of a chunk
123 + 257
options(repos = c(skranz = 'https://skranz.r-universe.dev',
CRAN = 'https://cloud.r-project.org'))
install.packages("RTutor")
hint()
y <- 8
x * y
stats()
r <- sqrt(123456789)
stats()
stats()
awards()
(0.1+0.2-0.3)*10000000000000000000
c(4,2,73)
# Chunk 1
knitr::opts_chunk$set(collapse = T)
Sys.setenv(lang = 'EN')
# Chunk 2
library(tidyverse)
theme_set(theme_minimal())
tib <- tribble(
~x, ~xend, ~y, ~yend, ~size_col,
0, 1, 0, 1, 1,
1, 2, 1, 1, 5
)
tib %>%
ggplot(aes(x = x, xend = xend, y = y, yend = yend, size = size_col)) +
geom_segment(col = 'dodgerblue4') +
scale_size_identity()
# Chunk 4
tib %>%
ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +
geom_curve(col = 'dodgerblue4', size = tib$size_col, curvature = 0.6)
# Chunk 5
tib %>%
ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +
geom_curve(
col = 'dodgerblue4',
size = tib$size_col,
curvature = c(-0.3, 0.6) # two curves, two different curvatures
)
# Chunk 6
tib %>%
ggplot(aes(x = x, xend = xend, y = y, yend = yend)) +
geom_curve(
aes(curvature = c(-0.3, 0.6)),
col = 'dodgerblue4',
size = tib$size_col
)
# Chunk 7
ggplot(mapping = aes(x = x, xend = xend, y = y, yend = yend)) +
geom_curve(
data = slice(tib, 1), # first row of tib
col = 'dodgerblue4',
size = tib$size_col[1], # one size only
curvature = -0.3
) +
geom_curve(
data = slice(tib, 2), # second row of tib
col = 'dodgerblue4',
size = tib$size_col[2], # other size
curvature = 0.6
)
# Chunk 8
tib <- tib %>% mutate(curvature = c(-0.3, 0.6))
tib
# Chunk 9
curve_layers <- tib %>%
pmap(~geom_curve(
mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4),
size = ..5,
curvature = ..6,
col = 'dodgerblue4'
))
curve_layers
# Chunk 10
ggplot() +
curve_layers
# Chunk 11
n_curves <- 50
curve_layers <- tibble(
x = runif(n_curves),
y = runif(n_curves),
xend = runif(n_curves),
yend = runif(n_curves),
size = runif(n_curves, 0, 2),
curvature = runif(n_curves, -1, 1)
) %>%
pmap(~geom_curve(
mapping = aes(x = x, xend = xend, y = y, yend = yend),
size = szoe,
curvature = curvature,
col = 'dodgerblue4'
))
ggplot() + curve_layers
# Chunk 12
combine_gg_elements <- function(...) {
Reduce(`+`, list(...))
}
combine_gg_elements(ggplot(), curve_layers)
blogdown::serve_site()
blogdown::new_post('Recreating-the-SWD-look', ext = '.Rmarkdown')
library(tidyverse)
dat <- tibble(
id = 1:19,
fulfilled = c(803, 865, 795, 683, 566, 586, 510, 436, 418, 364, 379, 372, 374, 278, 286, 327, 225, 222, 200),
accuracy = c(86, 80, 84, 82, 86, 80, 80, 93, 88, 87, 85, 85, 83, 94, 86, 78, 89, 88, 91),
error = c(10, 14, 10, 14, 10, 16, 15, 6, 11, 7, 12, 13, 8, 4, 12, 12, 7, 10, 7),
null = 100 - accuracy - error
) %>%
mutate(across(accuracy:null, ~. / 100))
dat
library(tidyverse)
dat <- tibble(
id = 1:19,
fulfilled = c(803, 865, 795, 683, 566, 586, 510, 436, 418, 364, 379, 372, 374, 278, 286, 327, 225, 222, 200),
accuracy = c(86, 80, 84, 82, 86, 80, 80, 93, 88, 87, 85, 85, 83, 94, 86, 78, 89, 88, 91),
error = c(10, 14, 10, 14, 10, 16, 15, 6, 11, 7, 12, 13, 8, 4, 12, 12, 7, 10, 7),
null = 100 - accuracy - error
) %>%
mutate(across(accuracy:null, ~. / 100))
dat
dat_long <- dat %>%
pivot_longer(
cols = accuracy:null,
names_to = 'type',
values_to = 'percent'
)
dat_long %>%
ggplot(aes(id, percent, fill = factor(type, levels = c('null', 'accuracy', 'error')))) +
geom_col() +
labs(
title = 'Warehouse Accuracy Rates',
x = 'Warehouse ID',
y = '% of total orders',
fill = element_blank()
) +
scale_y_continuous(labels = ~scales::percent(., accuracy = 1), breaks = seq(0, 1, 0.1))
blogdown:::preview_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::new
blogdown::new_post('get-twitter-posts-into-your-notetaking-system', ext = '.qmd')
blogdown::serve_site()
blogdown::serve_site()
bearer_token <- keyring::key_get('twitter-bearer-token')
user_mail <- keyring::key_get('dataviz-mail')
password_mail <- keyring::key_get('dataviz-mail-password')
allowed_senders <- keyring::key_get('allowed-senders')
allowed_senders <- keyring::key_get('allowed_senders')
library(httr) # for API communication
blogdown::serve_site()
library(httr) # for API communication
# Chunk 1
knitr::opts_chunk$set(collapse = T)
# Chunk 2
library(purrr) # for pluck and map functions
library(readr) # for reading and writing files from/to disk
library(tibble) # for easier readable tribble creation
# Chunk 3
#| echo: false
#| fig-cap: 'Click on detailed features to apply for higher access'
knitr::include_graphics('project-elevated.PNG')
# Chunk 4
bearer_token <- keyring::key_get('twitter-bearer-token')
user_mail <- keyring::key_get('dataviz-mail')
password_mail <- keyring::key_get('dataviz-mail-password')
allowed_senders <- keyring::key_get('allowed_senders')
# Chunk 5
library(stringr) # for regex matching
library(dplyr) # for binding rows and pipe
tweet_url <- 'https://twitter.com/c_gebhard/status/1510533315262042112'
tweet_id <- tweet_url %>% str_match("status/([0-9]+)") %>% .[, 2]
API_url <- 'https://api.twitter.com/2/tweets'
# Chunk 6
library(httr) # for API communication
auth <- paste("Bearer", bearer_token) # API needs format "Bearer <my_token>"
# Make request to API and parse to list
request <- GET(
API_url,
add_headers(Authorization = auth),
query = list(
ids = tweet_id,
tweet.fields = 'created_at', # time stamp
expansions = 'attachments.media_keys,author_id',
# necessary expansion fields for img_url
media.fields = 'url' # img_url
)
)
request
# Chunk 7
GET("http://google.com/", path = "search", query = list(q = "ham"))
GET("http://httpbin.org/get", add_headers(a = 1, b = 2))
# Chunk 8
#| eval: false
auth <- paste("Bearer", bearer_token) # API needs format "Bearer <my_token>"
# Make request to API and parse to list
request <- GET(
API_url,
add_headers(Authorization = auth),
query = list(
ids = tweet_id,
tweet.fields = 'created_at', # time stamp
expansions = 'attachments.media_keys,author_id',
# necessary expansion fields for img_url
media.fields = 'url' # img_url
)
)
# Chunk 9
parsed_request <- request %>% content('parsed')
parsed_request
# Chunk 10
# Extract necassary information from list-like structure
tweet_text <- parsed_request %>%
pluck("data", 1, 'text')
tweet_text
tweet_user <-  parsed_request %>%
pluck("includes", 'users', 1, 'username')
tweet_user
# We will use the tweet date and time as part of unique file names
# Replace white spaces and colons (:) for proper file names
tweet_date <- parsed_request %>%
pluck("data", 1, 'created_at') %>%
lubridate::as_datetime() %>%
str_replace(' ', '_') %>%
str_replace_all(':', '')
tweet_date
img_urls <- parsed_request %>%
pluck("includes", 'media') %>%
bind_rows() %>% # bind_rows for multiple pictures, i.e. multiple URLS
filter(type == 'photo') %>%
pull(url)
img_urls
# Chunk 11
# Download image - set mode otherwise download is blurry
img_names <- paste('tweet', tweet_user, tweet_date, seq_along(img_urls), sep = "_")
walk2(img_urls, img_names, ~download.file(.x, paste0(.y, '.png'), mode = 'wb'))
# Chunk 12
request_twitter_data <- function(tweet_url, bearer_token) {
# Extract tweet id by regex
tweet_id <- tweet_url %>% str_match("status/([0-9]+)") %>% .[, 2]
auth <- paste("Bearer", bearer_token) # API needs format "Bearer <my_token>"
API_url <- 'https://api.twitter.com/2/tweets'
# Make request to API and parse to list
parsed_request <- GET(
API_url,
add_headers(Authorization = auth),
query = list(
ids = tweet_id,
tweet.fields = 'created_at', # time stamp
expansions='attachments.media_keys,author_id',
# necessary expansion fields for img_url
media.fields = 'url' # img_url
)
) %>% content('parsed')
# Extract necassary information from list-like structure
tweet_text <- parsed_request %>%
pluck("data", 1, 'text')
tweet_user <-  parsed_request %>%
pluck("includes", 'users', 1, 'username')
# Make file name unique through time-date combination
# Replace white spaces and colons (:) for proper file names
tweet_date <- parsed_request %>%
pluck("data", 1, 'created_at') %>%
lubridate::as_datetime() %>%
str_replace(' ', '_') %>%
str_replace_all(':', '')
img_urls <- parsed_request %>%
pluck("includes", 'media') %>%
bind_rows() %>%
filter(type == 'photo') %>%
pull(url)
# Download image - set mode otherwise download is blurry
img_names <- paste('tweet', tweet_user, tweet_date, seq_along(img_urls), sep = "_")
walk2(img_urls, img_names, ~download.file(.x, paste0(.y, '.png'), mode = 'wb'))
# Return list with information
list(
url = tweet_url,
text = tweet_text,
user = tweet_user,
file_names = paste0(img_names, '.png'),
file_paths = paste0(getwd(), '/', img_names, '.png')
)
}
# Chunk 13
request <- request_twitter_data(tweet_url, bearer_token)
# Chunk 14
cat(readr::read_file('template.md'))
# Chunk 15
md_import_strings <- function(file_names) {
paste0('![[', file_names, ']]', collapse = '\n')
}
# Chunk 16
# Replace the placeholders in the template
# We change original mail place holder later on
replace_template_placeholder <- function(template_name, request) {
# Create a dictionary for what to replace in template
replace_dict <- tribble(
~template, ~replacement,
'\\!\\[\\[insert_img_name_here\\]\\]', md_import_strings(request$file_names),
'insert_text_here', request$text %>% str_replace_all('#', '(#)'),
'insert_URL_here', request$url
)
# Iteratively apply str_replace_all and keep only final result
reduce2(
.x = replace_dict$template,
.y = replace_dict$replacement,
.f = str_replace_all,
.init =  read_lines(template_name)
) %>%
paste0(collapse = '\n') # Collaps lines into a single string
}
replace_template_placeholder('template.md', request) %>% cat()
# Chunk 17
write_replaced_text <- function(replaced_text, request) {
file_name <- request$file_name[1] %>% str_replace('_1.png', '.md')
write_lines(replaced_text, file_name)
paste0(getwd(), '/', file_name)
}
replaced_template <- replace_template_placeholder('template.md', request) %>%
write_replaced_text(request)
# Chunk 18
move_files <- function(request, replaced_template, vault_location, attachments_dir) {
# Create from-to dictionary with file paths in each column
move_dict <- tribble(
~from, ~to,
request$file_path, paste0(vault_location, '/', attachments_dir),
replaced_template, vault_location
) %>%
unnest(cols = 'from')
# Copy files from current working directory to destination
move_dict %>% pwalk(file.copy, overwrite = T)
# Delete files in current working directory
walk(move_dict$from, file.remove)
}
# Chunk 19
library(mRpostman) # for email communication
imap_mail <- 'imaps://imap.gmail.com' # mail client
# Establish connection to imap server
con <- configure_imap(
url = imap_mail,
user = user_mail,
password = password_mail
)
# Switch to Inbox
con$select_folder('Inbox')
# Extract mails that are from the list of allowed senders
mails <- allowed_senders %>%
map(~con$search_string(expr = ., where = 'FROM')) %>%
unlist() %>%
na.omit() %>% # Remove NAs if no mail from a sender
as.numeric() # avoids attributes
# Chunk 20
if (!is_empty(mails)) mail_bodys <- mails %>% con$fetch_text()
mail_bodys %>% map(cat)
mail_bodys
mail_bodys %>% map(cat)
mail_bodys
cat(mail_bodys[[1]])
cat(mail_bodys[[2]])
blogdown::serve_site()
blogdown::serve_site()
blogdown::serve_site()
blogdown::new_post('lessons-learned-from-teaching-nonprogrammers', ext = '.qmd')
library(tidyverse)
cars
subset(cars, x)
subset(cars, speed)
subset(cars, 'speed')
subset(cars, speed > 50)
subset(cars, speed > 20)
