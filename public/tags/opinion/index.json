[{"content":"A couple of weeks back, I recreated an info graphic with ggplot2. The result and the whole story is embedded in this thread on Twitter:\nThe fun thing about getting better at #ggplot2 is that you begin to mimick other #dataviz.\nHere is a practice #rstats info graphic I created after seeing a similar infographic from @EatSmarter_de Original graphic, making of, comments and some ressources below ‚¨áÔ∏èüßµ pic.twitter.com/FslScy9sc7\n\u0026mdash; Albert Rapp (@rappa753) March 5, 2022  Aside from the embarrasing typo in \u0026ldquo;What you should know\u0026hellip;\u0026rdquo;, I picked up a useful technique for what do when I want aesthetics to vary within a geom. Sounds complicated? Let\u0026rsquo;s take a look at a couple of examples.\nHow do I manually set aesthetics with aes() and scale_*_identity()? This one is the easy case when all geoms behave properly.\n1 2 3 4 5 6 7 8 9 10 11 12  library(tidyverse) theme_set(theme_minimal()) tib \u0026lt;- tribble( ~x, ~xend, ~y, ~yend, ~size_col, 0, 1, 0, 1, 1, 1, 2, 1, 1, 5 ) tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend, size = size_col)) + geom_segment(col = \u0026#39;dodgerblue4\u0026#39;) + scale_size_identity()   Notice that\n the sizes were determined in the size_col column of tib. sizes were mapped to the aesthethic via aes(). the scale_size_identity() layer makes sure that the sizes are not assigned by ggplot but taken as given (identity scale layers are available for other aesthetics as well).  How do I manually set aesthetics without aes()? The last example used aes() to access size_col from tib. However, we then had to make sure that ggplot does not assign sizes based on unique values in size_col. Instead, sizes were supposed to be taken as is. This was the job of scale_size_identity(). Let\u0026rsquo;s make it work without it.\n1 2 3  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_segment(col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col)   This will generate the exact same plot as before (which is why I suppressed the output). In this case, we mapped the sizes manually by assigning a vector of sizes to the size aesthetic within geom_segment() but outside aes().\nOf course, now we cannot simply write size = size_col because geom_segment() won\u0026rsquo;t know that variable. Before, aes() let ggplot know that we mean size_col from the data set tib. Now, we have to pass the vector by accessing it from tib ourself through tib$size_col.\nHow do I manually set aesthethics when the previous approaches do not work? Finally, let\u0026rsquo;s switch from geom_segment() to geom_curve().\n1 2 3  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve(col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col, curvature = 0.6)   This changes our straight lines from before to curved lines. What\u0026rsquo;s more, I can control how strong the curvature is supposed to be via curvature. But as it is right now, both of our differently-sized curves have the same level of curvature.\nMaybe, this ought to be different. Maybe, not all curves are made the same. Maybe, our visualization should reflect the diversity of all the curves out there in this gigantic world we inhabit. All curves are beautiful!\nLet\u0026rsquo;s make this happen as we did before.\n1 2 3 4 5 6 7 8 9 10 11 12  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve( col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col, curvature = c(-0.3, 0.6) # two curves, two different curvatures ) ## Warning in if (curvature == 0) {: the condition has length \u0026gt; 1 and only the ## first element will be used ## Warning in if (curvature \u0026gt; 0) hand \u0026lt;- \u0026#34;right\u0026#34; else hand \u0026lt;- \u0026#34;left\u0026#34;: the condition ## has length \u0026gt; 1 and only the first element will be used ## Error in seq.default(0, dir * maxtheta, dir * maxtheta/(ncp + 1)): \u0026#39;to\u0026#39; must be of length 1   Oh no! It seems as if geom_curve() expects the argument of curvature to be a single number. Maybe aes() then?\n1 2 3 4 5 6 7 8  tib %\u0026gt;% ggplot(aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve( aes(curvature = c(-0.3, 0.6)), col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col ) ## Warning: Ignoring unknown aesthetics: curvature   Well, at least this time we can see curves. Unfortunately, the warning let\u0026rsquo;s us know that curvature is an unknown aesthetic which will be ignored. As you can see, this results in the same curvature for both curves again.\nSo, it looks like we can only hope to set each curvature separately.\n1 2 3 4 5 6 7 8 9 10 11 12 13  ggplot(mapping = aes(x = x, xend = xend, y = y, yend = yend)) + geom_curve( data = slice(tib, 1), # first row of tib col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col[1], # one size only curvature = -0.3 ) + geom_curve( data = slice(tib, 2), # second row of tib col = \u0026#39;dodgerblue4\u0026#39;, size = tib$size_col[2], # other size curvature = 0.6 )   Alright, this time we got what we wanted. That\u0026rsquo;s something at least. Honestly, our \u0026ldquo;solution\u0026rdquo; is not scalable though. What if we want to draw hundreds of curves?\nIn fact, this is what slowed me down when I created the info graphic that started this blog post. The text boxes were not vectorized so I would have to place each text box manually. That\u0026rsquo;s a lot of text boxes and I was having none of that.\nSo, here is where functional programming stepped in. Let\u0026rsquo;s recreate what I did based on our curve example. First, we extend tib with another curvature column.\n1 2 3 4 5 6 7  tib \u0026lt;- tib %\u0026gt;% mutate(curvature = c(-0.3, 0.6)) tib ## # A tibble: 2 x 6 ## x xend y yend size_col curvature ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0 1 0 1 1 -0.3 ## 2 1 2 1 1 5 0.6   Then, we use pmap() to create a list of curve layers. If you have not used any functional programming before, checkout my YARDS lecture notes on that topic. Basically, what we will do is to apply the geom_curve() function to each row of the tib data. Via ~ (in front of the function) and ..1, ..2, etc. we can then say where to stick in the values from each of tib\u0026rsquo;s columns.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  curve_layers \u0026lt;- tib %\u0026gt;% pmap(~geom_curve( mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4), size = ..5, curvature = ..6, col = \u0026#39;dodgerblue4\u0026#39; )) curve_layers ## [[1]] ## mapping: x = 0, y = 0, xend = 1, yend = 1  ## geom_curve: arrow = NULL, arrow.fill = NULL, curvature = -0.3, angle = 90, ncp = 5, lineend = butt, na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity  ##  ## [[2]] ## mapping: x = 1, y = 1, xend = 2, yend = 1  ## geom_curve: arrow = NULL, arrow.fill = NULL, curvature = 0.6, angle = 90, ncp = 5, lineend = butt, na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity   Here, we have set the first column of tib (x) to the x-aesthetic within aes. Then, we proceeded similarly for all other columns. This resulted in a list of curve layers.\nThese are useless without a ggplot() head. So, let\u0026rsquo;s complete the plot.\n1 2  ggplot() + curve_layers   Damn, these are some nice functionally created curves. Now, let\u0026rsquo;s put our new technique to a test. Can it handle arbitrarily many curves?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  n_curves \u0026lt;- 50 curve_layers \u0026lt;- tibble( x = runif(n_curves), xend = runif(n_curves), y = runif(n_curves), yend = runif(n_curves), size = runif(n_curves, 0, 2), curvature = runif(n_curves, -1, 1) ) %\u0026gt;% pmap(~geom_curve( mapping = aes(x = ..1, xend = ..2, y = ..3, yend = ..4), size = ..5, curvature = ..6, col = \u0026#39;dodgerblue4\u0026#39; )) ggplot() + curve_layers   Congratulations! We have successfully created drawings of a toddler. And the even better news is that we can draw as many curves as we want.\nSurprisingly, before I started this blog post, I was not aware that you can simply add lists to ggplot() and it works. As you will see in the Twitter thread on top of this post, I initially thought that one had to combine the list with more functional programming like so.\n1 2 3 4  combine_gg_elements \u0026lt;- function(...) { Reduce(`+`, list(...)) } combine_gg_elements(ggplot(), curve_layers)   This was something I picked up from Hadley Wickham\u0026rsquo;s ggplot2 book but it seems that we don\u0026rsquo;t need that anymore (the combine function, the book is still a great ressource). But I leave this here for completeness' sake. Once again, writing a blog post has taught me stuff I thought I already knew. If you want to watch me learn more stuff or want to learn more ggplot things yourself, feel free to subscribe to my RSS feed or follow me on Twitter.\n","description":"Functional programming is a mighty sword. Today, we use it to avoid tedious repetitions when things go wrong in ggplot.","id":0,"section":"post","tags":[],"title":"How to use functional programming for ggplot","uri":"https://albert-rapp.de/post/2022-03-25-functional-programming-when-geoms-are-not-vectorized/"},{"content":"For some reason, using other than the default font in plots has been a major problem for me in R. Supposedly, one can use the extrafont package to manage all of that but I found it too cumbersome. Instead, I found out that the showtext package can make my life easier.\nEven though working with text in plot is not yet completely free of troubles, showtext has made many things easier. Now, I can finally choose fonts freely and even use icons. This blogposts gives you a how-to so that you can do that too.\nImport and Use Fonts with showtext A great source for fonts is Google\u0026rsquo;s font page. What is great abut this page is that it can display texts in many different fonts.\nFigure 1: Screenshot from fonts.google.com\n\rOnce we found a nice font, we can use its name to make it available within R. This is done with showtext\u0026rsquo;s helpful font_add_google() function. Let\u0026rsquo;s import a couple of random fonts.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Packages that we will use in this post setwd(here::here(\u0026#39;content/en/post/2022-03-04-fonts-and-icons/\u0026#39;)) library(tidyverse) library(showtext) library(ggtext) library(gghighlight) # Import fonts # First argument = google name,  # Secont name = font name in R font_add_google(\u0026#39;Lora\u0026#39;, \u0026#39;lora\u0026#39;) font_add_google(\u0026#39;Lobster\u0026#39;, \u0026#39;lobster\u0026#39;) font_add_google(\u0026#39;Anton\u0026#39;, \u0026#39;anton\u0026#39;) font_add_google(\u0026#39;Fira Sans\u0026#39;, \u0026#39;firasans\u0026#39;) font_add_google(\u0026#39;Syne Mono\u0026#39;, \u0026#39;syne\u0026#39;) # Important step to enable showtext font rendering! showtext_auto()   Notice that we have also used showtext_auto(). This is necessary for showtext to take over the show. Otherwise, the new fonts would not be usable. Now, let\u0026rsquo;s take a look at our new fonts.\n1 2 3 4 5 6 7 8 9 10 11 12  tib \u0026lt;- tibble( family = c(\u0026#39;firasans\u0026#39;, \u0026#39;lora\u0026#39;, \u0026#39;lobster\u0026#39;, \u0026#39;anton\u0026#39;, \u0026#39;syne\u0026#39;), x = 0, y = seq(0.0, 1, length.out = 5), label = \u0026#39;Showtext shows text. Wow. What an insight.\u0026#39; ) tib %\u0026gt;% ggplot(aes(x, y, label = label)) + geom_text(family = tib$family, size = 13, hjust = 0, col = \u0026#39;dodgerblue4\u0026#39;) + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) + theme_void()   You may wonder why we have used coord_cartesian() here. We did this in order to ensure that the x-axis is not centered at 0 and our example texts won\u0026rsquo;t be outside of the plot. Personally, I find this somewhat tedious but this can\u0026rsquo;t be helped, I guess. With text elements we always run at the risk of writing outside of the plot area.\nNext, let\u0026rsquo;s make our use of fonts somewhat more practical. In my last blog post, I stressed the use of highlighting a few important things instead of using many colors. Combine this with direct labels instead of a legend and you get this plot I created using the Fira Sans font.\nNow, see what it would look like had I used the Lobster font instead.\nFeels different doesn\u0026rsquo;t it? And this is still different than the Anton font.\nImport and Use Icon Fonts with showtext We can not only use regular text fonts but also icons with showtext. For example, we may want to use one of the free Fontawesome icons. To do so, download the newest version and extract the .otf-files into your working directory. These contain the font information that you need. Importing these (and any other font for that matter) works with font_add() and the path to the .otf-files.\n1 2 3 4 5  # First argument = name in R # Second argument = path to .otf-file font_add(\u0026#39;fa-reg\u0026#39;, \u0026#39;fonts/Font Awesome 6 Free-Regular-400.otf\u0026#39;) font_add(\u0026#39;fa-brands\u0026#39;, \u0026#39;fonts/Font Awesome 6 Brands-Regular-400.otf\u0026#39;) font_add(\u0026#39;fa-solid\u0026#39;, \u0026#39;fonts/Font Awesome 6 Free-Solid-900.otf\u0026#39;)   Now that we imported the fonts, we can use ggtext\u0026rsquo;s geom_richtext() and some HTML wizardry to add icons to our previously imported fonts from Google. But first, what we need is an icon\u0026rsquo;s unicode identifier? Uni-what?\nThe easiest way to find that is to stroll through the Fontawesome icons online. Then, find one that matches the font you want to use, e.g. free and solid. Finally, find it\u0026rsquo;s unicode character in the corresponding popup menu.\nFigure 2: Screenshot from fontawesome.com. Unicode highlighted in yellow.\n\rOnce you got this, you can add \u0026amp;#x in front of the unicode and wrap \u0026lt;span\u0026gt; tags around it. Within these tags, you will have to specify font-family so that the icon is rendered.\n1 2 3 4 5 6 7 8 9 10 11 12  tib \u0026lt;- tibble( family = c(\u0026#39;firasans\u0026#39;, \u0026#39;lora\u0026#39;, \u0026#39;lobster\u0026#39;, \u0026#39;anton\u0026#39;, \u0026#39;syne\u0026#39;), x = 0, y = seq(0.0, 1, length.out = 5), label = \u0026#34;Let\u0026#39;s talk cash \u0026lt;span style=\u0026#39;font-family:fa-solid\u0026#39;\u0026gt;\u0026amp;#xf651;\u0026lt;/span\u0026gt;\u0026#34; ) tib %\u0026gt;% ggplot(aes(x, y, label = label)) + geom_richtext(family = tib$family, size = 16, hjust = 0, col = \u0026#39;dodgerblue4\u0026#39;, label.colour = NA) + coord_cartesian(xlim = c(0, 1), ylim = c(-0.1, 1.1)) + theme_void()   This way, you can also use icons in scatter plots. Though, make sure to set fill=NA if you do not want to have white boxes around the icons.\n1 2 3 4  tibble(x = runif(25), y = runif(25)) %\u0026gt;% ggplot(aes(x, y, label = \u0026#34;\u0026lt;span style=\u0026#39;font-family:fa-solid;\u0026#39;\u0026gt;\u0026amp;#xf651;\u0026lt;/span\u0026gt;\u0026#34;)) + geom_richtext(size = 12, label.colour = NA, fill = NA, col = \u0026#39;dodgerblue4\u0026#39;,) + theme_minimal()   You will notice that using the two previous code chunks will generate a lot of warnings about \u0026ldquo;native encoding\u0026rdquo;. So far, I have always been able to ignore these without any trouble. I really don\u0026rsquo;t know why they appear. And if you know, please let me know in the comments below.\n","description":"This is a short tutorial on how to import fonts and icons in R using the showtext package.","id":1,"section":"post","tags":[],"title":"How to use Fonts and Icons in ggplot","uri":"https://albert-rapp.de/post/2022-03-04-fonts-and-icons/"},{"content":"When creating a plot I frequently catch myself using way too many colors. Thus, I have to remind myself often to keep things simple. Usually, this makes a data visualization way more effective.\nLuckily, I found a neat datawrapper blogpost by Lisa Charlotte Muth that shows us how to reduce the use of colors.\nBut as I was reading the blog post, I found myself wondering how some of the mentioned principles could be implemented in ggplot. Naturally, I began experimenting and created a few example plots using fewer colors. This post will show you how you can do that too.\nPreliminaries For completeness' sake, let me mention the basic settings I will use for all visualizations. Honestly, if you have no idea what happens in the following code chunk, just skip it. More or less, this chunk makes sure that all plots are using theme_minimal() plus a small number of tweaks. These tweaks are\n The use of the Fira Sans font with help from the showtext package. The plot titles are aligned to the left, have some spacing around them and are colored using a color from the Okabe Ito color palette. Ever since I read Fundamentals of Data Visualization by Claus Wilke, I am fond of this color palette as I find the colors nice and apparently it is also color-blind safe.  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  library(tidyverse) library(showtext) font_add_google(\u0026#34;Fira Sans\u0026#34;, \u0026#34;firasans\u0026#34;) showtext_auto() theme_customs \u0026lt;- theme( text = element_text(family = \u0026#39;firasans\u0026#39;, size = 16), plot.title.position = \u0026#39;plot\u0026#39;, plot.title = element_text( face = \u0026#39;bold\u0026#39;, colour = thematic::okabe_ito(8)[6], margin = margin(t = 2, r = 0, b = 7, l = 0, unit = \u0026#34;mm\u0026#34;) ), ) theme_set(theme_minimal() + theme_customs)   Show shades, not hues Alright, enough with the preliminaries. Let\u0026rsquo;s count how many different car classes are represented in the mpg dataset from the ggplot2 package. I am sure you have seen the data already when you read this ggplot post. So, no further comment on this data set.\n1 2 3  mpg %\u0026gt;% ggplot(aes(x = year, fill = class)) + geom_bar()   Ugh, this is a colorful mess and sort of reminds me of the gnome rainbow puking gif. Let\u0026rsquo;s reduce the color load by sticking to only three colors. To differentiate between classes we will make some colors more transparent.\nThus, we need to create a new variable in our data set that lumps the classes into three groups (for the colors).\n1 2 3 4 5 6 7 8 9 10  # Group classes into three groups (to reduce colors to 3) dat \u0026lt;- mpg %\u0026gt;% mutate( year = factor(year), class_group = case_when( class %in% c(\u0026#39;2seater\u0026#39;, \u0026#39;compact\u0026#39;, \u0026#39;midsize\u0026#39;) ~ \u0026#34;grp1\u0026#34;, class == \u0026#39;minivan\u0026#39; ~ \u0026#34;grp2\u0026#34;, T ~ \u0026#34;grp3\u0026#34; ) )   Now that this is done, we can map fill to our new class_group variable and the regular class variable to alpha.\n1 2 3 4 5 6 7 8 9 10  shades_plt \u0026lt;- dat %\u0026gt;% ggplot(aes(x = year, fill = class_group, alpha = class)) + geom_bar() + labs( x = \u0026#39;Year\u0026#39;, y = \u0026#39;Counts\u0026#39;, alpha = \u0026#39;Class\u0026#39;, title = \u0026#39;Show shades, not hues\u0026#39; ) shades_plt   For better control of the visuals let us manually create and assign colors and the transparency levels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Color-blind safe colors colors \u0026lt;- thematic::okabe_ito(3) # Possible levels of transparency (one for each class) alpha_max \u0026lt;- 1 alpha_min \u0026lt;- 0.7 alpha_vals \u0026lt;- c( seq(alpha_max, alpha_min, length.out = 4), seq(alpha_min, alpha_max, length.out = 4)[-1] ) alpha_vals ## [1] 1.0 0.9 0.8 0.7 0.8 0.9 1.0 # Tweak previous plot shades_plt \u0026lt;- shades_plt + scale_fill_manual(values = colors) + scale_alpha_manual(values = alpha_vals) shades_plt   Next, let us consolidate the two legends into one. This can be done via guides(). Here, the fill guide will be set to guide_none() to get rid of the class_group legend.\nAlso, the alpha guide needs to be manually overwritten via override.aes in guide_legend() using the color codes that we saved in the vector colors. This way, the alpha legend will also depict the colors instead of only the transparency level.\n1 2 3 4 5 6 7 8 9  shades_plt \u0026lt;- shades_plt + guides( fill = guide_none(), alpha = guide_legend( override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)] ) ) ) shades_plt   Group categories together by color, but keep showing them So, this already looks better. However, adjacent colored blocks now \u0026ldquo;merge\u0026rdquo; into each other. This can make it hard to differentiate between classes.\nTo overcome this issue, add lines between blocks. Luckily, this is spectacularly easy and done by setting the color aesthetic in geom_bar() to white. Here\u0026rsquo;s the complete code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  dat %\u0026gt;% ggplot(aes(x = year, fill = class_group, alpha = class)) + geom_bar(col = \u0026#39;white\u0026#39;) + # Add lines for distinction scale_fill_manual(values = colors) + scale_alpha_manual(values = alpha_vals) + guides( fill = guide_none(), alpha = guide_legend(override.aes = list(fill = colors[c(1, 1, 1, 2, 3, 3, 3)])) ) + labs( x = \u0026#39;Year\u0026#39;, y = \u0026#39;Counts\u0026#39;, alpha = \u0026#39;Class\u0026#39;, title = \u0026#39;Group categories together by color, \\nbut keep showing them\u0026#39; )   Emphasize just one or a few categories Next, let us switch tracks and look at some other kind of data. At Our World in Data you can find a lot of interesting data sets. One of these contains survey information on who Americans spend their time with (in average minutes per day by age). If you download this data set, you can create a plot like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  # Some data wrangling time_data \u0026lt;- read_csv(\u0026#34;time-spent-with-relationships-by-age-us.csv\u0026#34;) %\u0026gt;% rename_with( ~c(\u0026#39;Entitity\u0026#39;, \u0026#39;Code\u0026#39;, \u0026#39;Age\u0026#39;, \u0026#39;alone\u0026#39;, \u0026#39;friends\u0026#39;, \u0026#39;children\u0026#39;, \u0026#39;parents\u0026#39;, \u0026#39;partner\u0026#39;, \u0026#39;coworkers\u0026#39;) ) %\u0026gt;% pivot_longer( cols = alone:coworkers, names_to = \u0026#39;person\u0026#39;, values_to = \u0026#39;minutes\u0026#39; ) %\u0026gt;% janitor::clean_names() %\u0026gt;% filter(age \u0026lt;= 80) # Color-blind safe colors colors \u0026lt;- thematic::okabe_ito(8)[-6] # Line plot p \u0026lt;- time_data %\u0026gt;% ggplot(aes(x = age, y = minutes, col = person)) + geom_line(size = 1.5) + scale_color_manual(values = colors) + coord_cartesian(xlim = c(15, 81), expand = F) + scale_y_continuous(minor_breaks = NULL) + labs(x = \u0026#39;Age (in years)\u0026#39;, y = \u0026#39;Minutes\u0026#39;, col = \u0026#39;Time spent\u0026#39;) p   Once again, we created a plot with loads of color. If this were an interactive plot where we can focus on one line at a time, this would not necessarily be a problem. However, as it is, this is a rather messy spaghetti plot and extracting meaning from it is hard.\nBut if we know what story we want to tell, then we can save this plot by emphasizing only the important parts. This is where the gghighlight package shines. It works by adding a gghighlight() layer to an existing plot with conditions for filtering. All data points that do not fulfill these conditions are greyed out.\n1 2 3 4 5  library(gghighlight) alone_plt \u0026lt;- p + gghighlight(person == \u0026#39;alone\u0026#39;, use_direct_label = F) + labs(title = \u0026#39;Emphasize just one or a few categories\u0026#39;) alone_plt   Finally, we are only one text annotation away from telling a story.\n1 2 3 4 5 6 7 8 9 10 11  alone_plt + annotate( \u0026#39;text\u0026#39;, x = 15, y = 455, label = \u0026#39;We spend a lot of time alone...\u0026#39;, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, size = 7 )   Of course, a data set may contain multiple stories that may also need multiple highlights. No problem. With gghighlight() we can combine as many conditions as we like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  age_40_plt \u0026lt;- p + gghighlight( person %in% c(\u0026#39;alone\u0026#39;, \u0026#39;children\u0026#39;), age \u0026gt;= 38, use_direct_label = F ) + geom_segment(x = 38, xend = 38, y = -Inf, yend = 300, linetype = 2, col = \u0026#39;grey20\u0026#39;) + labs(title = \u0026#39;Emphasize just one or a few categories\u0026#39;) age_40_plt + annotate( \u0026#39;text\u0026#39;, x = 15, y = 403, label = \u0026#39;Around the age of 40, we spend \\nless time with children and \\nmore time alone.\u0026#39;, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, lineheight = 0.85, size = 5.5 )   Label directly In all previous plots, we displayed a legend at the side of the plot. However, this requires quite a large amount of space which we can save by direct labeling (either with annotate() for a single label or geom_text() for multiple labels).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  alone_plt + annotate( \u0026#39;text\u0026#39;, x = 15, y = 455, label = \u0026#39;We spend a lot of time alone...\u0026#39;, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, size = 7 ) + annotate( \u0026#39;text\u0026#39;, x = 70, y = 420, label = \u0026#39;alone\u0026#39;, hjust = 0, vjust = 0, size = 7, family = \u0026#39;firasans\u0026#39;, color = colors[1] ) + labs(title = \u0026#39;Label directly\u0026#39;) + theme(legend.position = \u0026#39;none\u0026#39;)   This way, we save a lot of space and can give the remaining part of the plot more room. Also, this saves the reader some cognitive effort because one does not have to switch back and forth between legend and actual plot.\nIn this particular case, there is another option for direct labelling. Notice how close the word \u0026lsquo;alone\u0026rsquo; from the original text annotation is to the highlighted line anyway. Therefore, we may as well save us one additional annotation and colorize a single word in the orginal annotation.\nTo do so, the ggtext package and a bit of HTML magic will help us. Basically, what we need is to change the annotation from text geom to richtext geom and create a string that contains the HTML-code for colored text. Here that is \u0026lt;span style = 'color:#E69F00;'\u0026gt;...\u0026lt;/span\u0026gt;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  library(ggtext) color_alone \u0026lt;- glue::glue( \u0026#34;We spend a lot of time \u0026lt;span style = \u0026#39;color:{colors[1]};\u0026#39;\u0026gt;alone\u0026lt;/span\u0026gt;...\u0026#34; ) color_alone ## We spend a lot of time \u0026lt;span style = \u0026#39;color:#E69F00;\u0026#39;\u0026gt;alone\u0026lt;/span\u0026gt;... alone_plt + labs(title = \u0026#39;Label directly\u0026#39;) + annotate( \u0026#39;richtext\u0026#39;, x = 15, y = 400, label = color_alone, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, size = 7, label.color = NA ) + theme(legend.position = \u0026#39;none\u0026#39;)   Naturally, we can do this for our second highlighted plot as well. In this case, the colored key words are not adjacent to the actual lines.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  age_40_text \u0026lt;- glue::glue( \u0026#34;Around the age of 40, we spent less \u0026lt;br\u0026gt; time with \u0026lt;span style = \u0026#39;color:{colors[2]};\u0026#39;\u0026gt;children\u0026lt;/span\u0026gt; and more \u0026lt;br\u0026gt; time \u0026lt;span style = \u0026#39;color:{colors[1]};\u0026#39;\u0026gt;alone\u0026lt;/span\u0026gt;.\u0026#34; ) age_40_plt + labs(title = \u0026#39;Label directly\u0026#39;) + annotate( \u0026#39;richtext\u0026#39;, x = 15, y = 400, label = age_40_text, hjust = 0, vjust = 0, family = \u0026#39;firasans\u0026#39;, lineheight = 1.25, size = 5.5, label.color = NA ) + theme(legend.position = \u0026#39;none\u0026#39;)   Consequently, the reader may have to go back and forth between text and lines again but still we used our space more efficiently. So, I will let this count as direct labeling.\nFinally, let us come full circle and return to our initial bar plot. This one could also use some direct labels. Normally, I would simply add a geom_text() layer together with position_stack() to the initial plot as described here.\nBut for some magical reason, this did not align the labels properly and it was driving me crazy. Therefore, I counted the car classes and computed the label positions manually.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  manual_counts \u0026lt;- mpg %\u0026gt;% count(year, class) %\u0026gt;% mutate( year = factor(year), class_group = case_when( class %in% c(\u0026#39;2seater\u0026#39;, \u0026#39;compact\u0026#39;, \u0026#39;midsize\u0026#39;) ~ \u0026#34;grp1\u0026#34;, class == \u0026#39;minivan\u0026#39; ~ \u0026#34;grp2\u0026#34;, T ~ \u0026#34;grp3\u0026#34; ) ) labels \u0026lt;- manual_counts %\u0026gt;% mutate(class = factor(class)) %\u0026gt;% group_by(year) %\u0026gt;% arrange(year, desc(class)) %\u0026gt;% mutate( csum = cumsum(n), n = (lag(csum, default = 0) + csum) / 2 )   But once this small detour is overcome, we can label the plot in the same manner as before. Unfortunately, the 2seater class is so small that the label wouldn\u0026rsquo;t fit into the box. Therefore, I decided to plot the label on top.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  manual_counts %\u0026gt;% ggplot(aes(x = year, y = n, fill = class_group)) + geom_col(aes(alpha = class), col = \u0026#39;white\u0026#39;) + scale_fill_manual(values = colors) + scale_alpha_manual(values = alpha_vals) + labs( x = \u0026#39;Year\u0026#39;, y = \u0026#39;Counts\u0026#39;, alpha = \u0026#39;Class\u0026#39;, title = \u0026#39;Label directly\u0026#39; ) + # Add all but one label geom_text( data = labels %\u0026gt;% filter(class != \u0026#39;2seater\u0026#39;), aes(label = class), col = \u0026#39;white\u0026#39;, family = \u0026#39;firasans\u0026#39;, size = 5, fontface = \u0026#39;bold\u0026#39; ) + # Add 2seater label geom_text( data = labels %\u0026gt;% filter(class == \u0026#39;2seater\u0026#39;), aes(y = n + 3, label = class), col = \u0026#39;black\u0026#39;, family = \u0026#39;firasans\u0026#39;, size = 5, fontface = \u0026#39;bold\u0026#39; ) + theme(legend.position = \u0026#39;none\u0026#39;)   Closing remarks The blog post that inspired this post contains a few more tips like using other indicators than color and you should definitely check it out. Also, Lisa Muth apparently writes a book on colors in data visualizations and documents her thoughts here. If you look for more content on colors, this might be a fountain of information.\nAs for using patterns instead of colors, I recently wrote a blog post that leverages the ggpattern package to do just that. Check it out here. And as always, if you don\u0026rsquo;t want to miss new blog post, either follow me on Twitter or via my RSS feed.\n","description":"Inspired by a datawrapper blogpost, we explore how to work with fewer colors in ggplot.","id":2,"section":"post","tags":[],"title":"4 Ways to use colors in ggplot more efficiently","uri":"https://albert-rapp.de/post/2022-02-19-ggplot2-color-tips-from-datawrapper/"},{"content":"I\u0026rsquo;ve been reading Mastering Shiny by Hadley Wickham lately and one of the things that intrigued me is that you can make ggplots interactive. Though I believe that there are limitation to the level of interactiveness compared to using, say, plotly, I really wanted to practice interactive ggplots with Shiny. Naturally, I build a Shiny app to figure things out. Here\u0026rsquo;s a demonstration of what the app can do. The rest of this chapter teaches you how some parts of the app were implemented.\nWorking with clicks If you have build at least one Shiny app, then you are probably aware that you can include plots on the UI with plotOutput(). (If, in fact, you have never touched Shiny, then feel free to check out how I thought my students the basics of Shiny.) But what you may not know, is that apart from its outputId, width and height arguments, this output function also uses arguments like click and dblclick. These are the secrets to unlocking interactiveness.\nImagine that you have a user interface that includes a plot output via\n1  plotOutput(\u0026#39;awesome_plot\u0026#39;, click = \u0026#39;awesome_click\u0026#39;)   Now, what this small additional argument gives you is a way to access the coordinates of something the user clicks on. What you will have to do is to observe input$awesome_click. Here\u0026rsquo;s a minimal example of how that works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  library(shiny) library(ggplot2) library(dplyr) ui \u0026lt;- fluidPage( plotOutput(\u0026#39;awesome_plot\u0026#39;, click = \u0026#39;awesome_click\u0026#39;), ) server \u0026lt;- function(input, output, session) { # Create dummy data as reactive value my_dat \u0026lt;- reactiveVal(tibble(x = 3, y = 4, msg = \u0026#39;Click me\u0026#39;)) # Render plot with fixed coordinate system output$awesome_plot \u0026lt;- renderPlot({ ggplot(data = my_dat()) + geom_text(aes(x, y, label = msg), size = 15, vjust = 0, hjust = 0) + coord_cartesian(xlim = c(0, 7), ylim = c(0, 8)) }) # Update dummy data on click  observeEvent(input$awesome_click, { my_dat( my_dat() %\u0026gt;% mutate( x = input$awesome_click$x, y = input$awesome_click$y, msg = if (runif(1) \u0026lt; 0.5) \u0026#39;I like that. Do it again.\u0026#39; else \u0026#39;Stop that!\u0026#39; ) ) }) } shinyApp(ui, server)   This will give us the following app.\nLike a cat, this app is a master of mixed signals and wants to be touched but only a random amount of times. Unlike a cat, the app will show you a plot displaying its latest message at the most recently clicked spot.\nAll of this is powered by observing changes in input$awesome_click and then using this list\u0026rsquo;s new x- and y-values to update the reactive value my_dat that underlies the plot. Notice that I have fixed the axes of the plot because otherwise the message will always be displayed in the middle of the plot. After all, the plot will be entirely rebuilt using new underlying data. Fundamentally, this is how I build the \u0026lsquo;color my voronoi\u0026rsquo; from above.\nBut, of course, I have tried out more stuff like user feedback and even some javascript magic. Stick around if you want to learn these ancient skills as well. Destiny is calling.\nLet your user know that he messed up and stop him before it\u0026rsquo;s too late To my surprise, UI elements like numericInput() do not actually check that an input is valid even though there are arguments like min and max. Of course, a user may end up giving wrong inputs that your app can\u0026rsquo;t handle. We can\u0026rsquo;t have that now, can we?\nWe will need to stop that insubordinate and churlish behavior immediately. In case you recognized that combination of \u0026lsquo;insubordinate\u0026rsquo; and \u0026lsquo;churlish\u0026rsquo;, then I will have you now, yes, this is a reference to Mr. Garvey and the rest of this section is a homage to a skit that makes me giggle every time.\nSo, let\u0026rsquo;s build an app that works as follows:\nThe notifications in this app are all powered through the shinyFeedback package. In order to activate its powers, drop a shinyFeedback::useShinyFeedback() in the UI like so.\n1 2 3 4 5 6 7 8 9 10 11  library(shiny) library(dplyr) ui \u0026lt;- fluidPage( shinyFeedback::useShinyFeedback(), h3(\u0026#39;A Day with Mr. Garvey\u0026#39;), textInput( \u0026#39;name\u0026#39;, \u0026#39;What\\\u0026#39;s your name?\u0026#39;, ) )   Then, you are all set up to activate warnings and notifications by your server function. Here is a simplified version of the app\u0026rsquo;s remaining code.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  names \u0026lt;- c(\u0026#39;Jay Quellin\u0026#39;,\u0026#39;Jacqueline\u0026#39;, \u0026#39;Balakay\u0026#39;, \u0026#39;Blake\u0026#39;, \u0026#39;Dee-nice\u0026#39;, \u0026#39;Denise\u0026#39;, \u0026#39;Ay-Ay-Ron\u0026#39;, \u0026#39;Aaron\u0026#39;) return_msg \u0026lt;- function(name) { case_when( name == \u0026#39;Balakay\u0026#39; ~ \u0026#39;My name is Blake.\u0026#39;, name == \u0026#39;Blake\u0026#39; ~ \u0026#39;Do you wanna go to war, Balakay? You better check yourself!\u0026#39;, name == \u0026#39;Jay Quellin\u0026#39; ~ \u0026#39;Do you mean Jacqueline?\u0026#39;, name == \u0026#39;Jacqueline\u0026#39; ~ \u0026#39;So that\\\u0026#39;s how it\\\u0026#39;s going to be. I got my eye on you Jay Quellin!\u0026#39;, name == \u0026#39;Dee-nice\u0026#39; ~ \u0026#39;Do you mean Denise?\u0026#39;, name == \u0026#39;Denise\u0026#39; ~ \u0026#39;You say your name right!\u0026#39;, name == \u0026#39;Ay-Ay-Ron\u0026#39; ~ \u0026#39;It is pronounced Aaron.\u0026#39;, name == \u0026#39;Aaron\u0026#39; ~ \u0026#39;You done messed up Ay-Ay-Ron!\u0026#39; ) } server \u0026lt;- function(input, output, session) { name_input \u0026lt;- reactive(input$name) observeEvent(name_input(), { shinyFeedback::feedbackDanger( \u0026#39;name\u0026#39;, show = (name_input() %in% names), text = return_msg(name_input()) ) shinyFeedback::feedbackSuccess( \u0026#39;name\u0026#39;, show = !(name_input() %in% names), text = \u0026#39;Thank you!\u0026#39; ) }) } shinyApp(ui, server)   As you can see, the feedback functions work with\n the name of an input, a rule when to show up and a text to display.  This code is pretty straightforward but, unfortunately, this app does not work like the one you have seen above. There are a couple of problems:\n For starters, if you start the app, then the text input is empty, so !(name_input() %in% names) is true and the app will immediately display \u0026ldquo;Thank you!\u0026rdquo; After you write a name on the list of defined names, then no warning will be displayed. That is because the later feedbackSuccess() will always command that there is nothing to display.  Therefore, we need something that stops the feedbackSuccess() from executing when we don\u0026rsquo;t need it. This can be achieved through the little but powerful req() function. It checks that all given conditions are met or stops the execution where it is. In this case, you will need to drop req(name_input(), !(name_input() %in% names)) in front of feedbackSuccess().\nSmall technical detail: Notice that name_input() will be '' in the beginning. Technically, this is not a boolean but that doesn\u0026rsquo;t matter to Shiny. What matters is that '' is not \u0026ldquo;truthy\u0026rdquo;. See ?isTruthy for more details.\nNow, even with this small change. Our app won\u0026rsquo;t run smoothly because sometimes the notifaction will not change from \u0026ldquo;success\u0026rdquo; to \u0026ldquo;danger\u0026rdquo;. This is is because sometimes the notifaction needs to be reseted to work with new notifications. Therefore, a hideFeedback() is in order.\nAlso, if you are not fast at typing, then a notification might already show up, when you are still typing. It is rude to interrupt our kind user like this. Therefore, let\u0026rsquo;s make sure our app waits a little before giving out notifications. We can let out app wait for a defined amount of milliseconds by sending our reactive name_input() to debounce(). In total, our server function now looks like this.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  server \u0026lt;- function(input, output, session) { name_input \u0026lt;- reactive(input$name) %\u0026gt;% debounce(250) observeEvent(name_input(), { shinyFeedback::hideFeedback(\u0026#39;name\u0026#39;) shinyFeedback::feedbackDanger( \u0026#39;name\u0026#39;, show = (name_input() %in% names), text = return_msg(name_input()) ) req(name_input(), !(name_input() %in% names)) shinyFeedback::feedbackSuccess( \u0026#39;name\u0026#39;, show = !(name_input() %in% names), text = \u0026#39;Thank you!\u0026#39; ) }) }   Finally, let me mention that, within the function req(), it is also possible to set cancelOutput = TRUE. This stops the code execution as usual but avoids destroying previously displayed outputs.\nSprinkle some javascript magic on top of your app For my final trick before I disappear into the ether, let me show you a little bit of javascript. As I, myself, do not know much about JS, I am particularly proud that I included some of that web magic into my voronoi coloring app. \u0026lsquo;What did you do?', you ask? Well, did you notice that the colour dropdown menu in the voronoi app contains the actual colors next to the color names? That is some JS magic right there! Impressive, I know.\nTo make that work, I had to use the options argument of selectizeInput() together with the render() function and some actual JS code. The whole thing is adapted from this SO post and looks like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  col2hex \u0026lt;- gplots::col2hex colorValues \u0026lt;- colors() colorNames \u0026lt;- glue::glue(\u0026#34;{colorValues} \u0026lt;span style=\u0026#39;background-color:{col2hex(colorValues)}\u0026#39;\u0026gt;{rep(\u0026#39;\u0026amp;nbsp;\u0026#39;, 15) %\u0026gt;% stringr::str_c(collapse = \u0026#39;\u0026#39;)}\u0026lt;/span\u0026gt;\u0026#34;) colors \u0026lt;- setNames(colorValues, colorNames) js_render_string \u0026lt;- I(\u0026#34; { item: function(item, escape) { return \u0026#39;\u0026lt;div\u0026gt;\u0026#39; + item.label + \u0026#39;\u0026lt;/div\u0026gt;\u0026#39;; }, option: function(item, escape) { return \u0026#39;\u0026lt;div\u0026gt;\u0026#39; + item.label + \u0026#39;\u0026lt;/div\u0026gt;\u0026#39;; } }\u0026#34;) selectizeInput( \u0026#34;color\u0026#34;, \u0026#34;Colour\u0026#34;, selected = \u0026#39;grey80\u0026#39;, choices = colors, options = list(render = js_render_string) )   Let\u0026rsquo;s untangle this step by step. The first part of this code gives us a vector colors containing the color names like \u0026ldquo;white\u0026rdquo; and \u0026ldquo;aliceblue\u0026rdquo; as values. The same vector also uses names for the vector elements that will be displayed to the user. In principal, this colors vector looks like this:\n1 2 3 4 5 6 7 8 9 10  x \u0026lt;- c(\u0026#39;white\u0026#39;, \u0026#39;aliceblue\u0026#39;) # no names x ## [1] \u0026#34;white\u0026#34; \u0026#34;aliceblue\u0026#34; x \u0026lt;- setNames(x, c(\u0026#39;name1\u0026#39;, \u0026#39;name2\u0026#39;)) # with names x ## name1 name2  ## \u0026#34;white\u0026#34; \u0026#34;aliceblue\u0026#34; x[\u0026#39;name1\u0026#39;] # named vectors can be used like dictionaries ## name1  ## \u0026#34;white\u0026#34;   In our color example, instead of using arbitrary names, I converted the color names to their hexvalues like #FFFFFF and wrapped those in some HTML code that could potentially look like \u0026quot;\u0026lt;span style='background-color#FFFFFF'\u0026gt;white\u0026lt;/span\u0026gt;\u0026quot;. This corresponds to the word white with background color #FFFFFF (also white - unspectacular).\nBut in the actual app I wanted to have colored bars next to the color names. Thus, I have used the HTML code for white space \u0026amp;nbsp; and made this into \u0026quot;white \u0026lt;span style='background-color#FFFFFF'\u0026gt;\u0026amp;nbsp;\u0026lt;/span\u0026gt;\u0026quot;. Now, to make that color bar longer, I repeated whited space with rep() and glued those into a single string using stringr::str_c(). This is what the vector looks like if I only use two white space repeats.\n1 2 3 4 5 6 7 8 9 10 11  library(dplyr, warn.conflicts = F) col2hex \u0026lt;- gplots::col2hex colorValues \u0026lt;- colors() colorNames \u0026lt;- glue::glue(\u0026#34;{colorValues} \u0026lt;span style=\u0026#39;background-color:{col2hex(colorValues)}\u0026#39;\u0026gt;{rep(\u0026#39;\u0026amp;nbsp;\u0026#39;, 2) %\u0026gt;% stringr::str_c(collapse = \u0026#39;\u0026#39;)}\u0026lt;/span\u0026gt;\u0026#34;) colors \u0026lt;- setNames(colorValues, colorNames) colors[1] ## white \u0026lt;span style=\u0026#39;background-color:#FFFFFF\u0026#39;\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;\u0026lt;/span\u0026gt;  ## \u0026#34;white\u0026#34; colors[2] ## aliceblue \u0026lt;span style=\u0026#39;background-color:#F0F8FF\u0026#39;\u0026gt;\u0026amp;nbsp;\u0026amp;nbsp;\u0026lt;/span\u0026gt;  ## \u0026#34;aliceblue\u0026#34;   In the dropdown menu of the app the user will see the names of the color vector, i.e. the HTML code and within the server function of our app the selection will then correspond to the actual value of the vector, i.e. the color name without the html stuff.\nIn our dummy example from above, the user would see name1 and name2 in the dropdown menu but within the server function a user\u0026rsquo;s selection would correspond to input$color which would evaluate to white or aliceblue.\nClearly, we don\u0026rsquo;t want the user to see the raw HTML code. This is where JS comes into play. The code that is stored in js_render_string evaluates the HTML code in order to display the actual colors instead of the raw code. Finally, to execute the JS code we need to pass it to the options of selectizeInput via options = list(render = js_render_string).\nThere you go, this is how I created the color bars in my app using a JS snippet I found on Stackoverflow. You can find the complete codes of the apps we\u0026rsquo;ve build here (click app), here (notification names app) and here (voronoi coloring app). If you liked this post and want to see more Shiny posts, let me know in the comments or simply hit the applause button below. Of course, you can also always follow my work via Twitter.\n","description":"Here's how I turned a ggplot interactive using Shiny and what else I learned while building that app","id":3,"section":"post","tags":["shiny"],"title":"Interactive ggplots, user feedback, and a little bit of javascript magic with Shiny","uri":"https://albert-rapp.de/post/2022-01-17-drawing-a-ggplot-interactively/"},{"content":"The janitor package contains only a little number of functions but nevertheless it is surprisingly convenient. I never really fully appreciated its functionality until I took a look into the documentation. Of course, other packages can achieve the same thing too but janitor makes a lot of tasks easy. Thus, here is a little showcase.\nClean column names As everyone working with data knows, data sets rarely come in a clean format. Often, the necessary cleaning process already starts with the column names. Here, take this data set from TidyTuesday, week 41.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  nurses \u0026lt;- readr::read_csv(\u0026#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-10-05/nurses.csv\u0026#39;) names(nurses) ## [1] \u0026#34;State\u0026#34;  ## [2] \u0026#34;Year\u0026#34;  ## [3] \u0026#34;Total Employed RN\u0026#34;  ## [4] \u0026#34;Employed Standard Error (%)\u0026#34;  ## [5] \u0026#34;Hourly Wage Avg\u0026#34;  ## [6] \u0026#34;Hourly Wage Median\u0026#34;  ## [7] \u0026#34;Annual Salary Avg\u0026#34;  ## [8] \u0026#34;Annual Salary Median\u0026#34;  ## [9] \u0026#34;Wage/Salary standard error (%)\u0026#34;  ## [10] \u0026#34;Hourly 10th Percentile\u0026#34;  ## [11] \u0026#34;Hourly 25th Percentile\u0026#34;  ## [12] \u0026#34;Hourly 75th Percentile\u0026#34;  ## [13] \u0026#34;Hourly 90th Percentile\u0026#34;  ## [14] \u0026#34;Annual 10th Percentile\u0026#34;  ## [15] \u0026#34;Annual 25th Percentile\u0026#34;  ## [16] \u0026#34;Annual 75th Percentile\u0026#34;  ## [17] \u0026#34;Annual 90th Percentile\u0026#34;  ## [18] \u0026#34;Location Quotient\u0026#34;  ## [19] \u0026#34;Total Employed (National)_Aggregate\u0026#34;  ## [20] \u0026#34;Total Employed (Healthcare, National)_Aggregate\u0026#34; ## [21] \u0026#34;Total Employed (Healthcare, State)_Aggregate\u0026#34;  ## [22] \u0026#34;Yearly Total Employed (State)_Aggregate\u0026#34;   These column names are intuitively easy to understand but not necessarily easy to process by code as there are white spaces and other special characters. Therefore, I accompany most data input by clean_names() from the janitor package.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  library(janitor) library(dplyr) # load for pipe %\u0026gt;% and later wrangling names(nurses %\u0026gt;% clean_names) ## [1] \u0026#34;state\u0026#34;  ## [2] \u0026#34;year\u0026#34;  ## [3] \u0026#34;total_employed_rn\u0026#34;  ## [4] \u0026#34;employed_standard_error_percent\u0026#34;  ## [5] \u0026#34;hourly_wage_avg\u0026#34;  ## [6] \u0026#34;hourly_wage_median\u0026#34;  ## [7] \u0026#34;annual_salary_avg\u0026#34;  ## [8] \u0026#34;annual_salary_median\u0026#34;  ## [9] \u0026#34;wage_salary_standard_error_percent\u0026#34;  ## [10] \u0026#34;hourly_10th_percentile\u0026#34;  ## [11] \u0026#34;hourly_25th_percentile\u0026#34;  ## [12] \u0026#34;hourly_75th_percentile\u0026#34;  ## [13] \u0026#34;hourly_90th_percentile\u0026#34;  ## [14] \u0026#34;annual_10th_percentile\u0026#34;  ## [15] \u0026#34;annual_25th_percentile\u0026#34;  ## [16] \u0026#34;annual_75th_percentile\u0026#34;  ## [17] \u0026#34;annual_90th_percentile\u0026#34;  ## [18] \u0026#34;location_quotient\u0026#34;  ## [19] \u0026#34;total_employed_national_aggregate\u0026#34;  ## [20] \u0026#34;total_employed_healthcare_national_aggregate\u0026#34; ## [21] \u0026#34;total_employed_healthcare_state_aggregate\u0026#34;  ## [22] \u0026#34;yearly_total_employed_state_aggregate\u0026#34;   Did you see what happened? White spaces were converted to _ and parantheses were removed. Even the % signs were converted to percent. Now, these labels are easy to understand AND process by code. This does not mean that you are finished cleaning but at least now the columns are more accessible.\nRemove empty and or constant columns and rows Data sets come with empty or superfluous rows or columns are not a rare sighting. This is especially true if you work with Excel files because there will be a lot of empty cells. Take a look at the dirty Excel data set from janitor\u0026rsquo;s GitHub page. It looks like this when you open it with Excel.\nTaking a look just at this picture we may notice a couple of things.\n  First, Jason Bourne is teaching at a school. I guess being a trained assassin qualifies him to teach physical education. Also - and this is just a hunch - undercover work likely earned him his \u0026ldquo;Theater\u0026rdquo; certification.\n  Second, the header above the actual table will be annoying, so we must skip the first line when we read the data set.\n  Third, the column names are not ideal but we know how to deal with that by now.\n  Fourth, there are empty rows and columns we can get rid of.\n  Fifth, there is a column that contains only \u0026lsquo;YES\u0026rsquo;. Therefore it contains no information at all and can be removed.\n  So, let us read and clean the data. The janitor package will help us with remove_empty() and remove_constant().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  xl_file \u0026lt;- readxl::read_excel(\u0026#39;dirty_data.xlsx\u0026#39;, skip = 1) %\u0026gt;% clean_names() %\u0026gt;% remove_empty() %\u0026gt;% remove_constant() xl_file ## # A tibble: 12 x 9 ## first_name last_name employee_status subject hire_date percent_allocated ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jason Bourne Teacher PE 39690 0.75 ## 2 Jason Bourne Teacher Drafting 43479 0.25 ## 3 Alicia Keys Teacher Music 37118 1  ## 4 Ada Lovelace Teacher \u0026lt;NA\u0026gt; 38572 1  ## 5 Desus Nice Administration Dean 42791 1  ## 6 Chien-Shiung Wu Teacher Physics 11037 0.5  ## 7 Chien-Shiung Wu Teacher Chemistry 11037 0.5  ## 8 James Joyce Teacher English 36423 0.5  ## 9 Hedy Lamarr Teacher Science 27919 0.5  ## 10 Carlos Boozer Coach Basketball 42221 NA  ## 11 Young Boozer Coach \u0026lt;NA\u0026gt; 34700 NA  ## 12 Micheal Larsen Teacher English 40071 0.8  ## # ... with 3 more variables: full_time \u0026lt;chr\u0026gt;, certification_9 \u0026lt;chr\u0026gt;, ## # certification_10 \u0026lt;chr\u0026gt;   Here, remove_empty() defaulted to remove, both, rows and colums. If we wish, we can change that by setting e.g. which = 'rows'.\nNow, we may also want to see the hire_data in a sensible format. For example, in this dirty data set, Jason Bourne was hired on 39690. Luckily, our janitor can make sense of it all.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  xl_file %\u0026gt;% mutate(hire_date = excel_numeric_to_date(hire_date)) ## # A tibble: 12 x 9 ## first_name last_name employee_status subject hire_date percent_allocat~ ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Jason Bourne Teacher PE 2008-08-30 0.75 ## 2 Jason Bourne Teacher Drafting 2019-01-14 0.25 ## 3 Alicia Keys Teacher Music 2001-08-15 1  ## 4 Ada Lovelace Teacher \u0026lt;NA\u0026gt; 2005-08-08 1  ## 5 Desus Nice Administration Dean 2017-02-25 1  ## 6 Chien-Shiung Wu Teacher Physics 1930-03-20 0.5  ## 7 Chien-Shiung Wu Teacher Chemistry 1930-03-20 0.5  ## 8 James Joyce Teacher English 1999-09-20 0.5  ## 9 Hedy Lamarr Teacher Science 1976-06-08 0.5  ## 10 Carlos Boozer Coach Basketball 2015-08-05 NA  ## 11 Young Boozer Coach \u0026lt;NA\u0026gt; 1995-01-01 NA  ## 12 Micheal Larsen Teacher English 2009-09-15 0.8  ## # ... with 3 more variables: full_time \u0026lt;chr\u0026gt;, certification_9 \u0026lt;chr\u0026gt;, ## # certification_10 \u0026lt;chr\u0026gt;   Rounding To my surprise shock, R uses some unexpected rounding rule. In my world, whenever a number ends in .5, standard rounding would round up. Apparently, R uses something called banker\u0026rsquo;s rounding that in these cases rounds towards the next even number. Take a look.\n1 2  round(seq(0.5, 4.5, 1)) ## [1] 0 2 2 4 4   I would expect that the rounded vector contains the integers from one to five. Thankfully, janitor offers a convenient rounding function.\n1 2  round_half_up(seq(0.5, 4.5, 1)) ## [1] 1 2 3 4 5   Ok, so that gives us a new function for rounding towards integers. But what is really convenient is that janitor can round_to_fractions.\n1 2  round_to_fraction(seq(0.5, 2.0, 0.13), denominator = 4) ## [1] 0.50 0.75 0.75 1.00 1.00 1.25 1.25 1.50 1.50 1.75 1.75 2.00   Here, I rounded the numbers to the next quarters (denominator = 4) but of course any fraction is possible. You can now live the dream of rounding towards arbitrary fractions.\nFind matches in multiple characteristics In my opinion, the get_dupes() function is really powerful. It allows us to find \u0026ldquo;similar\u0026rdquo; observations in a data set based on certain characteristics. For example, the starwars data set from dplyr contains a lot of information on characters from the Star Wars movies. Possibly, we want to find out which characters are similar w.r.t. to certain traits.\n1 2 3 4 5 6 7 8 9 10 11 12 13  starwars %\u0026gt;% get_dupes(eye_color, hair_color, skin_color, sex, homeworld) %\u0026gt;% select(1:8) ## # A tibble: 7 x 8 ## eye_color hair_color skin_color sex homeworld dupe_count name height ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 blue black yellow female Mirial 2 Luminara U~ 170 ## 2 blue black yellow female Mirial 2 Barriss Of~ 166 ## 3 blue blond fair male Tatooine 2 Luke Skywa~ 172 ## 4 blue blond fair male Tatooine 2 Anakin Sky~ 188 ## 5 brown brown light female Naboo 3 Cord√© 157 ## 6 brown brown light female Naboo 3 Dorm√© 165 ## 7 brown brown light female Naboo 3 Padm√© Amid~ 165   So, Luke and Anakin Skywalker are similar to one another. Who would have thought that. Sadly, I don\u0026rsquo;t enough about Star Wars to know whether the other matches are similarly \u0026ldquo;surprising\u0026rdquo;. In any case, the point here is that we can easily find matches according to arbitrarily many characteristics. Conveniently, these characteristics are the first columns of the new output and we get a dupe_count.\nAlright, this concludes our little showcase. In the janitor package, there is another set of tabyl() functions. These are meant to improve base R\u0026rsquo;s table() functions. Since I rarely use that function I did not include it but if you use table() frequently, then you should definitely check out tabyl().\n","description":"I demonstrate a couple of functions from the janitor package I find quite useful","id":4,"section":"post","tags":[],"title":"Showcasing the janitor package","uri":"https://albert-rapp.de/post/2022-01-12-janitor-showcase/"},{"content":"TidyTuesday, the weekly social data project that brings together R users, is a great way to connect to the R community and learn to wrangle and visualize data. But more importantly, it is a superb chance to learn new data visualization skills by doing thieving. Let me elaborate.\nEach week, you get a chance to work with a new data set and create a (hopefully) nice visualization1. Afterwards, you can share visualizations with the world on twitter using #tidyTuesday. Of course, being the curious person that you are, you check out contributions from other fellow R users. And more often than not, you will see really cool visualizations and wish that you could do something like that too. And you can!\nUsually, people share their code together with their viz. Consequently, you are only one ctrl-C away from stepping up your dataviz game. Do I mean that you should take the entire code and brand that as your own work? Of course not! But you can maybe ctrl-C aspects of the code and reuse it for something you have been wanting to do for a long time. Let\u0026rsquo;s make this specific. Last week, I found this gem by Georgios Karamanis.\nTransphobic hate crimes in Sweden for this week\u0026#39;s Bring Your Own Data #TidyTuesday.\nThe inspiration was a plot made by @thomasoide for this Axios article: https://t.co/zMrnr9tszG\nSource: @myndigheten_bra\ncode: https://t.co/HSCew2zrUg\n#Rstats #dataviz pic.twitter.com/IVQ1wTBZmt\n\u0026mdash; Georgios Karamanis (@geokaramanis) January 5, 2022  What intrigued me were the bars with criss-cross lines. Now, clearly I want to be able to do that too. Luckily, the tweet also contains a link to the corresponding GitHub repository. Et voil√†, a quick glance at the code reveals the use of a so-called ggpattern package and a quick ctrl-C of the package name combined with a internet search leads me to the package\u0026rsquo;s documentation.\nThere, I find out that it is quite easy to get bars with different patterns2 using geom_col_pattern(). For example, these code snippets are taken straight from the documentation (more ctrl-Cs). For more, check out the documentation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  library(ggplot2) library(ggpattern) library(patchwork) df \u0026lt;- data.frame(level = c(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#39;d\u0026#39;), outcome = c(2.3, 1.9, 3.2, 1)) stripes \u0026lt;- ggplot(df) + geom_col_pattern( aes(level, outcome, pattern_fill = level), pattern = \u0026#39;stripe\u0026#39;, fill = \u0026#39;white\u0026#39;, colour = \u0026#39;black\u0026#39; ) + theme_bw(18) + theme(legend.position = \u0026#39;none\u0026#39;) kittens \u0026lt;- ggplot(df) + geom_col_pattern( aes(level, outcome, fill = level), pattern = \u0026#39;placeholder\u0026#39;, pattern_type = \u0026#39;kitten\u0026#39;, pattern_size = 3 ) + theme_bw(18) + theme(legend.position = \u0026#39;none\u0026#39;) stripes + kittens   There you go. So, now I \u0026ldquo;can do\u0026rdquo; bars with different patterns. \u0026ldquo;Hold on, it is not like you are totally an expert now. How does any of that help?\u0026quot;, you might think. And, clearly you are right. Having emulated something I saw online, does not make me exactly into an visual artist but now I am equipped with one more tool to try out come next TidyTuesday.\nRepeat that often enough and soon you have acquired a lot of tools to use in diverse settings. Eventually, the lines between \u0026ldquo;I copied what I found online\u0026rdquo; and \u0026ldquo;This is a trick I like to do frequently\u0026rdquo; blur. In the end, repeated practice and learning from others is what makes you into an expert. And sometimes that \u0026ldquo;learning from others\u0026rdquo; part is as simple as strolling through GitHub repositories on the lookout for your next great coup.\n Honestly, it does not really matter if your visualization is looking \u0026ldquo;nice\u0026rdquo;. I have ended up sharing a bunch of, say, average at best visualizations. (Exhibit A, Exhibit B). The point is too keep showing up and trying. In fact, even the visualizations I am not totally proud of contain elements which I have spent a lot of time working on. This practice has often ended up helping me in unexpected situations.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I know, I know. The tweet was using geom_rect_pattern(). Not exactly the same but the principles are.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I advocate to take part in the TidyTuesday events to learn with and from others.","id":5,"section":"post","tags":["visualization","opinion"],"title":"ggplot-tips: Learning by Thieving","uri":"https://albert-rapp.de/post/2022-01-10-learning-by-thieving/"},{"content":"It is almost the beginning of a new year and I have decided to finish off this year with a quick blog post. Also, friends were shaming me that I have been slacking off on this blog lately. Therefore, let\u0026rsquo;s get started right away. We\u0026rsquo;ll keep things simple and look at a few cool plots from the ggforce package. Of course, we have already glimpsed at this package in the previous installment of this ggplot2-tips series.\nMark Point Plots Let us first take a look at the penguins data set from the palmerpenguins package. Same as last time, this will be the dummy data set we use for plots but of course any other data set would be fine too.\n1 2 3 4 5 6 7 8 9  library(dplyr) library(ggplot2) theme_set(theme_light()) dat \u0026lt;- palmerpenguins::penguins %\u0026gt;% filter(!is.na(sex)) p \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, col = species)) + geom_point() p   Visually, we can see that the points are strongly grouped by species which makes sense as these kind of measurements often define a species. With help from ggforce we can visually emphasize this grouping by drawing rectangles or ellipses around the groups.\n1 2 3 4 5 6 7 8  library(ggforce) rect_plot \u0026lt;- p + geom_mark_rect(size = 1) ellipse_plot \u0026lt;- p + geom_mark_ellipse(aes(fill = species), alpha = 0.25) library(patchwork) # see last ggplot2-tips post rect_plot / ellipse_plot   There is also a geom_mark_hull() function that requires the concaveman package to be installed. Using this function, we can draw a hull around the points.\n1 2  p + geom_mark_hull(size = 1, concavity = 3)   Beware though that this hull is \u0026ldquo;redrawn at draw time\u0026rdquo;, so your hull may look different when you zoom into the plot. Also, let me point out that geom_mark_hull() has an argument concavity that allows you to make the hull \u0026ldquo;more wiggly\u0026rdquo;.\nAlluvial Plots With ggforce you can easily draw so-called alluvial plots. Originally, these are used to visualize a \u0026ldquo;stream over time\u0026rdquo; as for instance shown on Wikipedia. But the same visualization can be used to visualize \u0026ldquo;composition of groups\u0026rdquo; like so.\nFrom this plot, it is clear that unsurprisingly most of high weight penguins are male. What is maybe more surprising is that all Chinstrap penguins live on Dream. Obviously, the first layer in this alluvial plot is sort of redundant as the color already codes the sex but for accessibility it is often encouraged to use some form of double encoding (e.g. different shape AND color for groups). Thus, I find it practical and somewhat convenient to add this first layer.\nCreating this plot requires a couple of steps but ggforce has useful functions that make our life easier. More precisely we will need to\n count occurences in each subgroup and convert this in a suitable format for later plotting. gather_set_data() will help us doing that. draw lines between subgroups with geom_parallel_sets() draw boxes to identify subgroups with geom_parallel_sets_axes() label the boxes with geom_parallel_sets_labels  The first step is processed as follows\n1 2 3 4 5 6 7 8 9  reshaped_dat \u0026lt;- dat %\u0026gt;% mutate( mass_group = factor( cut_number(body_mass_g, 3), labels = c(\u0026#34;high\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;low\u0026#34;) ) ) %\u0026gt;% count(species, island, sex, mass_group) %\u0026gt;% gather_set_data(x = 1:4)   This simply counts the occurences in each subgroup and then adds three columns x, y and id based on the subgroup labels. These three new columns are necessary for generating the plot which is done as follows\n1 2 3 4 5 6 7 8 9 10  reshaped_dat %\u0026gt;% ggplot(aes( x = x, split = y, id = id, value = n )) + geom_parallel_sets(aes(fill = sex), alpha = 0.5) + geom_parallel_sets_axes(axis.width = 0.2) + geom_parallel_sets_labels(colour = \u0026#39;white\u0026#39;, size = 4)   Here, value is the counts of the subgroups. Also, notice that the splits on the x-axis is not in the same order as in my original plot. The order can be easily changed by converting x to a factor whose levels have the desired ordering. The complete code is\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  reshaped_dat %\u0026gt;% ggplot(aes( x = factor(x, c(\u0026#34;sex\u0026#34;, \u0026#34;species\u0026#34;, \u0026#34;island\u0026#34;, \u0026#34;mass_group\u0026#34;)), split = y, id = id, value = n )) + geom_parallel_sets(aes(fill = sex), alpha = 0.5) + geom_parallel_sets_axes(axis.width = 0.2) + geom_parallel_sets_labels(colour = \u0026#39;white\u0026#39;, size = 4) + labs(x = element_blank()) + scale_y_continuous(breaks = NULL) + theme(text = element_text(size = 12)) + scale_fill_brewer(palette = \u0026#39;Set1\u0026#39;)   Voronoi Diagrams Next, let us explore Voronoi diagrams. These are constructed from a set of \u0026ldquo;center points\u0026rdquo; which are used to form polygons such that these fill the whole plane and each polygons consists of the points that are closest to a polygon\u0026rsquo;s center point. If you found this somewhat confusing, then you are in luck because Wikipedia has a super neat animation that illustrates this concept.\nUsing bill and flipper lengths to define the center points' x- and y-coordinates, we can create a Voronoi diagram via geom_voronoi_tile() and geom_voronoi_segment() as follows.\n1 2 3 4 5 6  dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, group = 1)) + geom_voronoi_tile(aes(fill = species)) + geom_voronoi_segment() + scale_fill_brewer(palette = \u0026#34;Set1\u0026#34;) + theme_void()   Here, the lines between polygons are shown due to geom_voronoi_segment() and if we wish to get rid of the lines we can simply remove this layer. Also, let us ignore possible applications of Voronoi diagrams1 for a bit. What I really wanted to demonstrate is a small bit of Rtistry I found on Twitter and found really cool.\nWith a couple of random numbers and a bit of coloring one can create some visually appealing graphics (at least I like to think so). First, let\u0026rsquo;s take a look at only a few random numbers\n1 2 3 4 5 6 7 8  set.seed(23479) N \u0026lt;- 25 tibble(x = runif(N), y = runif(N)) %\u0026gt;% ggplot(aes(x, y)) + geom_voronoi_tile(aes(fill = y)) + scale_fill_viridis_c(option = \u0026#39;A\u0026#39;) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;)   Not so super impressive but using many random numbers a \u0026ldquo;smoother\u0026rdquo; picture will be created,\n1 2 3 4 5 6 7 8  set.seed(23479) N \u0026lt;- 1000 tibble(x = runif(N), y = runif(N)) %\u0026gt;% ggplot(aes(x, y)) + geom_voronoi_tile(aes(fill = y)) + scale_fill_viridis_c(option = \u0026#39;A\u0026#39;) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;)   Of course, arranging the center points differently and using other colors leads to very different pictures.\n1 2 3 4 5 6 7 8  set.seed(23479) N \u0026lt;- 1000 tibble(x = runif(N, -1, 1), y = sqrt(abs(x) + runif(N))) %\u0026gt;% ggplot(aes(x, y)) + geom_voronoi_tile(aes(fill = y)) + scale_fill_viridis_c(option = \u0026#39;E\u0026#39;) + theme_void() + theme(legend.position = \u0026#39;none\u0026#39;)   Sina Plots Coming back to less artistic plots, consider the following violin plots from the ggplot2 package.\n1 2 3  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_violin(fill = \u0026#34;grey80\u0026#34;)   Compared with common boxplots, these kind of plots show the distribution of the data more explicitly with density estimates (rotated by 90 degrees and mirrored for symmetry). This gets rid of the intrinsic problem of boxplots, i.e. only showing quantiles. Sometimes though, we want to see the quantiles as well. In these instances, an additional boxplot is plotted within the violin plots like so.\n1 2 3 4  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_violin(fill = \u0026#34;grey80\u0026#34;) + geom_boxplot(width = 0.25)   However, even with both of these plots combined we still don\u0026rsquo;t know how many points are in this data set. To make that information available in the visualizations, so-called sina plots fill the area of violin plots with jittered data points instead of depicting the estimated density directly.\n1 2 3  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_sina()   If a data set is large, then the points will display the same contour as the violin plot. In any case, the violin plot can be plotted beneath the points as well for better visibility.\n1 2 3 4  dat %\u0026gt;% ggplot(aes(x = species, y = body_mass_g)) + geom_violin(fill = \u0026#34;grey80\u0026#34;) + geom_sina()   This way, we can see both the distribution AND the number of data points in a single plot. Of course, there are more ways to display the distribution of data and ggdist is just the right package to do that job. I will show you that particular package in the next installment of the ggplot2-tips series.\nAnd that concludes our small demonstration of a few ggforce functions. For more functions check out ggforce\u0026rsquo;s website. For sure, there is more cool stuff like Bezier curves and facet zooms to explore.\nFinally, here is an overview of all the cool visuals we have created. Let me know what you think in the comments or simply hit the applause button below if you liked the content.\n See Wikipedia if you\u0026rsquo;re interested in a list of applications.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"The ggplot2-tips series is continued with a few example plots from the ggforce package","id":6,"section":"post","tags":[],"title":"A couple of visualizations from ggforce","uri":"https://albert-rapp.de/post/2021-12-31-ggforce-examples/"},{"content":"A couple of weeks back, I wanted to explain to my student what I mean when I talk about the \u0026ldquo;variance of the sample variance\u0026rdquo;. In my head, this term sounds quite confusing and contains the word \u0026ldquo;variance\u0026rdquo; at least one too many times. But as I was not sure whether my subsequent explanation really came through, I decided to let my students explore the notion on their own through a Shiny app.\nHonestly, I thought this would be quite simple to code because I have already learned the basics of Shiny when I wanted to show my students what exciting web developmental things R can do. Back then, I summarized the basics in one chapter of my YARDS lecture notes.\nHowever, even though the idea of my app was simple, I soon came to realize that I would need to learn a couple more Shiny-related things to get the job done. And, as is usual with coding, I did this mostly by strolling through the web in order to find code solutions for my particular problems. Most of the time, I consulted Hadley Wickham\u0026rsquo;s Mastering Shiny but still I ended up searching for a lot of random other stuff on the web.\nConsequently, I decided that it might be nice to collect what I have learned in one place. So, here is a compilation of loosely connected troubles I solved during my Shiny learning process. May this summary serve someone well.\nUse a theme for simple customization Let\u0026rsquo;s start with something super easy. If you wish to customize the appearance of you app, you can set the theme argument of fluidPage() to either a CSS-file that contains the necessary configuration (this is the hard way) or use a theme from bslib::bs_theme(). The latter approach comes with a lot of named preimplemented themes and is easily implemented by bootswatch = \u0026quot;name\u0026quot;. In my app, I have simply added theme = bslib::bs_theme(bootswatch = \u0026quot;superhero\u0026quot;). For other themes, have a look at RStudio\u0026rsquo;s Shiny themes page.\nCheck out this super simple example that I have adapted from the default \u0026ldquo;new Shiny app\u0026rdquo; output (you will actually have to copy and run this in an R script on your own).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  library(shiny) library(tidyverse) ui \u0026lt;- fluidPage( # Theme added here theme = bslib::bs_theme(bootswatch = \u0026#34;superhero\u0026#34;), titlePanel(\u0026#34;Old Faithful Geyser Data\u0026#34;), sidebarLayout( sidebarPanel( sliderInput(\u0026#34;bins\u0026#34;, \u0026#34;Number of bins:\u0026#34;, min = 1, max = 50, value = 30) ), mainPanel( plotOutput(\u0026#34;distPlot\u0026#34;) ) ) ) server \u0026lt;- function(input, output) { output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } shinyApp(ui = ui, server = server)   During the course of this text, we will extend this small example bit by bit. But, I want to avoid copy-and-pasting code each time we change something. Thus, for the remaining examples I will only describe the changes to the previous version instead of pasting the whole code. Nevertheless, I will provide links after each example so that each script can be downloaded at will. The current example can be found here.\nIsolate slider from reactivity As is currently intended, our app\u0026rsquo;s histogram changes whenever the slider is moved. Sometimes, though, this is not what we wish to do. Instead, we may want to delay the rendering of the plot until a button is clicked.\nThis can be achieved through a simple isolate() command which, well, isolates whatever is in between the function\u0026rsquo;s parentheses from changes on the UI. Here, let us put input$bins into the isolate() function and check what happens when we move the slider (full code here), i.e. we changed\n1  bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1)   Excellent! Nothing happens when we move the slider. Dumb and useless but excellent anyway.\nObserve that we could have also put the whole renderPlot() function call into isolate(). This app would work in the sense that we created valid code but then the reactivity of the slider is still active. The isolate() documentation hints at this with \u0026ldquo;\u0026hellip;if you assign a variable inside the isolate(), its value will be visible outside of the isolate()\u0026rdquo;.\nCreate and observe Buttons Let us bring back some reactivity to our app by adding a button that reevaluates our histogram when clicked. First, we will add a button to the UI. Second, we will implement what needs to happen on the server side of things when the button is clicked.\nThe first step is pretty simple. All we have to do is add actionButton() to the UI. Same as sliderInput() we have to specify a inputId and label for the button. Here, we could add\n1  actionButton(\u0026#34;draw_button\u0026#34;, \u0026#34;Reevaluate!\u0026#34;, width = \u0026#34;100%\u0026#34;)   Then, on the server side we will have to catch each click on the button. Once a click is registered, the plot is supposed to be rendered again. We do this with observeEvent() which expects an event expression and a handler expression. In our case, the former is simply the id of our button, i.e. input$draw_button, and the latter is what code is to be executed when the event is observed. Therefore, we move our code for rendering the plot into this part of observeEvent(). Thus, in our server function we now have\n1 2 3 4 5 6 7 8 9  observeEvent( input$draw_button, { output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   Notice that we have wrapped our code into {}. Strictly speaking, this is not necessary because we only \u0026ldquo;do one thing\u0026rdquo; but, of course, we can easily imagine that we want to tie multiple calculations to a button click. In this case, we will need to wrap all commands into {}. In any case, our code now does what we expect it to do and on each click a new histogram is rendered using the current value of the slider input. This new app\u0026rsquo;s complete code can be found here.\nUse eventReactive() as an alternative for updating values Honestly, this part I learned just 5 minutes ago while I was writing the last section of this blog post. When I looked into the documentation of observeEvent(), I noticed that there is also a function eventReactive() which may be better suited for our current use case as it allows us to avoid manually isolating input$bins.\nThis new function works similar to observeEvent() but it creates a reactive variable instead. This, we can use for rendering. Check this out\n1 2 3 4 5 6 7 8 9  plot \u0026lt;- eventReactive( input$draw_button, { x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) } ) output$distPlot \u0026lt;- renderPlot({plot()})   Notice how we do not use isolate() anymore and use the plot variable like a reactive in renderPlot(), i.e. we have to \u0026ldquo;call\u0026rdquo; its value with ().\nHowever, be aware that eventReactive() creates a reactive variable such that you cannot change, say, multiple plots at once. Nevertheless, eventReactive() can be a great way to tie a plot to an event. So, I guess it dependes on your use case and personal preference if you want to use eventReactive() rather than observeEvent(). Anyway, this version\u0026rsquo;s code can be copied from here.\nUse reactiveVal() to manually change values on click Another neat function is reactiveVal() which helps you to construct for instance counters that increase on the click of a button. We can initialize a reactive value by writing\n1  counter \u0026lt;- reactiveVal(value = 0)   within the server function. This way, our counter is set to zero and we can update it and set it to, say, one by calling counter(value = 1). The current value of the counter can be accessed through counter().\nClearly, we can tie the updating of a reactive value to an event that we observe through observeEvent(). For instance, we count how often the draw button in our small app is clicked by changing our previous observeEvent(input$draw_button, ...). Here, we would change this particular line of code to\n1 2 3 4 5 6 7 8 9 10 11 12  observeEvent( input$draw_button, { tmp \u0026lt;- counter() counter(tmp + 1) output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   Finally, we can show this information on our UI for demonstration purposes by adding a textOutput(\u0026quot;demonstration_text\u0026quot;) to our UI and setting\n1 2 3 4 5  output$demonstration_text \u0026lt;- renderText(paste( \u0026#34;You have clicked the draw button\u0026#34;, counter(), \u0026#34;times. Congrats!\u0026#34; ))   The complete app can be found here.\nUse tabsetPanel and unique plot names Often, you do not want to display all information at once. In my particular case, I wanted to show only one out of two plots based on the user\u0026rsquo;s chosen estimator (sample mean or sample variance). A great way to achieve that is to use tabsetPanel() in the UI.\nOrdinarily, you can create a UI this way by setting\n1 2 3 4 5 6 7  mainPanel( tabsetPanel( tabPanel(\u0026#34;Plot\u0026#34;, plotOutput(\u0026#34;plot\u0026#34;)), tabPanel(\u0026#34;Summary\u0026#34;, verbatimTextOutput(\u0026#34;summary\u0026#34;)), tabPanel(\u0026#34;Table\u0026#34;, tableOutput(\u0026#34;table\u0026#34;)) ) )   This was an example taken straight out of the documentation of tabsetPanel(). What you will get if you start an app containing a UI like this is a panel with three tabs (each one corresponding to a plot, text or table output) and the user can click on the tabs to switch between the views. This isn\u0026rsquo;t that surprising.\nHowever, if we also add an id to this and set type to hidden, like so\n1 2 3 4 5 6 7 8 9  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;Plot\u0026#34;, plotOutput(\u0026#34;plot\u0026#34;)), tabPanel(\u0026#34;Summary\u0026#34;, verbatimTextOutput(\u0026#34;summary\u0026#34;)), tabPanel(\u0026#34;Table\u0026#34;, tableOutput(\u0026#34;table\u0026#34;)) ) )   then, by default, the user does not have the options to change between views by clicking on tabs. Now, the view will need to change based on other interactions of the user with the UI. This change will then need to be customized within the server function. This is where the id argument comes into play because it allows ourselves to address the tabs via updateTabsetPanel().\nHere, let us take our previous example and display the same information on a different panel, i.e. at the end we will have two panels with exactly the same information in each tab. I know. This is not particularly exciting or meaningful but it serves our current purpose well.\nNaively, we might implement our user-interface like so\n1 2 3 4 5 6 7 8 9 10 11 12  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;panel1\u0026#34;, { # UI commands from before here }), tabPanel(\u0026#34;panel2\u0026#34;, { # UI commands from before here }), ) )   However, we will have to be careful! If we simply copy-and-paste our UI from before, then we won\u0026rsquo;t have unique identifiers to address e.g. the draw button or the plot output. Since this is a serious NO-NO (all caps for dramatic effect) and the app won\u0026rsquo;t work properly, let us instead write a function that draws the UI for us but creates it with different identifiers like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  create_UI \u0026lt;- function(unique_part) { sidebarLayout( sidebarPanel( # unique label here by adding unique_part to bins sliderInput(paste(\u0026#34;bins\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Number of bins:\u0026#34;, min = 1, max = 50, value = 30), actionButton(paste(\u0026#34;draw_button\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Reevaluate!\u0026#34;, width = \u0026#34;100%\u0026#34;), actionButton(paste(\u0026#34;change_view\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Change view\u0026#34;, width = \u0026#34;100%\u0026#34;) ), mainPanel( textOutput(paste(\u0026#34;demonstration_text\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)), # Counter text added textOutput(paste(\u0026#34;countEvaluations\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)), plotOutput(paste(\u0026#34;distPlot\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)) ) ) }   Also, notice that I have created another button called \u0026ldquo;Change view\u0026rdquo; within the UI. Further, this button\u0026rsquo;s name is so mind-baffling that I won\u0026rsquo;t even try to elaborate what it will do. Finally, using create_UI, we can set up the UI like so\n1 2 3 4 5 6 7 8 9  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;panel1\u0026#34;, create_UI(\u0026#34;panel1\u0026#34;)), tabPanel(\u0026#34;panel2\u0026#34;, create_UI(\u0026#34;panel2\u0026#34;)), ) )   and address everything within the UI in a unique manner. Of course, such a functional approach only works well if the two panels look sufficiently similar such that it makes sense to design them through a single function. In my particular app that deals with the variance of estimators, this was the case because the tabs for the sample mean and sample variance were quite similar in their structure.\nNow that we have covered how the UI needs to be set up, let me show you how to change the view from one panel to the next. Shockingly, let us link this to a click on the \u0026ldquo;change view\u0026rdquo; button(s) like so\n1 2 3 4 5 6 7 8  observeEvent( input$change_view_panel1, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel2\u0026#34;) ) observeEvent( input$change_view_panel2, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;) )   Also, note that the previous code\n1 2 3 4 5 6 7 8 9 10 11 12  observeEvent( input$draw_button, { tmp \u0026lt;- counter() counter(tmp + 1) output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   won\u0026rsquo;t work anymore because the old identifiers like draw_button etc. need to be updated to draw_button_panel1 or draw_button_panel2. Clearly, this could potentially require some code duplication to implement the server-side logic for both tabs. But since we feel particularly clever today1, let us write another function that avoids a lot of code duplication.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  render_my_plot \u0026lt;- function(panel, counter, input, output) { tmp \u0026lt;- counter() # save current value of counter counter(tmp + 1) # update counter # Create identifier names bins_name \u0026lt;- paste(\u0026#34;bins\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) distplot_name \u0026lt;- paste(\u0026#34;distPlot\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) demonstration_text \u0026lt;- paste(\u0026#34;demonstration_text\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) # Render Plot output[[distplot_name]] \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(pluck(input, bins_name)) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) # Render counter text output[[demonstration_text]] \u0026lt;- renderText(paste( \u0026#34;You have clicked the draw button\u0026#34;, counter(), \u0026#34;times. Congrats!\u0026#34; )) }   Notice a few things here:\n Our function needs to know the objects counter, input and output to work. Also we need to switch to double-bracket notation for assigning new variables like distPlot_panel1 to output. Obviously, we couldn\u0026rsquo;t use $ for assignment anymore but single-bracket notation like output[var_name] is for some reason forbidden in Shiny. At least, that\u0026rsquo;s what an error message will kindly tell you when you dare to use only one bracket.  So, all in all our server-side logic looks like this now\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  server \u0026lt;- function(input, output) { # Counter initialization counter \u0026lt;- reactiveVal(value = 0) counter2 \u0026lt;- reactiveVal(value = 0) # Plot Rendering observeEvent( input$draw_button_panel1, { render_my_plot(\u0026#34;panel1\u0026#34;, counter, input, output) } ) observeEvent( input$draw_button_panel2, { render_my_plot(\u0026#34;panel2\u0026#34;, counter2, input, output) } ) # Panel Switching observeEvent( input$change_view_panel1, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel2\u0026#34;) ) observeEvent( input$change_view_panel2, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;) ) }   The complete app that we have just build can be found here.\nClosing Alright, I hope this helps you to build your own small Shiny app. In my particular case, I had to use another cool function from the shinyjs package to update the text on the UI such that it appears in red for a second (in order for the user to notice what changes). And because I have the feeling that shinyjs has way more in store for us, I will end this already quite long blog post here and save that (exciting) story for another time. Hope you will be there when I talk about shinyjs.\n And with that I really mean today. When I built my Shiny app, I actually used code duplication. But in hindsight, I feel somewhat embarrassed to leave it as it is for this blog post. Thus, I figured out how to make it work with a function.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recently built a small Shiny app where I had to search for how to do a lot of things. Here are 6 things I learned doing that. Maybe a Shiny beginner will find something useful in here.","id":7,"section":"post","tags":["shiny"],"title":"6 simple Shiny things I have learned from creating a somewhat small app","uri":"https://albert-rapp.de/post/2021-11-21-a-few-learnings-from-a-simple-shiny-app/"},{"content":"In this week\u0026rsquo;s TidyTuesday, I noticed that I am frequently not using only ggplot2 to create plots. In fact, it has become essential to me to leverage the powers of other great additional packages that align well with ggplot2. Therefore, I decided to extend my ggplot2-tips series by introducing a few packages I use quite often.\nIn this post, I want to cover how to arrange multiple plots. In particular, I will talk about the fantastic patchwork package by Thomas Lin Pedersen which helps to arrange plots quite intuitively. Further, I want to take a glance at ggforce, another package written by the same author as patchwork, because it also has a neat function for arranging plots. However, ggforce can do way more and I will demonstrate that in another installment of this series.\nSo, let us begin by creating a data set we want to fiddle with for plotting purposes. For simplicity, let us use the penguins data (without missing values) from the palmerpenguins package.\n1 2 3 4 5  library(tidyverse) theme_set(theme_light()) # All missing values can be filtered out by filtering the `sex` variable dat \u0026lt;- palmerpenguins::penguins %\u0026gt;% filter(!is.na(sex))   Arrange Plots via patchwork Often, we want to show multiple plots that tell a story when looked at together. Using patchwork, we can easily compose a single plot consisting of subplots. This is done by using the simple symbols + resp. / to display plots next to resp. on top of each other.\nFor demonstration purposes, let us generate a few simple plots.\n1 2 3 4  point_plot \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, fill = sex)) + geom_jitter(size = 3, alpha = 0.5, shape = 21) point_plot   1 2 3 4  point_plot2 \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, bill_depth_mm, fill = sex)) + geom_jitter(size = 3, alpha = 0.5, shape = 21) point_plot2   1 2 3 4 5  # plot_plot is obviously a fun name boxplot_plot \u0026lt;- dat %\u0026gt;% ggplot(aes(x = body_mass_g, fill = sex)) + geom_boxplot() boxplot_plot   Clearly, showing each plot separately is boring and may not tell a story convincingly. Possibly, here you may want to say that the length and depth measurements give no clear distinction between male and female penguins but the weight measurements offers a better distinguishabilty between sexes. Maybe, if we see all plots together, we can tell that story without boring the reader.\n1 2 3  library(patchwork) p \u0026lt;- (point_plot + point_plot2) / boxplot_plot p   See how I have used + to put the point plots next to each other and / to plot the boxplots below the two point plots. Obviously, that was super easy and neat. But this simple arrangement leads to a doubling of the legends which is somewhat bothersome. However, this is no cause for concern. plot_layout() is there to collect those legends for you.\n1  p + plot_layout(guides = \u0026#34;collect\u0026#34;)   Of course, this leaves you with two legends which is kind of superfluous. The easy way to get rid of this is to plot no legends for the boxplots.\n1 2 3  boxplot_plot \u0026lt;- boxplot_plot + guides(fill = \u0026#34;none\u0026#34;) p \u0026lt;- (point_plot + point_plot2) / boxplot_plot p + plot_layout(guides = \u0026#34;collect\u0026#34;)   Now, what about legend positioning? Well, we already know how that usually works for a single plot (via theme() in case you forgot) and the good news is that the exact same thing works with patchwork as well. But beware to apply an additional theme() layer to the whole plot and not just to the last plot added to our composed plot. To make sure that happens, we have to add this layer via \u0026amp;.\n1  p + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;)   By the same logic, we can make additional changes to the whole plot e.g. to change the color mapping.\n1 2 3 4  p + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   Next, let us control the layout a bit more and annotate the plot with plot_annotation().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  (point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3))) / boxplot_plot + plot_layout(guides = \u0026#34;collect\u0026#34;, heights = c(0.4, 0.6)) + plot_annotation( title = \u0026#34;Look at that arrangement!\u0026#34;, subtitle = \u0026#34;Wow\u0026#34;, caption = \u0026#34;Ol√†.\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;(\u0026#34;, tag_suffix = \u0026#34;)\u0026#34; ) \u0026amp; labs(fill = \u0026#34;Penguin sex\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   We did quite a lot here, so let\u0026rsquo;s recap:\n We changed the widths of the plots in the first row by passing a vector of relative widths to widths in plot_layout(). Same thing with heights in plot_layout() to make the boxplots larger. Renamed legend label with the regular labs() function. Added a title, subtitle, caption and tags to the whole plot with plot_annotation().  Also, if you want to have the tags to only label the upper and lower row, you may want to wrap the first row together by wrap_elements(). Think of this as creating a new single unit.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  wrapped_plots \u0026lt;- wrap_elements( point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3)) ) (wrapped_plots) / boxplot_plot + plot_layout(guides = \u0026#34;collect\u0026#34;, heights = c(0.4, 0.6)) + plot_annotation( title = \u0026#34;Look at that arrangement!\u0026#34;, subtitle = \u0026#34;Wow\u0026#34;, caption = \u0026#34;Ol√†.\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;(\u0026#34;, tag_suffix = \u0026#34;)\u0026#34; ) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   Notice how the upper row reinstated the default colors and has two legends. This demonstrates how wrap_elements() made the plots \u0026ldquo;independent\u0026rdquo; from the overall theming via \u0026amp;, so to speak. On the bright side, there is no (C) tag anymore.\nUnsurprisingly, patchwork can do much more but for starters I think the previous examples will already get you quite far. They are you \u0026ldquo;80/20 leverage points\u0026rdquo;, if you will. But in order to add one more neat feature, let me finish our intro to patchwork by showing you how to create plots in plots via insets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Tweak boxplots a bit for better visual fit to point_plot plt \u0026lt;- boxplot_plot + theme_minimal() + coord_flip() + theme(plot.background = element_rect(fill = \u0026#34;grey80\u0026#34;)) point_plot + coord_cartesian(xlim = c(25, 60)) + inset_element( plt, left = 0.01, right = 0.4, top = 0.99, bottom = 0.6 )   Create Subplots via ggforce I really enjoy arranging plots with patchwork because, to me, the syntax feels quite intuitive (mostly). However, as you probably noticed, I had to design each subplot and arrange them by hand. Clearly, if I want to use a grid-like arrangement to display each combination of two variables from a given set of variables, this may become tedious.\nLuckily, there is the ggforce package that has a neat faceting function to accomplish just that. As was already mentioned above, apart from that, the ggforce package offers even more cool stuff which we will look at in a future blog post.\nWith facet_matrix() it becomes quite easy to get a grid of subplots to display multiple combinations of two variables. For instance, take a look at this.\n1 2 3 4 5 6 7  library(ggforce) dat %\u0026gt;% ggplot(aes(x = .panel_x, y = .panel_y, fill = sex)) + geom_point(alpha = 0.5, size = 2, shape = 21) + facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g) )   Now, while this is not a particular beautiful plot, it gives us a quick overview of interesting variables which might be great for an exploratory analysis. Notice how we had to use .panel_x and .panel_y as placeholder for the individual variables. We could use the geom_auto*() functions to avoid typing that as they default to the correct values for x and y. Consequently, we could have written\n1 2 3 4 5 6  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g) )   With a little bit of tweaking, we can make this plot more interesting. For example. it would be neat if we had density plots on the diagonal. No problem! Add another geom_autodensity() layer and make sure that facet_matrix() understands to map only this layer to the diagonal subplots.\n1 2 3 4 5 6 7 8  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1 geom_autodensity(alpha = 0.5, position = \u0026#34;identity\u0026#34;) + # Layer 2 facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g), layer.diag = 2 )   See how layer.diag = 2 maps the diagonal elements to the second line of geom_* code. Similarly, we can manipulate the content of the upper and lower triangle in this grid by changing layer.lower or layer.upper in facet_matrix(). Let\u0026rsquo;s add another layer to see that in action.\n1 2 3 4 5 6 7 8 9 10  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1 geom_autodensity(alpha = 0.75, position = \u0026#34;identity\u0026#34;) + # Layer 2 geom_hex(aes(x = .panel_x, y = .panel_y), alpha = 0.75) + # Layer 3 facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g), layer.diag = 2, layer.lower = 3 )   Last but not least, let me mention that we can also easily create what is called an \u0026ldquo;asymmetric grid\u0026rdquo; in ggforce by mapping rows and columns manually. This is great for having categorical variables on one axis and numerical variables on the other axis.\n1 2 3 4 5 6 7 8 9  dat %\u0026gt;% ggplot() + geom_boxplot( aes(x = .panel_x, y = .panel_y, group = .panel_x) ) + facet_matrix( cols = vars(sex, species), rows = vars(bill_depth_mm:body_mass_g) )   Beware that geom_boxplot() is a bit tricky as it requires the group argument to be explicitly set. Furthermore, if you want to add another aesthetic, e.g. fill, you will have to set group via interaction().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  dat %\u0026gt;% ggplot() + geom_boxplot( aes( x = .panel_x, y = .panel_y, fill = island, group = interaction(.panel_x, island) ) ) + facet_matrix( cols = vars(sex, species), rows = vars(bill_depth_mm:body_mass_g) )   This concludes our short summary of possibilities to arrange plots. In the next post of this ggplot2-tips series we will take a closer look at ggforce. I hope you enjoyed today\u0026rsquo;s blog post and I look forward to \u0026ldquo;see\u0026rdquo; you at my next blog post. In the meantime, feel free to leave a comment or a click on the applause button below.\n","description":"The patchwork and ggforce packages can be used to compose plots from multiple subplots. Let's have a look at how that works.","id":8,"section":"post","tags":["visualization"],"title":"ggplot tips: Arranging plots","uri":"https://albert-rapp.de/post/2021-10-28-extend-plot-variety/"},{"content":"Currently, I am quite curious about interactive plots which is why I am reading Scott Murray\u0026rsquo;s highly recommendable book on D3. For those of you who don\u0026rsquo;t know it, D3.js is a JavaScript library that is great for creating amazing interactive Data-Driven-Documents on the web.\nUnfortunately, compared to ggplot2, D3\u0026rsquo;s learning curve feels quite steep and I am not yet able to work with it yet. Fortunately, there are other interactive graphing libraries out there that I can use to get a first feel for creating interactive plots. For instance, there is another JavaScript library plotly.js, which is built on top of D3 and can be easily used in R through the plotly package.\nTherefore, I decided to play around with this R package in the hope of figuring out how it works. Also, I summarized what I played around with in this blog post by writing what I like to call an \u0026ldquo;exploratory introduction\u0026rdquo;.\nAs the name implies, this is not really a formal introduction to plotly and more of an experience report. Nevertheless, I suspect that this can be useful for people who have already knowledge about ggplot2 and want to get started with plotly as well.\nFurther, in case you do not want to read my informal commentary, you can also check out the video version of this blog post. Think of it as a summary of this blog post.\nAs of now, I plan on doing a similar exploratory introduction to r2d3 which is an R interface to D3. Possibly, this will then be combined with knowledge I gathered from Scott Murray\u0026rsquo;s book.\nOne Side Note before We Dive in According to Wikipedia, JavaScript \u0026ldquo;is one of the core technologies of the World Wide Web\u0026rdquo;. Surprisingly, JavaScript\u0026rsquo;s ubiquity on the interwebs also messed with this blog\u0026rsquo;s theme template such that font formatting was completely destroyed when I included an interactive plotly plot.\nTo work around this issue, I had to save the plots in an auxillary html-file and include it as a separate frame. This is why you will find that, here, all plots are saved in a variable (for later export) even if it does not really make sense to save it for plotting purposes.\nFinally, be aware that, for some reason, interactive plotly plots are quite large such that it might take some time to load them all.\nFrom ggplot2 to plotly Let us begin by transforming a ggplot to a plotly plot. Using a built-in function from the plotly package, it is straight-forward to convert a ggplot.\n1 2 3 4 5 6 7 8  library(tidyverse) library(plotly) p \u0026lt;- mpg %\u0026gt;% ggplot(aes(hwy, cty, fill = class)) + geom_jitter(shape = 21, size = 2, alpha = 0.5) plotly_p \u0026lt;- ggplotly(p) plotly_p   \rMarvel at what one can do here:\n Get additional information by hovering your cursor over a point Filter classes by clicking on the corresponding legend items Click and draw a rectangle with your cursor to zoom into the plot  But there is one thing that bothers me. If I move the cursor across the plot window, then the plotly options bar at the top of the window overlaps the \u0026ldquo;class\u0026rdquo; legend label. Can I fix this by moving the legend to the bottom?\n1 2  p \u0026lt;- p + theme(legend.position = \u0026#34;bottom\u0026#34;) ggplotly(p)   \rHmm, it appears that the legend cannot be moved by this. Do other changes in theme() get registered? Let\u0026rsquo;s check by applying a theme.\n1 2  p \u0026lt;- p + theme_light() ggplotly(p)   \rApparently, at least some changes will be conducted. Let\u0026rsquo;s see, if I can use layout() as shown in the getting started documentation of plotly to move the legend.\nThe R documentation of the layout() function is somewhat minimalistic. Basically, it refers to the online plotly reference manual. Using the table of content on that website, one can easily find the layout options.\nI believe the options xanchor and yanchor are exactly what I need as both of them can be set to options like left, bottom, etc. Though, I wonder why the value auto was not changed by ggplotly() as the initial plot p contains legend.position = \u0026quot;bottom\u0026quot;.\nIn any case, I am not a hundred percent sure what kind of syntax layout() requires. An example in the R documentaion would have been nice. Well, let\u0026rsquo;s try the straight-forward approach then.\n1 2 3 4 5 6  p %\u0026gt;% ggplotly() %\u0026gt;% layout(xanchor = \u0026#34;center\u0026#34;) ## Warning: \u0026#39;layout\u0026#39; objects don\u0026#39;t have these attributes: \u0026#39;xanchor\u0026#39; ## Valid attributes include: ## \u0026#39;font\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;uniformtext\u0026#39;, \u0026#39;autosize\u0026#39;, \u0026#39;width\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;margin\u0026#39;, \u0026#39;computed\u0026#39;, \u0026#39;paper_bgcolor\u0026#39;, \u0026#39;plot_bgcolor\u0026#39;, \u0026#39;separators\u0026#39;, \u0026#39;hidesources\u0026#39;, \u0026#39;showlegend\u0026#39;, \u0026#39;colorway\u0026#39;, \u0026#39;datarevision\u0026#39;, \u0026#39;uirevision\u0026#39;, \u0026#39;editrevision\u0026#39;, \u0026#39;selectionrevision\u0026#39;, \u0026#39;template\u0026#39;, \u0026#39;modebar\u0026#39;, \u0026#39;newshape\u0026#39;, \u0026#39;activeshape\u0026#39;, \u0026#39;meta\u0026#39;, \u0026#39;transition\u0026#39;, \u0026#39;_deprecated\u0026#39;, \u0026#39;clickmode\u0026#39;, \u0026#39;dragmode\u0026#39;, \u0026#39;hovermode\u0026#39;, \u0026#39;hoverdistance\u0026#39;, \u0026#39;spikedistance\u0026#39;, \u0026#39;hoverlabel\u0026#39;, \u0026#39;selectdirection\u0026#39;, \u0026#39;grid\u0026#39;, \u0026#39;calendar\u0026#39;, \u0026#39;xaxis\u0026#39;, \u0026#39;yaxis\u0026#39;, \u0026#39;ternary\u0026#39;, \u0026#39;scene\u0026#39;, \u0026#39;geo\u0026#39;, \u0026#39;mapbox\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;radialaxis\u0026#39;, \u0026#39;angularaxis\u0026#39;, \u0026#39;direction\u0026#39;, \u0026#39;orientation\u0026#39;, \u0026#39;editType\u0026#39;, \u0026#39;legend\u0026#39;, \u0026#39;annotations\u0026#39;, \u0026#39;shapes\u0026#39;, \u0026#39;images\u0026#39;, \u0026#39;updatemenus\u0026#39;, \u0026#39;sliders\u0026#39;, \u0026#39;colorscale\u0026#39;, \u0026#39;coloraxis\u0026#39;, \u0026#39;metasrc\u0026#39;, \u0026#39;barmode\u0026#39;, \u0026#39;bargap\u0026#39;, \u0026#39;mapType\u0026#39;   Well, that didn\u0026rsquo;t go as expected, but the warning displays valid attributes and legend is among them. Upon closer inspection, I also realize that, according to the reference manual, xanchor and yanchor have parent layout.legend. So, I guess, I will have to use this somehow. Maybe, pass a list to legend via layout()?\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(xanchor = \u0026#34;center\u0026#34;)) p_layout   \rAha! At least this did not crash again but the resulting plot does not look as I would expect it to. Let\u0026rsquo;s see what happens when we change yanchor as well and then we\u0026rsquo;ll go from there.\n1 2 3 4 5 6 7  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list( xanchor = \u0026#34;center\u0026#34;, yanchor = \u0026#34;bottom\u0026#34; )) p_layout   \rUnsurprisingly, this did not help at all. In retrospect, I don\u0026rsquo;t know how I could think that changing yanchor as well would magically cure things. Again, referring back to the manual (I should really read the descriptions instead of relying on the possible values), it appears that xanchor only sets a reference points for the option x. Same thing for yanchor and y.\nSo, how about changing x and y instead? Possible values for both options range from -2 to 3, so my best guess is that ranges from 0 to 1 refer to the window the points are plotted in. Consequently, moving the legend to the bottom could be as easy as using a positive x value and negative y value which are both close to zero.\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(x = 0.1, y = -0.1)) p_layout   \rNow, this brings us closer to what I had in mind when I set legend.position = \u0026quot;bottom\u0026quot; in theme(). Possibly, we can change the orientation of the legend from vertical to horizontal, tweak the x and y values a bit and then we\u0026rsquo;re there. Scrolling through the manual (again), reveals that there is an option orientation which can be set to h. This sounds promising.\n1 2 3 4 5 6 7 8  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list( x = 0.1, y = -0.2, orientation = \u0026#34;h\u0026#34; )) p_layout   \rNice! Finally, I am satisfied. Out of curiosity, let us investigate what had happened, if we had set orientation to \u0026quot;h\u0026quot; from the start.\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;)) p_layout   \rThis already looks nice enough, so our manual tweaking was not technically necessary. But then again, this does not recreate what legend.position = \u0026quot;bottom\u0026quot; usually does. Now that we understand how layout() works, we can roam the reference manual and try to tweak the legend box. Let\u0026rsquo;s try to change a couple of things. This does not have to be pretty, we only want to see how plotly works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, borderwidth = 3, bgcolor = \u0026#34;grey\u0026#34;, bordercolor = \u0026#34;red\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ), title = list( text = \u0026#34;Class\u0026#34;, side = \u0026#34;top\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ) ) ) ) p_layout   \rNote how we have used lists in lists (in lists) to customize the legend. Interestingly, our initial blunder of ignoring the \u0026ldquo;parent\u0026rdquo; resp. the hierarchy of options earlier helped to understand that as an option\u0026rsquo;s parent\u0026rsquo;s name gets longer, e.g. layout.legend.title.font, we will have to use more convoluted lists to change that option.\nCreating a Plotly Chart Manually Since we have learned how to tweak a plotly object, we might as well figure out how to create one without having to use ggplotly(). It is not that I want to avoid using ggplot altogether but, in principle, it cannot hurt if we can understand plotly\u0026rsquo;s implementation in the R package plotly.\nSo, from the book Interactive web-based data visualization with R, plotly, and shiny I gather that the plotly R package implements the JavaScript plotly.js library via a Grammar of Graphics approach. Thus, it works similar to ggplot2 in the sense that we can add layers of graphical objects to create a plot. In plotly\u0026rsquo;s case, we pass a plotly object from one add_* layer to the next (via %\u0026gt;%).\n1 2 3 4  plt \u0026lt;- mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class) plt   \rNotice the ~. These are used to ensure that the variables are mapped from the data (similar to aes() in ggplot2). Alternatively, one could also use x = mpg$hwy to create the same plot.\nBecause we can see a lot of overplotting, let us jitter the points. Unfortunately, I couldn\u0026rsquo;t find a built-in option for that. Therefore, let\u0026rsquo;s do the jittering manually.\n1 2 3 4 5 6 7 8 9 10 11 12  set.seed(123) jitter_hwy \u0026lt;- 2 jitter_cty \u0026lt;- 1 jittered_mpg \u0026lt;- mpg %\u0026gt;% mutate( hwy = hwy + runif(length(hwy), -jitter_hwy, jitter_hwy), cty = cty + runif(length(cty), -jitter_cty, jitter_cty) ) plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class) plt   \rRegarding the customization of the points aka markers, we can pass a list of options (taken from the reference manual again) to the marker argument in add_markers().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  set.seed(123) plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers( x = ~hwy, y = ~cty, color = ~class, marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) plt   \rAlternatively, and what I find surprisingly convenient, we can leave our initial plotly object as it is and pass it to the style() function. This functions works just like the layout() function we have seen before.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers( x = ~hwy, y = ~cty, color = ~class ) %\u0026gt;% style( marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) plt   Similarly, we could pass this along to layout() if we want to customize the legend box again.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  plt_layout \u0026lt;- plt %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, borderwidth = 3, bgcolor = \u0026#34;grey\u0026#34;, bordercolor = \u0026#34;red\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ), title = list( text = \u0026#34;Class\u0026#34;, side = \u0026#34;top\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ) ) ) ) plt_layout   \rInteresting! For some obscure reason the exact same legend command behaves differently now. Honestly, I have no clue what is going on here. If I had to hazard a guess, I would say that during the conversion of a ggplot object to a plotly object via ggplotly() some default values were implemented that cause the change but this is just a hunch. Possibly, this is connected to xanchor or yanchor.\nIn any case, we got a glimpse of how the plotly R package works which uses the JavaScript library plotly.js to create interactive plots. Also, we have learned how to convert a ggplot2 object to a plotly object and how we can customize this further.\nInterestingly, when converting from ggplot2 to plotly, the pop-up window that appears when you hover over a point is already customized compared to the default from plot_ly(). Did you notice the difference already?\nSo, in order to end our \u0026ldquo;exploratory introduction\u0026rdquo; let us adjust the hovertemplate according to the description in the reference manual. Here, we will use \\n for line breaks.\n1 2 3 4  plt \u0026lt;- plt %\u0026gt;% style(hovertemplate = \u0026#34;hwy: %{x:.2f}\\ncty: %{y:.2f}\u0026#34;) %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;, y = -0.2)) plt   \rNotice how the class labels appear outside of the box. The reference manual refers to this position as \u0026ldquo;secondary\u0026rdquo; box. To get rid of this, we simply add \u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt; to our hover template. Unfortunately, it appears as if what is not displayed in the primary box cannot be used as part of the hover template.\nThus, we cannot use %{color}. Instead, we simply map class to the text attribute of the markers as well. Then, we can use %{text}.\n1 2 3 4 5 6 7 8 9 10 11 12 13  plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class, text = ~class) %\u0026gt;% style( marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) %\u0026gt;% style(hovertemplate = \u0026#34;hwy: %{x:.2f}\\ncty: %{y:.2f}\\nclass: %{text} \u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt;\u0026#34;) %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;, y = -0.2)) plt   \rAll right! That\u0026rsquo;s enough exciting plotting action for today. Hope you enjoyed this blog post and see you next time.\n","description":"We try to do a few simple things with the plotly package in order to figure out how it works.","id":9,"section":"post","tags":["visualization","exploratory-intro"],"title":"An Exploratory Introduction to the Plotly Package","uri":"https://albert-rapp.de/post/2021-10-16-exploratory-intro-plotly/"},{"content":"A bit more than two weeks ago, Germany held a federal election and, naturally, this is always reason for a lot of discussions and subjective truths. One subjective truth I encountered myself related to how fast the party CDU/CSU was able to collect and lose votes according to polls right before the election.\nAccording to the Allensbach Institute, a private polling institute based in Allensbach, Baden-W√ºrttemberg, on July 20th, approximately three months before the official election, the CDU/CSU could get 31.5% of the votes1. Almost four weeks later on the 19th of August, the Allensbach institute forecast only 27.5% for the CDU/CSU.\nAt that time, I had the subjective feeling that it was quite common to assume that the CDU/CSU is on a steep downward spiral. In the end, the CDU/CSU was able to slow its downward course and got 24.1% of the votes in the election. While this is still an abysmal outcome for this party, I was surprised that it was not worse.\nA \u0026ldquo;similar\u0026rdquo; surprising tale can be told for other parties too. For instance, the party SPD was gaining a lot of steam in the last three months of the election campaign and the party DIE GR√úNE, once surprisingly popular during the election, lost a lot of votes towards the end as well.\nAll of these ups and downs left a feeling of rapid change for some. For example, last weekend I had an interesting discussion about whether voters no longer cast their votes according to \u0026ldquo;belief\u0026rdquo; but are much more influenced by the spur of the moment and flip-flop back and forth between parties depending on who is making the headlines at that time. Consequently, I decided that this might be something worth looking at with data.\nThus, this blog post tries to look at historic data from election polls to see if this year\u0026rsquo;s change before the election is indeed something unprecedented. If this is so, then that might support that voters become more impulsive. So, this is why I scraped election polls2 since 1998 from the Allensbach institute and the Kantar (Emnid) institute whose election polls can be found publicly here3.\nData Warning I believe it is worth pointing out that the polling results will have to be taken with a grain of salt. Especially the fact that people might judge their current preference differently in non-election years compared to election years has to be taken into account. Obviously, I suspect that the polling institutes considered this as part of their forecast but nevertheless it cannot hurt to mention potential caveats.\nPopularity Over Time This being said, let\u0026rsquo;s take a look at the six (currently) largest parties and their popularity over time. Here, I will only look at the data from the Allensbach institute as these are already quite a lot of data points and the picture might get messy otherwise. As this is an election-focused blog post, I took the liberty of labeling only the election years on the x-axis4.\nSome recent trends are detectable but in this particular figure, I don\u0026rsquo;t see anything that points to an increased volatility in recent times.\nWhat Happens Close to an Election? Instead of looking at the overall fluctuations, we could look at the last three months before an election. Since an election takes places in September, in the next plot I have depicted only the polling results in the months July, August and September in an election year. To make trends more visible, I have added a regression line for each party and each election.\nInterestingly, the most recent election seems to have had more volatile last three months compared to previous elections. Indeed, this could indicate more impulsive voteing behavior but I am not entirely convinced yet.\nThree-Month Volatility To see if the change in the last three months of the most recent election is truly something out of the ordinary, we need context. In order to get this context, let us consult our historical data again and compute the average share of votes for each party in every quarter of every year. Then, hopefully, these computed mean percentages represent the mood of the majority of people in a given quarter and we can see how much these means change from quarter to quarter.\nTaking the fluctuations over time into account, the quarterly change right before this year\u0026rsquo;s election looks less extreme. In fact, most of the parties have had more extreme or similar changes on a quarterly basis in the past.\nClearly, the fact that we aggregate the share of votes over a period of three months could potentially obscure fluctuations. But as the next plot shows, if we do the same thing but aggregate on a monthly basis, then the overall impression of the new monthly plot is the same as with the original quarterly plot.\nCoefficient of Variation Before we try to make sense of what all that we have seen could mean in terms of voters' impulsiveness, let us take one more stab at trying to measure the volatility. This time, let us compute the coefficient of variation (CV)5 of the mean weekly share of votes for each quarter and each party and display this over time.\nExcept for the AfD and SPD, no profound trend in the CV can be detected as most regression lines appear to have a slope that is close to zero. Further, the AfD\u0026rsquo;s decrease in its monthly CV might be explained by the fact that it is a comparatively new party which means that might not have had a solid voter base in the beginning.\nSince we are interested in more recent voter behavior, let us try to take only the data since the beginning of 2013 into account. This should give us an impression about the three most recent elections.\nOverall, as the confidence bands of the regression lines indicate that the real slope of the lines might as well be close to zero, I find it hard to argue either way about a more impulsive voting behavior.\nConclusion So, we have observed something odd here. In previous elections, a party\u0026rsquo;s changes in popularity in the last three months were not as profound as the last quarter\u0026rsquo;s changes that we witnessed this year. Overall, however, the degree of how much changed right before the election is nothing that could not be witnessed in the past at other non-election times.\nIf I had to guess, I would say that this might indicate that over the last 20 years, people\u0026rsquo;s willingness to vote for a different party remained somewhat similar. But it appears like the moment, when voters eventually decide for a party, has been moved closer to the election itself. Thus, one might argue that it has become harder to pinpoint which party one feels most connected to and this leads to an indecision right until the end. This indecision could indeed lead to a flipping back and forth between two or three parties one feels similarly connected to. But this, I would argue, is a sign of people lacking a strong connection to one single party and not one of impulsiveness.\nWhat do you think? Is there more that we can extract from this kind of data? As I am quite new to this type of analysis, I am always glad about suggestions about how to improve. If you want to share your ideas, feel free to send me an e-mail or leave a message in the comment section. As always, if you liked this blog post, I would appreciate a hit on the applause button below.\n The data from the Allensbach institute can be found online here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The resulting data can be found here and the script I used to extract the data is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There are a couple of more polling institutes available online but I decided to not scrape all of their results to save time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For better legibility, I have tried to use the colors the parties are usually associated with. In some instances, using only one (primary) color resulted in a hard to read plot. Thus, whenever possible, I have consulted online party guidelines to find colors they use in their own publications.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This quantity is defined as the sample standard deviation divided by the sample mean. See also Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"We try to find out if voters in Germany became more impulsive over time.","id":10,"section":"post","tags":[],"title":"Did German Voters Become More Impulsive?","uri":"https://albert-rapp.de/post/2021-10-03-sonntagsfrage/"},{"content":"This week, I had to deal with two very similar tasks on two very similar but not identical data sets that required me to write a function that is versatile enough to deal with both data sets despite their subtle differences. The differences that had to be accounted for mainly related to using functions in the two cases that relied on differently many arguments. Also, some of the column names were different which meant that I could not hard-code the column names into the function I was creating.\nConsequently, I had to use a few non-standard concepts (at least not standard to me) that enabled me to create the function which did everything I asked it to do. Since these concepts seemed interesting to me, I decided to implement a small example resulting in this blog post. Actually, I was even motivated to create a video for this blog post. You can find it on YouTube.\nWhat We Want To Achieve The aim of this example is to write a function that can create two tibbles that are conceptually similar but do not necessarily use the same column names or compute the existing columns in the same way. For this blog post, I have already set up two dummy data sets like that so that we can see what we want to do.\nLet\u0026rsquo;s take a look at these data sets I creatively called dat_A and dat_B.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  library(tidyverse) dat_A %\u0026gt;% head(3) ## # A tibble: 3 x 3 ## mu sigma dat  ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;  ## 1 -1 1 \u0026lt;tibble [5 x 2]\u0026gt; ## 2 -1 1.5 \u0026lt;tibble [5 x 2]\u0026gt; ## 3 -1 2 \u0026lt;tibble [5 x 2]\u0026gt; dat_B %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## lambda dat  ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;  ## 1 0.5 \u0026lt;tibble [5 x 2]\u0026gt; ## 2 0.7 \u0026lt;tibble [5 x 2]\u0026gt; ## 3 0.9 \u0026lt;tibble [5 x 2]\u0026gt;   As you can see, each tibble contains a column dat. This column consists of tibbles with multiple summarized stochastic processes which were simulated using parameters that are given by the remaining columns of dat_A and dat_B.\nYou probably have already noticed that the stochastic processes must have been simulated using differently many parameters since tibble A contains additional columns mu and sigma whereas tibble B can offer only one additional column lambda. However, even if differently many and differently named parameters are used, the logic of the generating function needs to be the same:\n Take parameters. Simulate stochastic processes with these parameters. Summarize processes  Thus, in step 1 the generating function which we want to code, needs to be versatile enough to handle different argument names and amounts. Next, let\u0026rsquo;s see what the dat column has in store for us.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  dat_A %\u0026gt;% pluck(\u0026#34;dat\u0026#34;, 1) %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## n proc_mean ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -1.01  ## 2 2 -0.958 ## 3 3 -0.968 dat_B %\u0026gt;% pluck(\u0026#34;dat\u0026#34;, 1) %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## n proc_variance ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 5.27 ## 2 2 2.66 ## 3 3 3.08   First of all, notice that I accessed the first tibble in the dat column using the super neat pluck() function. In my opinion, this function is preferable to the clunky base R usage of $ and [[, e.g. like dat_A$dat[[1]].\nAs you can see, the tibbles that are saved in dat contain columns n and proc_mean resp. proc_variance. As hinted at before, each row is supposed to represent a summary of the n-th realization of a stochastic process.\nHowever, notice that the summary statistics in use are not the same! The different column names proc_mean and proc_variance indicate that in tibble A the sample mean was used whereas tibble B contains sample variances. Again, our function that generates tib_A and tib_B should be flexible enough to create differently named and differently computed columns.\nHelpful Concepts Now that we know what we want to create, let us begin by learning how to handle differently many arguments and their varying names.\ndot-dot-dot For these kinds of purposes, R offers the ...-operator (pronounced dot-dot-dot). Basically, it serves as a placeholder for everything you do not want to evaluate immediately.\nFor instance, have you ever wondered how dplyr\u0026rsquo;s select() function is able to select the correct column?1 If you\u0026rsquo;re thinking \u0026ldquo;No, but what\u0026rsquo;s so special about this?\u0026rdquo;, then you may want to notice that it is actually not that simple to define your own select() function even with the help of the dplyr function.\nThis is because defining an appropriate function to select two columns from, say, the iris data set cannot be done like this:\n1  my_select \u0026lt;- function(x, y) {select(iris, x, y)}   Now, if you want to use the function the same way you would use dplyr::select(), i.e. simply passing, say, Sepal.Width, Sepal.Length (notice no \u0026quot;\u0026quot;) to your new function, it would look like this\n1 2  my_select(Sepal.Width, Sepal.Length) #\u0026gt; Error: object \u0026#39;Sepal.Width\u0026#39; not found   This error appears because at some point, R will try to evaluate the arguments as variables from your current environment. But of course this variable is not present in your environment and only present within the iris data set. Therefore, what dplyr::select() accomplishes is that it lets R know to evaluate the input argument only later on, i.e. when the variable from the data set is \u0026ldquo;available\u0026rdquo;.\nThis is where ... comes into play. It is not by chance that select() only has arguments .data and .... Here, select() uses that everything which is thrown into ..., will be passed along to be evaluated later. This can save our my_select() function, too.\n1 2 3 4 5 6  my_select \u0026lt;- function(...) {select(iris, ...)} my_select(Sepal.Width, Sepal.Length) %\u0026gt;% head(3) ## Sepal.Width Sepal.Length ## 1 3.5 5.1 ## 2 3.0 4.9 ## 3 3.2 4.7   Works like a charm! This will help us to define a function that is flexible enough for our purposes. Before we start with that, let us learn about another ingredient we will use.\ncurly-curly If we were to only select a single column from iris using our my_select() function, we could have also written the function using {{ }} (pronounced curly-curly). It operators similar to ... in the sense that it allows for later evaluation but applies this concept to specific variable. Check out how that can be used here.\n1 2 3 4 5 6  my_select \u0026lt;- function(x) {select(iris, {{x}})} my_select(Sepal.Width) %\u0026gt;% head(3) ## Sepal.Width ## 1 3.5 ## 2 3.0 ## 3 3.2   What\u0026rsquo;s more the curly-curly variables - curly-curlied variables (?) - can also be used later on for stuff like naming a new column. For example, let us modify our previous function to demonstrate how that can be used.\n1 2 3 4 5 6 7 8 9 10  select_and_add \u0026lt;- function(x, y) { select(iris, {{x}}) %\u0026gt;% mutate({{y}} := 5) # 5 can be replaced by some meaningful calculation } select_and_add(\u0026#34;Sepal.Width\u0026#34;, \u0026#34;variable_y\u0026#34;) %\u0026gt;% head(3) ## Sepal.Width variable_y ## 1 3.5 5 ## 2 3.0 5 ## 3 3.2 5   Mind the colon! Here, if you want to use y as column name later on you cannot use the standard mutate() syntax but have to use := instead.\nFunctional Programming One last thing that we will use, is the fact that R supports functional programming. Thus, we can use functions as arguments of other functions. For instance, take a look at this super simple, yet somewhat useless wrapper function for illustration purposes.\n1 2 3 4 5 6  my_simulate \u0026lt;- function(n, func) { func(n) } set.seed(564) my_simulate(5, rnorm) ## [1] 0.4605501 -0.7750968 -0.7159321 0.6882645 -2.0544591   As you just witnessed, I simply passed rnorm (without a call using ()) to my_simulate as the func argument such that rnorm is used whenever func is called. In our use case, this functionality can be used to simulate different stochastic processes (that may depend on different parameters).\nThe Implementation Alright, we have assembled everything we need in order to create our simulate_and_summarize_proc() function. In this example, the simulation of the stochastic processes will consist of simply calling rnorm() or rexp() but, of course, these functions can be substituted with arbitrarily complex simulation functions.\nWe will use n_simus as the amount of realizations that are supposed to be simulated and each realization will be of length TMax. Further, we will use ... to handle an arbitrary amount of parameters that are supposed to be passed to simulation_func. So, let\u0026rsquo;s implement the simulation part first (detailed explanations below).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  simulate_and_summarize_proc \u0026lt;- function(..., TMax, n_simus, simulation_func) { argslist \u0026lt;- list(n = TMax, ...) %\u0026gt;% map(~rep(., n_simus)) tibble( t = list(1:TMax), n = 1:n_simus, value = pmap(argslist, simulation_func) ) } set.seed(457) simulate_and_summarize_proc( mean = 1, sd = 2, TMax = 200, n_simus = 3, simulation_func = rnorm # arguments -\u0026gt; n, mean, sd ) ## # A tibble: 3 x 3 ## t n value  ## \u0026lt;list\u0026gt; \u0026lt;int\u0026gt; \u0026lt;list\u0026gt;  ## 1 \u0026lt;int [200]\u0026gt; 1 \u0026lt;dbl [200]\u0026gt; ## 2 \u0026lt;int [200]\u0026gt; 2 \u0026lt;dbl [200]\u0026gt; ## 3 \u0026lt;int [200]\u0026gt; 3 \u0026lt;dbl [200]\u0026gt;   As you can see, this created three (simple) stochastic processes of length 200 using the parameters mean = 1 and sd = 2. We can validate that the correct parameters were used once we implement the summary functions.\nFirst, let us address the tricky part in this function. In order to pass a list of arguments to pmap() that are then used with simulation_func, we first need to rearrange the lists a bit. After the first step, by simply putting everything from ... into the list we have a list like this:\n1 2 3 4 5  list(n = 100, mean = 1, sd = 2) %\u0026gt;% str() ## List of 3 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2   However, we will need to have each variable in the list repeated n_simus time in order to simulate more than one realization. Thus, we use map() to replicate:\n1 2 3 4 5 6 7  list(n = 200, mean = 1, sd = 2) %\u0026gt;% map(~rep(., 3)) %\u0026gt;% str() ## List of 3 ## $ n : num [1:3] 200 200 200 ## $ mean: num [1:3] 1 1 1 ## $ sd : num [1:3] 2 2 2   Note that calling rep() without map() does not cause an error but does not deliver the appropriate format:\n1 2 3 4 5 6 7 8 9 10 11 12 13  list(n = 100, mean = 1, sd = 2) %\u0026gt;% rep(3) %\u0026gt;% str() ## List of 9 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2   Next, let us take the current output and implement the summary. To do so, we will add another variables summary_name and summary_func to the function in order to choose a column name resp. a summary statistic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  simulate_and_summarize_proc \u0026lt;- function(..., TMax, n_simus, simulation_func, summary_name, summary_func) { argslist \u0026lt;- list(n = TMax, ...) %\u0026gt;% map(~rep(., n_simus)) tibble( t = list(1:TMax), n = 1:n_simus, value = pmap(argslist, simulation_func) ) %\u0026gt;% # this part is added unnest(c(t, value)) %\u0026gt;% group_by(n) %\u0026gt;% summarise({{summary_name}} := summary_func(value)) } set.seed(457) simulate_and_summarize_proc( mean = 1, sd = 2, TMax = 200, n_simus = 5, simulation_func = rnorm, summary_name = \u0026#34;mega_awesome_mean\u0026#34;, summary_func = mean ) ## # A tibble: 5 x 2 ## n mega_awesome_mean ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.955 ## 2 2 0.932 ## 3 3 0.987 ## 4 4 1.07  ## 5 5 1.15   Finally, we can use our super versatile function in combination with map() to create dat_A and dat_B.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  dat_A \u0026lt;- expand_grid( mu = seq(-1, 1, 0.25), sigma = seq(1, 3, 0.5) ) %\u0026gt;% mutate(dat = map2( mu, sigma, ~simulate_and_summarize_proc( mean = .x, sd = .y, TMax = 200, n_simus = 3, simulation_func = rnorm, summary_name = \u0026#34;proc_mean\u0026#34;, summary_func = mean ) )) dat_B \u0026lt;- expand_grid( lambda = seq(0.5, 1.5, 0.2) ) %\u0026gt;% mutate(dat = map( lambda, ~simulate_and_summarize_proc( rate = ., TMax = 200, n_simus = 3, simulation_func = rexp, summary_name = \u0026#34;proc_variance\u0026#34;, summary_func = var ) ))   Conclusion So, we have seen that we can combine {{ }}, ... and functional programming to create highly versatile functions. Of course, as always one might be tempted to say that one could have just programmed two different functions for our particular example.\nHowever, this would cause a lot of code duplication because a lot of steps are essentially the same which is hard to debug and maintain. Also, creating numerous functions does not scale well if we need to cover way more than two cases.\nWith that being said, I hope that you found this blog post helpful and if so, feel free to hit the comments or push the applause button below. See you next time.\n If you have read my YARDS lecture notes and this sounds familiar to you, you are absolutely right. I have reused and adapted a part of the \u0026ldquo;Choose Your Own Data Science Adventure\u0026rdquo;-chapter here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"Using concepts like dot-dot-dot and curly-curly we create functions that are more versatile and can be used in multiple settings.","id":11,"section":"post","tags":[],"title":"Writing Versatile Functions with R","uri":"https://albert-rapp.de/post/2021-09-16-similar-data-and-list-like-columns/"},{"content":"For a long time I have wondered why some people would use position_stack() for position alignment instead of the simpler version position = \u0026quot;stack\u0026quot;. Recently, though, I learned the purpose of the former approach when I tried to add data labels to a stacked bar chart for better legibility.\nFurther, I decided that this knowledge is a good addition to this ggplot2-tips series, so let\u0026rsquo;s see what position_stack() can do. To achieve this, let us create a small dummy data set.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  library(tidyverse) dummy_dat \u0026lt;- tibble( group = c(rep(\u0026#34;A\u0026#34;, 3), rep(\u0026#34;B\u0026#34;, 3)), category = factor( c(rep(c(\u0026#34;low\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;high\u0026#34;), 2)), levels = rev(c(\u0026#34;low\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;high\u0026#34;)), ), percent = c(0.41, 0.16, 1 - 0.41 - 0.16, 0.26, 1 - 0.26 - 0.36, 0.36) ) dummy_dat ## # A tibble: 6 x 3 ## group category percent ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A low 0.41 ## 2 A medium 0.16 ## 3 A high 0.43 ## 4 B low 0.26 ## 5 B medium 0.38 ## 6 B high 0.36   Next, take a look at the corresponding stacked bar chart. Since we created a dataset that contains percentages, I took the liberty of appropriately transforming the y-axis via scale_y_continuous().\n1 2 3 4  dummy_dat %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + scale_y_continuous(labels = scales::percent_format())   I believe that this visualization could be improved by adding text labels to each part of the stacked bar chart in order for the reader to immediately detect how large each portion of the bars is. Let\u0026rsquo;s try this via simply converting the values to strings and adding geom_text() to the plot.\n1 2 3 4 5 6  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text(aes(label = percent_labels)) + scale_y_continuous(labels = scales::percent_format())   Clearly, this did not work as intended because geom_text() uses position = \u0026quot;identity\u0026quot; by default which is why the y-position of the labels is simply determined by its value. Now, here is where I would usually change the positioning via position = \u0026quot;stack\u0026quot;. However, the result this approach delivers is somewhat less than perfect.\n1 2 3 4 5 6  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text(aes(label = percent_labels), position = \u0026#34;stack\u0026#34;) + scale_y_continuous(labels = scales::percent_format())   Ideally, I would like the labels to appear in the middle of each colored block. We could try to use vjust to move the labels which is not a great idea since every label will be moved by the same amount and the blocks are of different height. Similarly, we could compute the block middle points by hand and use that as separate y-aesthetic in geom_text().\nClearly, this involves a tedious additional computation and we should avoid this, if possible. This is precisely where position_stack() comes in. Conveniently, using position = position_stack() stacks the bars just like position = \u0026quot;stack\u0026quot; does but the function position_stack() has another argument vjust by which we can move the labels individually.\nHere, the possible values of vjust range from 0 (bottom of the designated height) to 1 (top of the designated height). Therefore, moving the labels to the middle of each bar is as easy as setting vjust = 0.5.\n1 2 3 4 5 6 7 8 9  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text( aes(label = percent_labels), position = position_stack(vjust = 0.5) ) + scale_y_continuous(labels = scales::percent_format())   Finally, one may - and this is definitely a matter of taste - tweak this plot further by changing the color and text formatting. Personally, I like darker colors combined with a white, bold label. In this case, this would look like this.\n1 2 3 4 5 6 7 8 9 10 11 12  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text( aes(label = percent_labels), position = position_stack(vjust = 0.5), col = \u0026#34;white\u0026#34;, fontface = \u0026#34;bold\u0026#34; ) + scale_y_continuous(labels = scales::percent_format()) + scale_fill_brewer(palette = \u0026#34;Set1\u0026#34;)   In summary, we have seen that using position = position_stack() is a more powerful alternative to position = \u0026quot;stack\u0026quot; that allows individual positioning. Nevertheless, as long as the additional arguments of position_stack() are not needed I still find the latter version simpler.\n","description":"We take a look at the differences between position = 'stack' and position = position_stack().","id":12,"section":"post","tags":["visualization"],"title":"ggplot tips: Using position_stack() for Individual Positioning","uri":"https://albert-rapp.de/post/2021-09-11-position-adjustment/"},{"content":"This blog post is part of a series I am creating where I collect tips I found useful when I first learned to work with ggplot2. All posts which are part of this series can be found here. In this post I want to deal with how to manually or automatically create labels for some aesthetic.\nManually Assigning Labels Assigning labels by hand, e.g. via col = \u0026quot;some label\u0026quot;, can be a great idea in some instances. For example, when you use two different smoothing methods, a hand-written label to differentiate between the two methods helps a lot. For instance, take a look the relationship between city mileage cty and highway mileage hwy of cars in the mpg data set from the ggplot2 package.\n1 2 3  library(tidyverse) ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5)   If one suspects a linear relationship between those two variables, one might want to use geom_smooth(method = 'lm') to check that hypothesis by drawing a straight line through the points. Similarly, one may be inclined to see what geom_smooth() would return if a linear model is not enforced. Adding both smoothing methods to the plot (and removing the confidence bands) yields:\n1 2 3 4  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, se = F, size = 1.5)   Obviously, differently colored lines should be used here to differentiate between the two smoothing methods. We have two approaches to do this. Either, we can manually assign a color (without using aes()):\n1 2 3 4  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(se = F, size = 1.5, col = \u0026#39;red\u0026#39;) + geom_smooth(method = \u0026#39;lm\u0026#39;, se = F, size = 1.5, col = \u0026#39;blue\u0026#39;)   Or we can use aes() and assign labels instead and let ggplot2 handle the colors on its own.\n1 2 3 4 5  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(aes(col = \u0026#39;auto\u0026#39;), se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, aes(col = \u0026#39;lm\u0026#39;), se = F, size = 1.5) + labs(col = \u0026#39;Smoothing\u0026#39;)   Personally, I prefer the latter approach because it has a couple of small advantages\n A legend is automatically generated with the corresponding labels such that even without looking at the code it becomes more obvious how each line was generated. Also, creating labels for an aesthetic is kind of the point of this post. I do not have to bother about the specific color names. For me, this is something that could take up a lot of time if I want to change the appearance of the plot later on because I might spend way too much time on finding colors that \u0026ldquo;work\u0026rdquo; together. Here, if I want to change the colors, I could simply use a Brewer color palette and hope that the creators of that palette had good reasons to arrange the palette the way they did.  1 2 3 4 5 6  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(aes(col = \u0026#39;auto\u0026#39;), se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, aes(col = \u0026#39;lm\u0026#39;), se = F, size = 1.5) + labs(col = \u0026#39;Smoothing\u0026#39;) + scale_color_brewer(palette = \u0026#39;Set1\u0026#39;)   Automatically Assigning Labels via Pivoting Sometimes, manually coloring aspects of your data can also be a bad idea. Especially, if you find yourself using the exact same geom_* multiple times on different variables of a data set, you may want to think about using a different approach. One such approach can be to rearrange the data first. For example, take a look at the following two time series that were simulated and collected in a tibble as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  set.seed(123) x1 \u0026lt;- rnorm(10) x2 \u0026lt;- rnorm(10) tib \u0026lt;- tibble( t = seq_along(x1), ts1 = cumsum(x1), ts2 = cumsum(x2) ) tib ## # A tibble: 10 x 3 ## t ts1 ts2 ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -0.560 1.22 ## 2 2 -0.791 1.58 ## 3 3 0.768 1.98 ## 4 4 0.839 2.10 ## 5 5 0.968 1.54 ## 6 6 2.68 3.33 ## 7 7 3.14 3.82 ## 8 8 1.88 1.86 ## 9 9 1.19 2.56 ## 10 10 0.746 2.09   Now, it is possible to plot both times series using geom_line() and use different colors for each line. To do so, one might be tempted (as I often was when I first learned ggplot2) to write code similar to the one we wrote earlier:\n1 2 3 4  tib %\u0026gt;% ggplot(aes(x = t)) + geom_line(aes(y = ts1, col = \u0026#34;Time series 1\u0026#34;)) + geom_line(aes(y = ts2, col = \u0026#34;Time series 2\u0026#34;))   Here, we basically used geom_line() twice for more or less the same plot but with only one aesthetic slightly changed. However, this may not be the best approach. This is especially true if we were to do this for, say, 100 time series as it would involve a lot of code duplication.\nInstead, let\u0026rsquo;s try to rearrange the data via pivot_longer() before even beginning to plot anything1. This way, we might even plot way more than 2 time series with only a single geom_line():\n1 2 3 4 5 6 7 8 9 10 11  set.seed(123) # Create multiple time series tib \u0026lt;- map_dfc(1:6, ~cumsum(rnorm(10))) %\u0026gt;% rename_with(~glue::glue(\u0026#34;label{1:6}\u0026#34;)) %\u0026gt;% bind_cols(t = 1:10, .) # Pivot and plot tib %\u0026gt;% pivot_longer(cols = -1, names_to = \u0026#34;ts\u0026#34;) %\u0026gt;% ggplot(aes(t, value, col = ts)) + geom_line()   As you just saw, it is also possible to, if necessary, relabel the column names in bulk before rearranging the data in order to label the aesthetic the way we want.\nSame Procedure, Different Aesthetic For the sake of an additional example, let us use the same ideas but with geom_boxplot() instead of geom_line(). Therefore, we will generate a couple of \u0026ldquo;data sets\u0026rdquo; and plot a box plot for each one:\n1 2 3 4 5 6  set.seed(123) map_dfc(1:6, rnorm, n = 100) %\u0026gt;% rename_with(~(1:6)) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026#34;ds\u0026#34;) %\u0026gt;% ggplot(aes(col = ds, y = value)) + geom_boxplot()   Here, I have used col again but as I have recently come to realize, using fill instead of col creates the \u0026ldquo;prettier\u0026rdquo; box plots so let\u0026rsquo;s use that instead.\n1 2 3 4 5 6  set.seed(123) map_dfc(1:6, rnorm, n = 100) %\u0026gt;% rename_with(~(1:6)) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026#34;ds\u0026#34;) %\u0026gt;% ggplot(aes(fill = ds, y = value)) + geom_boxplot()   So, as you just witnessed, what I have described so far does not only work with the color aesthetic. In fact, we can pretty much use the same approaches for all other aesthetics.\nThus, we have seen how to easily create labels for an aesthetic of our choice by either manually assigning labels or rearranging the data first in order to use the previous column names to assign labels automatically. Let me know what you think in the comments or if you liked this post, simply hit the applause button below.\n I am not describing how pivot_longer() works in detail here because I want to keep this post short by only \u0026ldquo;connecting the dots\u0026rdquo;. If you are unfamiliar with pivoting, you may check out the tidy data chapter from my YARDS lecture notes which was of course inspired by the infamous R for Data Science book. For an animation that demonstrates what pivot_longer() and pivot_wider() do, see gadenbuie/tidyexplain on GitHub.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"We talk about how to easily create labels for an aesthetic.","id":13,"section":"post","tags":["visualization"],"title":"ggplot tips: Assigning Labels to an Aesthetic","uri":"https://albert-rapp.de/post/2021-08-19-aesthetic-labels/"},{"content":"It is not that long ago when I first encountered ggplot2 and decided to learn how to use it1. By no means do I think that I have sufficiently mastered this package yet but as time has passed I have certainly picked up a few tips on my journey to get better at creating more meaningful visualizations. So, in order to remind myself of and share what I learned, I decided to create a sort of series containing tips that enhanced my visualization skills.\nHowever, this is not supposed to be an intro to ggplot2 though. I have already done that and you can find it in the data exploration chapter of my \u0026ldquo;Yet Again: R + Data Science\u0026rdquo; lecture notes (see YARDS). Currently, I plan to make each installment of the series somewhat short to keep it simple and all further posts in this series will be collected under the ggplot2-tips series tag which you can also access from this blog\u0026rsquo;s main page. So, without further ado, let us begin.\nUsing log-transforms Often, one finds variables in a data set that resemble heavy-tailed distributions and you can detect it by a simple histogram in a lot of cases. For instance, take a look at the variable sale_price of the ames dataset from the modeldata package. This variable contains the sale price of 2930 properties in Ames, Iowa and its histogram looks like this:\n1 2 3 4 5 6 7 8 9  library(tidyverse) library(modeldata) data(ames) # I like to clean names s.t. no capital letters are used in the variable names ames \u0026lt;- ames %\u0026gt;% janitor::clean_names() ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram()   As you can see, the distribution looks skewed in the sense that most of the sale prices fall within one range but there are also sale prices that are comparatively high. In effect, the histogram depicts a \u0026ldquo;long tail\u0026rdquo; and the highly priced sales are not that easily discernible in the histogram as the column heights may become really small and there may be large \u0026ldquo;gaps\u0026rdquo; between columns as seen above.\nOne common way to deal with this is to apply a logarithm to the variable of interest. It does not really matter which logarithm you use but since we like to work in a decimal system, a logarithm with base 10 is often used. Let\u0026rsquo;s see how this changes the picture.\n1 2 3  ames %\u0026gt;% ggplot(aes(x = log(sale_price, 10))) + geom_histogram()   Admittedly, we have a gap in the histogram on the left hand side now but overall the histogram looks way less skewed. In fact, this somewhat resembles what a histogram of a normally distributed random variable could look like. This is nice because Gaussian variables are something which a lot of statistical techniques can work with best.\nThus, working with a logarithmized variable might be helpful in the long run. Note that sometimes a variable benefits from being logarithmized but also contains values that are zero. To apply the logarithm anyway, often one then offsets the variable by shifting the variable by 1.\nUnfortunately, it may be nice that logarithmized variables are beneficial for statistical techniques and that the histograms are less skewed but the way we achieved that in the above example let\u0026rsquo;s the audience of the visualization somewhat clueless as to what the actual sale prices were. Sure, if in doubt, one could simply use a calculator to compute \\(10^{4}\\) and \\(10^{6}\\) to get a feeling for the range of the sale prices but of course no one will want to do that. This brings me to my next point.\nUse scale_*_log10() Honestly, I don\u0026rsquo;t know why but for a long time I have logarithmized data for visualization purposes as above because using scale_x_log10() felt somewhat frightening because I did not understand what was going on there. Take a look what happens if I add this particular layer to our initial plot instead of logarithmizing manually.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10()   Notice that the overall impression of the picture is the same as with the manually logarithmized plot. However, the x-axis is now logarithmized as opposed to being linear. So, manual logarithmization of the variable leads to just that: A transformation of the data but the axis in the plot remains linear which is why the labels on that x-axis showed values that needed to be retransformed to its original values.\nIn contrast, using scale_x_log10() leaves the data as it is but transforms the x-axis. In this case, this new axis is used for binning and counting to compute the histogram. Therefore, we can easily see that the majority of the sale prices lie between 100,000 and 300,000. Of course, things would be even simpler if the axis labels were not given in scientific notation. Luckily, we can easily change that.\nAdjust labels using the scales package As its name says, the scales package works really well in conjunction with the scale_* layers from ggplot2. In fact, this can make it somewhat comfortable to quickly adjust axis labels by simply passing a function (mind the ()) from the scales package to the scale_* layer\u0026rsquo;s argument labels. Here, we may simply use label_number() to get rid of the scientific notation.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_number())   Even better, scales has a lot of functions that are useful for specific units such as dollar or week, month, year (in case you are working with time data whose labels can be a special kind of pain).\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_dollar())   Of course, the same thing works not only for the x-axis scale but for all kinds of other scales too. For instance, if you want to plot the same histogram but oriented vertically, you can simply change the x-aesthetic to be the y-aesthetic which means that you will need to adjust the y scale then.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(y = sale_price)) + geom_histogram() + scale_y_log10(labels = scales::label_dollar())   In retrospect, it is really easy to adjust the axis with scale_* layers and the scales package and I really do not know why I hesitated in the past to use these functions. I guess adding another layer to the plot felt somewhat harder and slower than brute-forcing my way through the transformation. But believe me, in the long run this takes up way more of your time (especially if you want to interpret your plot later on).\nI hope that you enjoyed this post and if you did, feel free to hit the applause button below. In any case, I look forward to see you in the next installment of this series.\n Fun fact: Actually I somehow disliked R at first (to be fair I was not a frequent user of R back then anyway) but ggplot2 changed that and motivated me to do more in R.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"This is the beginning of a series about a few ggplot2 tricks I picked up along the way. In this first installment we talk about how logarithmizing scales can be beneficial.","id":14,"section":"post","tags":["visualization"],"title":"Beginning a ggplot2 Series: Logarithmize Your Scales","uri":"https://albert-rapp.de/post/2021-08-07-a-few-ggplot-tips/"},{"content":"Recently, I decided to try out a few stretches in an effort to stay in shape during long stretches of working from home and not leaving the house. Also, to distract me from my inflexible body I thought I would watch a video on YouTube simultaneously and as luck would have it, I saw an interesting video on Veritasium\u0026rsquo;s YouTube Channel called \u0026ldquo;Is Success Luck or Hard Work\u0026rdquo;. Personally, I agree with a lot of things being said in that video and I recommend that you check out the video if you want to get a perspective on the role of luck compared to skill (or keep on reading for my own take on this topic).\nBut more importantly, in the video Derek Muller - the guy behind Veritasium - describes a simulation he ran in order to hint at whether luck played a role in the selection of 11 out of 18300 applicants in 2017 for the NASA astronaut training program. The underlying model that is simulated in the video is described as assuming that astronauts are selected mostly based on their skill. However, 5% of the decision is also based on luck.\nThis sounds a little bit vague, so Muller elaborates: First, each applicant is assigned a random skill score (out of 100) and a luck score (out of 100). Then, the weighted sum of the two scores result in an applicant\u0026rsquo;s overall score (the weights being of course 95% for skill and 5% for luck). Finally, the top 11 applicants according to this overall score will then go on to become astronauts.\nNow, based on a thousand runs of this simulation what Veritasium finds is that it was the very lucky applicants who were selected. More precisely, the average luck score of the picked applicants was 94.7. Similarly, on average out of the 11 picked astronauts only 1.6 applicants would have been the same had the selection process been based on skill alone.\nSo, as I was watching this video, I noticed two things. One, I am really inflexible and I need to stretch more and two, this simulation sounds pretty cool and I bet I could recreate this simulation quite easily. Thus, an idea for a new blog post was born.\nThe Original Approach Later on, I want to tweak the above approach a bit but for now let us simulate the process as described above. First, we will need a function to generate the applicants' scores. Here, I want to generate the luck score according to a uniform distribution on \\((0, 100)\\) because I assume that we are all lucky in a similar way regardless of what the position we apply for is.\nHowever, I think it is conceivable that for highly specialized jobs (such as astronauts) only really skilled applicants show up whereas jobs that fall more in the \u0026ldquo;jack of all trades\u0026rdquo; category may attract applicants from all kinds of skill ranges. This will, of course, affect the distribution of skill scores and I wonder if this has a significant effect on the overall results. Therefore, I will make sure that the score generating function has the ability to use different skill distributions. Finally, let us add an option to change the skill-luck ration which we will first set to its default value of 5%.1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  library(tidyverse) simulate_applicants \u0026lt;- function(n, dist, luck_ratio = 0.05) { tibble( skill = dist(n), luck = runif(n, min = 0, max = 100), overall = (1 - luck_ratio) * skill + luck_ratio * luck ) } set.seed(123) simulate_applicants(5, function(x) rnorm(x, 50, 1)) ## # A tibble: 5 x 3 ## skill luck overall ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 49.4 95.7 51.8 ## 2 49.8 45.3 49.5 ## 3 51.6 67.8 52.4 ## 4 50.1 57.3 50.4 ## 5 50.1 10.3 48.1   Obviously, each column represents the respective score for each applicant. Regarding the skill scores I propose a couple of different distributions:\n Uniform distribution: We assume that applicants come from all kinds of skill ranges and all skill levels are equally likely. Normal distribution: We assume that most people fall within a medium skill range which we model by a normal distribution with mean 50 and standard deviation \\(50/4\\) so that for our 18000 astronauts chances are very slim that one of them falls outside the range (due to the empirical rule of the normal distribution). High specialization: To cover the scenario of only highly skilled applicants, let us use a beta distribution \\(X \\sim \\text{Beta}(1.2, 10)\\) and use the transformed variable \\(100(1-X)\\) as skill distribution.2 Low Specialization: Similarly, let us use \\(100X\\) where \\(X \\sim \\text{Beta}(1.2, 10)\\) to simulate a scenario in which mostly applicants with a low skill score occur.  The corresponding functions that realize the distributions for us are given as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  normal_skills \u0026lt;- function(n) { rnorm(n, mean = 50, sd = 50 / 4) %\u0026gt;% # Make sure that score stays in bounds pmax(0) %\u0026gt;% pmin(100) } uniform_skills \u0026lt;- function(n) { runif(n, min = 0, max = 100) } high_skills \u0026lt;- function(n) { 100 * (1 - rbeta(n, 1.2, 10)) } low_skills \u0026lt;- function(n) { 100 * rbeta(n, 1.2, 10) }   To illustrate the high and low skill distribution, let us take a look at the densities of the beta distributions in question.\nNow, let us write a function that runs a single iteration of the selection process and marks applicants as either selected or not. We will denote the number of applicants to be picked via m. In our astronaut example it holds that \\(m = 11\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  pick_applicants \u0026lt;- function(n, dist, m = 11) { applicants \u0026lt;- simulate_applicants(n, dist) applicants %\u0026gt;% arrange(desc(overall)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, n - m))) } n \u0026lt;- 18300 pick_applicants(n, normal_skills) ## # A tibble: 18,300 x 4 ## skill luck overall selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  ## 1 98.1 11.3 93.8 yes  ## 2 97.4 5.56 92.8 yes  ## 3 93.1 72.9 92.1 yes  ## 4 93.2 60.5 91.6 yes  ## 5 90.5 94.4 90.7 yes  ## 6 94.3 15.4 90.4 yes  ## 7 89.1 88.6 89.1 yes  ## 8 89.4 76.1 88.7 yes  ## 9 87.7 83.8 87.5 yes  ## 10 88.0 75.2 87.3 yes  ## # ... with 18,290 more rows   Using this function, we can easily simulate the whole application process a couple of times Monte Carlo style.\n1 2 3 4 5 6 7 8 9  set.seed(123) N \u0026lt;- 1000 simus \u0026lt;- expand_grid( dist = c(\u0026#34;uniform_skills\u0026#34;, \u0026#34;normal_skills\u0026#34;, \u0026#34;low_skills\u0026#34;, \u0026#34;high_skills\u0026#34;), simuID = 1:N ) %\u0026gt;% mutate(applicants = map(dist, ~pick_applicants(n, get(.)))) %\u0026gt;% unnest(applicants) %\u0026gt;% filter(selected == \u0026#39;yes\u0026#39;)   Having created a tibble simus that contains information on the skill and luck scores of success candidates for each simulation run, we can visualize the distribution of the luck scores of the successful candidates using boxplots. The idea behind that is that if luck is important, then the boxplots should differ from that of a standard uniform distribution on \\((0, 100)\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  library(grid) library(gridExtra) library(gtable) avgs \u0026lt;- simus %\u0026gt;% group_by(dist) %\u0026gt;% summarize(avg_luck = mean(luck)) %\u0026gt;% arrange(desc(avg_luck)) %\u0026gt;% mutate( avg_luck = scales::comma(avg_luck, accuracy = .01), dist = case_when( dist == \u0026#34;high_skills\u0026#34; ~ \u0026#34;High skills\u0026#34;, dist == \u0026#34;low_skills\u0026#34; ~ \u0026#34;Low skills\u0026#34;, dist == \u0026#34;uniform_skills\u0026#34; ~ \u0026#34;Uniform skills\u0026#34;, dist == \u0026#34;normal_skills\u0026#34; ~ \u0026#34;Normal skills\u0026#34; ) ) colorScale \u0026lt;- glue::glue(\u0026#34;dodgerblue{1:4}\u0026#34;) avgTable \u0026lt;- tableGrob( avgs, rows = NULL, cols = NULL, theme = ttheme_minimal( core=list(fg_params = list(col = colorScale, fontface = 2)) ) ) simus %\u0026gt;% mutate(dist = fct_reorder(dist, luck)) %\u0026gt;% ggplot(aes(y = dist, x = luck, col = dist)) + geom_boxplot(show.legend = F) + theme_classic() + labs( x = \u0026#34;Luck\u0026#34;, y = element_blank(), title = \u0026#34;Luck Distribution Among Successful Applicants\u0026#34;, subtitle = \u0026#34;(Average luck score depicted in the table)\u0026#34; ) + annotation_custom(avgTable, xmin = 20, xmax = 60, ymin = 2, ymax = 5) + scale_y_discrete(breaks = NULL) + scale_color_manual(values = rev(colorScale)) + theme(axis.line.y = element_blank(), plot.subtitle = element_text(size = 10))   Here, we added a table of the average luck score of the successful applicants to the boxplots that summarize the distribution of their luck scores. As it turns out, we can clearly see differences between the luck distribution of the successful candidates across the different skill distributions. Interestingly, the uniform skill distribution comes quite close to the average value Veritasium finds in his video, so I guess we can assume that he probably used that skill distribution.\nI would say that the key takeaway of this picture is that the more specialized your area of expertise is, the luckier you have to be if you have many similarly skilled competitors. In a way, this makes sense. If you and your competition is basically at the top of the game and there is not much room to differentiate candidates w.r.t. skill, then luck may just be the deciding factor.\nInterestingly, the same holds true when skills are uniformly distributed among the whole range. Further, when the skill distribution of you and your competition looks more normally distributed or if the skill scores of all applicants are rather low, then you do not need to be extremely lucky (at least not as much as before). But still, you have to be more lucky than the average applicant (recall that luck is modeled via a uniform distribution here with mean 50).\nSo, in a sense this might mean that if chances are good that there are a couple of applicants who are as good as it gets, i.e. at the maximum of the skill range (which is the case for the high and uniform skills), then the successful candidates are indeed really lucky. Similarly, if chances are low that some applicants are the best of the best (normal and low skills), then successful applicants are luckier than the average Joe but in a less extreme way than in the previous example. In total, it looks like luck plays a role in either case.\nFinally, to break this scenario down to something more realistic with a higher available spots to applicants ratio I ran the same simulation but with only one available position and 100 applications.3 The results look similar but as expected if their are less applicants for one position, then luck plays a lesser but still important role.\nOvercoming Threshold Approach There is this old joke where a recruiter in an HR department gets a large stack of applications for a specific position within the company. The recruiter immediately begins to work on the applications by simply taking half of the applications and throwing them into the trash because he \u0026ldquo;does not hire unlucky people\u0026rdquo;.\nThis may be an extreme action but then again I have heard that some companies immediately reject an application if they see only a single typo in the documents. To me, this is similar to what the recruiter from the joke is doing because a typo can happen by accident to even the most careful person regardless of their skill. So, let us use this as a way to construct another hypothetical scenario.\nIn this scenario, an application goes through two stages. In the first stage, the application is either is moved to the second stage or is rejected due to some arbitrary small reason. By this we mean some small event based on luck which we will model here with our luck score from above. In the second stage, everything is judged purely on skill alone, i.e. the person with the highest skill score gets the job.\nLet us write a new function for this second hypothetical scenario. Notice that this new function ranks applicants according to their skill score, i.e. the most skilled applicant is ranked as 1, the second most skilled applicant is ranked as 2 and so on.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  pick_applicants2 \u0026lt;- function(n, dist, m = 11, luck_thresh = 50) { applicants \u0026lt;- simulate_applicants(n, dist) applicants \u0026lt;- applicants %\u0026gt;% mutate(skill_rank = min_rank(desc(skill))) %\u0026gt;% filter(luck \u0026gt; luck_thresh) applicants %\u0026gt;% arrange(desc(skill)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, nrow(applicants) - m))) } pick_applicants2(n, high_skills) ## # A tibble: 49 x 5 ## skill luck overall skill_rank selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  ## 1 99.9 87.4 99.3 1 yes  ## 2 99.1 54.6 96.9 3 yes  ## 3 98.8 54.5 96.6 5 yes  ## 4 98.8 97.5 98.7 6 yes  ## 5 98.2 90.6 97.8 10 yes  ## 6 97.6 60.8 95.8 12 yes  ## 7 97.0 87.0 96.5 13 yes  ## 8 96.8 71.9 95.5 14 yes  ## 9 96.7 97.0 96.8 16 yes  ## 10 96.6 55.1 94.5 18 yes  ## # ... with 39 more rows   Afterwards, we can run a similar simulation as before. However, this time we will take a look at the distribution of the ranks of the successful candidates. Here are the results for our astronaut scenario, i.e. 18300 applicants with 11 open positions.\nAs you can see, the distribution of the skill ranks of the successful applicants do not vary much across the skill distributions. Also, it looks like the average ranks are simply the amount of positions plus 1. This is somewhat unsurprising since we throw out 50% of the applicants randomly in the first stage of the process.\nNevertheless, this reinforces the idea that not necessarily the most skilled person gets hired to do the job. What is even more surprising to me is the fact that in the current scenario there is still a chance of around 8% that a successful applicant has a skill rank above 22 (recall that there are 11 open positions). Now, running the same analysis for 100 applicants for 1 job again yields similar results.\nIn these 1000 simulations, the chances that a successful applicant has a skill score of 5 or above are similarly high but depend on the skill distribution.\n## # A tibble: 4 x 3\r## # Groups: dist [4]\r## dist n prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;\r## 1 Normal skills 82 8.2% ## 2 Uniform skills 69 6.9% ## 3 High skills 65 6.5% ## 4 Low skills 58 5.8%\rSummary So, in our simulations we have seen that luck plays a role in whether an applicant gets a job. Further, this actually depended on the skill distribution of the applicants and the impact of luck was most strongly pronounced in the cases when there was a good chance that highly skilled people are present among the applicants. Probably, this is the most relevant scenario anyway since we all like to believe that we are skilled but are still somewhat more proficient than other skilled people.\nOf course, in terms of real world implications, this simulation can only give anecdotal evidence towards the hypothesis that luck plays a large role in success. Also, we made a lot of assumptions in our simulations that might be debatable. But, personally, I like to believe that luck cannot be neglected in the end and even the most skilled person may want to be grateful when he gets accepted for a job. If you wish to share your opinion on that, feel free to leave a comment. Finally, if you thought this post was interesting, I would also appreciate it if you clicked the applause button (just so that it feels less like I am talking into a void).\n When I first started to write this blog post, I wanted to investigate how results change for different ratios but this post is already long enough. Thus, I decided to simply leave that functionality unused but someone interested in recreating parts of this simulation may tweak that parameter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Here, the parameters of the beta distribution were picked so that the densities somewhat fit the desired description as will be seen in the plot a few lines ahead. Of course, one could potentially change the parameters but here I decided to go with this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I tried to research what is a realistic number of applicants for a given positions but results were varying and I found claims of less than 100 applicants on average and way more than 100 applicants. So, here I decided to go with something in between.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recreate a simulation study on the influence of luck on success compared to the influence of skill.","id":15,"section":"post","tags":["simulation"],"title":"Is Success Luck or Hard Work?","uri":"https://albert-rapp.de/post/2021-07-26-luck-vs-skill/"},{"content":"For my first post on this blog I decided to create an animation using the animation package. To give this animation some purpose let me demonstrate how kernel density estimators work with the help of an animation.\nIn general, kernel density estimators are, as is kind of obvious by the name, used to estimate the underlying density of a random sample. For instance, imagine that we have a sample drawn from an some unknown distribution.\n1 2  n \u0026lt;- 100 sample \u0026lt;- rexp(n)   Then, assuming that we do not actually know that the current sample was drawn from an exponential distribution, we might want to estimate the density and see if the estimate fits to some well-known parametric family of distributions. With the help of ggplot() and geom_density() this is straightforward.\n1 2 3  library(tidyverse) ggplot() + geom_density(aes(x = sample))   The underlying procedure to generate the plot it to use a kernel density estimator \\(\\hat{f}_h\\) in order to estimate the true underlying density \\(f\\) by using the formula $$ \\hat{f}_h(x) = \\frac{1}{nh} \\sum_{k = 1}^{n} K\\Big(\\frac{x - x_k}{h}\\Big) $$ for all \\(x \\in \\mathbb{R}\\) where \\(n\\) is the sample lengh and \\(h \u0026gt; 0\\) is a smoothing parameter that needs to be chosen and \\(K\\) is a \u0026ldquo;suitable\u0026rdquo; function. Usually, this parameter \\(h\\) is called bandwidth and \\(K\\) is called a kernel function which is often to be the density of a probability distribution.\nThe Bandwidth In geom_density(), the default kernel function is the Gaussian density and the bandwidth can be tweaked through the bw argument.\n1 2 3 4 5 6 7 8 9  h \u0026lt;- 1 ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) )   Of course, I could now create multiple plots and change the value if h each time to demonstrate the effect of the bandwidth but the point of this post was to create an animation. So let\u0026rsquo;s do that instead.\nNevertheless, to create an animation, we need to be able to create multiple plots. Therefore, let us use the previous code and wrap a function depending on h around that. This function will be our plot generator depending on the bandwidth.\n1 2 3 4 5 6 7 8 9 10 11 12  plot_gen \u0026lt;- function(h) { g \u0026lt;- ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) ) print(g) # For the animation we need this to be printed }   Now that we have that, define a function that creates all the plots we want to see in our animation, i.e. we create the animation frame by frame. This function can then be passed to saveGIF() from the animation package which then renders the animation for us. Creating a gif in R is as simple as that.\n1 2 3 4 5 6 7  all_the_plots \u0026lt;- function() { map(seq(0.05, 0.5, 0.05), plot_gen) } library(animation) saveGIF(all_the_plots()) ## Output at: animation.gif ## [1] TRUE   As you can see, the bandwidth really is a smoothing parameter. Of course, too much smoothing may not yield great results so the parameter needs to be chosen with care but let us not worry about this in this blog post.\nThe Actual Estimation Procedure Let us create another animation to visualize how kernel density estimation works on a more basic level, i.e. . In order to do so, notice that if the kernel \\(K\\) is a continuous density function of a random variable \\(X\\) (e.g. a Gaussian random variable), then the density function of the random variable \\((X + x_0)h\\) where \\(x_0 \\in \\mathbb{R}\\) and \\(h \u0026gt; 0\\) is given by \\(K((X - x_0)/h)/h.\\)\nConsequently, in the case of standard Gaussian random variables \\(X\\), the kernel density estimator is nothing but the average of the densities of \\(n\\) Gaussian random variables with individual means \\(x_k\\), \\(k = 1, \\ldots, n,\\) and common standard deviation \\(h.\\)\nTherefore, for a given sample you can run a kernel density estimation by taking the following steps:\n Check where each data point \\(x_k\\) is located on the x-axis For each data point \\(x_k\\) draw a Gaussian density \\(f_k\\) with standard deviation \\(h\\) and mean \\(x_k\\) For each \\(x \\in \\mathbb{R}\\) check what are the values of \\(f_k\\), \\(k = 1, \\ldots, n,\\) at \\(x\\) and average these.  So to create a visualization of the kernel density estimation principle, we simple create a function that plots each of those steps for us. Finally, we execute all of these functions and send them to saveGIF().\nWe begin by computing the data we need to create the plots later on, i.e. we simulate a sample and compute the values of the densities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  compute_density \u0026lt;- function(x_0, h, K = dnorm) { xx \u0026lt;- seq(-6, 6, 0.001) * h tibble( x = xx, density = K((x - x_0) / h) / h ) } set.seed(123) # For the sake of demonstration we use a  # small uniformly distributed sample here x_sample \u0026lt;- runif(5, -5, 5) h \u0026lt;- 1 tib \u0026lt;- tibble( k = seq_along(x_sample), density = map(x_sample, compute_density, h = h) ) %\u0026gt;% unnest(density)   Then, it becomes time for our first step, i.e. check where the sample values are located.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  draw_axis \u0026lt;- function(x_sample, tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) p \u0026lt;- ggplot(data = NULL, aes(x = x_sample)) + theme_minimal() + theme( axis.line.x = element_line(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank()) p } p \u0026lt;- draw_axis(x_sample, tib) p   Once we have that, we draw the kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  draw_kernel \u0026lt;- function(p, tib) { p \u0026lt;- p + geom_line(data = tib, aes(x, density, group = k), size = 1) + geom_segment( aes( x = x_sample, xend = x_sample, y = 0, yend = dnorm(0) ), linetype = 2 ) + labs(y = element_blank()) p } draw_kernel(p, tib)   Next, average the densities at an arbitrary position \\(x_0.\\) If we can do that, then we can iterate through different values of \\(x_0.\\)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  plot_until_x0 \u0026lt;- function(tib, x0) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% filter(x \u0026lt;= x0) %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- ggplot() + geom_line( data = tib, aes(x, density, group = k), alpha = 0.5, size = 1 ) + geom_point( data = filter(tib, x == x0), aes(x, density), #col = anim_col, alpha = 0.75, size = 3 ) + geom_vline(xintercept = x0, col = anim_col, alpha = 0.5) + geom_point( data = slice_tail(tib_x0, n = 1), aes(x, est), col = anim_col, size = 3 ) + geom_line( data = tib_x0, aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } x0 \u0026lt;- (0) plot_until_x0(tib, x0)   Last but not least, we may want to display the estimated density without the underlying kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  final_plot \u0026lt;- function(tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- tib_x0 %\u0026gt;% ggplot() + geom_line( aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } final_plot(tib)   Finally, we have all the ingredients to create the animation by collecting all of these functions in a wrapper function and using it in conjunction with saveGIF().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  gif \u0026lt;- function(x_sample, tib) { p \u0026lt;- draw_axis(x_sample, tib) map(1:3, ~print(p)) p \u0026lt;- draw_kernel(p, tib) map(1:5, ~print(p)) map(seq(min(tib$x), max(tib$x), 0.5), ~plot_until_x0(tib, .)) map(1:15, ~final_plot(tib)) } saveGIF(gif(x_sample, tib), interval = 0.4, # animation speed ani.width = 720, ani.height = 405, movie.name = \u0026#34;kernelAnimation.gif\u0026#34;)   Thus, we have created a short animation that illustrates the kernel density estimation procedure. Probably, there is some room for improving the animation by fine tuning the plots, tweaking with the animation speed or the number of frames. For now, though, let us leave everything as it is.\nBut feel free to let me know what could be improved in the comments. Similarly, if you want to leave any other form of feedback, feel free to roam the comment section too. Finally, if you enjoyed what you have seen here but do not want to bother writing a comment, you may simply hit the applause button instead.\n","description":"For my first post I create an animation using the animate package.","id":16,"section":"post","tags":["statistics","visualization"],"title":"Animating kernel density estimators","uri":"https://albert-rapp.de/post/visualize-kernel-density-estimation/"},{"content":"Welcome! My Name is Albert Rapp and I am currently a PhD student in mathematics at Ulm University. As part of my obligations as a PhD student I had to teach an applied course on statistical techniques using the statistical software R. Interestingly, I noticed that I actually enjoy using R which is why I went the extra mile and decided to write a set of lecture notes on my own and made them accessible online. In fact, I realized that the process of writing felt (mostly) fun and my retention rate of the stuff I had to learn in order to write about it was actually quite high.\nConsequently, I decided to make a habit out of writing about things I encounter in statistics and/or R. So, what you see here in this blog is the result of that decision. In the end, this is nothing but a personal project and is as much about helping me retain stuff as I learn new things as it is about making a contribution to the R community along the lines of many other blogs and tutorials that helped me learn R. Hopefully, what can be found here will be of benefit to someone other than me.\nCurrently, I aim at posting a new blog post on a biweekly schedule and regarding the content, I will write about whatever I find most interesting at the time of writing. On a long term basis though, I want to cover parts of my R/statistics bucket list which I described in the final chapter of my YARDS lecture notes. If you want to be notified when there is a new post online, you may want to check out the RSS feed which you can access here or via the RSS symbol at the bottom of the \u0026ldquo;Posts\u0026rdquo; Page.\nThis blog is build with blogdown and Hugo and all blog posts are released under a CC-BY-NC 4.0 license.\n","description":"About Me","id":17,"section":"","tags":null,"title":"About","uri":"https://albert-rapp.de/about/"}]