[{"content":"A couple of weeks back, I wanted to explain to my student what I mean when I talk about the \u0026ldquo;variance of the sample variance\u0026rdquo;. In my head, this term sounds quite confusing and contains the word \u0026ldquo;variance\u0026rdquo; at least one too many times. But as I was not sure whether my subsequent explanation really came through, I decided to let my students explore the notion on their own through a Shiny app.\nHonestly, I thought this would be quite simple to code because I have already learned the basics of Shiny when I wanted to show my students what exciting web developmental things R can do. Back then, I summarized the basics in one chapter of my YARDS lecture notes.\nHowever, even though the idea of my app was simple, I soon came to realize that I would need to learn a couple more Shiny-related things to get the job done. And, as is usual with coding, I did this mostly by strolling through the web in order to find code solutions for my particular problems. Most of the time, I consulted Hadley Wickham\u0026rsquo;s Mastering Shiny but still I ended up searching for a lot of random other stuff on the web.\nConsequently, I decided that it might be nice to collect what I have learned in one place. So, here is a compilation of loosely connected troubles I solved during my Shiny learning process. May this summary serve someone well.\nUse a theme for simple customization Let\u0026rsquo;s start with something super easy. If you wish to customize the appearance of you app, you can set the theme argument of fluidPage() to either a CSS-file that contains the necessary configuration (this is the hard way) or use a theme from bslib::bs_theme(). The latter approach comes with a lot of named preimplemented themes and is easily implemented by bootswatch = \u0026quot;name\u0026quot;. In my app, I have simply added theme = bslib::bs_theme(bootswatch = \u0026quot;superhero\u0026quot;). For other themes, have a look at RStudio\u0026rsquo;s Shiny themes page.\nCheck out this super simple example that I have adapted from the default \u0026ldquo;new Shiny app\u0026rdquo; output (you will actually have to copy and run this in an R script on your own).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  library(shiny) library(tidyverse) ui \u0026lt;- fluidPage( # Theme added here theme = bslib::bs_theme(bootswatch = \u0026#34;superhero\u0026#34;), titlePanel(\u0026#34;Old Faithful Geyser Data\u0026#34;), sidebarLayout( sidebarPanel( sliderInput(\u0026#34;bins\u0026#34;, \u0026#34;Number of bins:\u0026#34;, min = 1, max = 50, value = 30) ), mainPanel( plotOutput(\u0026#34;distPlot\u0026#34;) ) ) ) server \u0026lt;- function(input, output) { output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } shinyApp(ui = ui, server = server)   During the course of this text, we will extend this small example bit by bit. But, I want to avoid copy-and-pasting code each time we change something. Thus, for the remaining examples I will only describe the changes to the previous version instead of pasting the whole code. Nevertheless, I will provide links after each example so that each script can be downloaded at will. The current example can be found here.\nIsolate slider from reactivity As is currently intended, our app\u0026rsquo;s histogram changes whenever the slider is moved. Sometimes, though, this is not what we wish to do. Instead, we may want to delay the rendering of the plot until a button is clicked.\nThis can be achieved through a simple isolate() command which, well, isolates whatever is in between the function\u0026rsquo;s parentheses from changes on the UI. Here, let us put input$bins into the isolate() function and check what happens when we move the slider (full code here), i.e. we changed\n1  bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1)   Excellent! Nothing happens when we move the slider. Dumb and useless but excellent anyway.\nObserve that we could have also put the whole renderPlot() function call into isolate(). This app would work in the sense that we created valid code but then the reactivity of the slider is still active. The isolate() documentation hints at this with \u0026ldquo;\u0026hellip;if you assign a variable inside the isolate(), its value will be visible outside of the isolate()\u0026rdquo;.\nCreate and observe Buttons Let us bring back some reactivity to our app by adding a button that reevaluates our histogram when clicked. First, we will add a button to the UI. Second, we will implement what needs to happen on the server side of things when the button is clicked.\nThe first step is pretty simple. All we have to do is add actionButton() to the UI. Same as sliderInput() we have to specify a inputId and label for the button. Here, we could add\n1  actionButton(\u0026#34;draw_button\u0026#34;, \u0026#34;Reevaluate!\u0026#34;, width = \u0026#34;100%\u0026#34;)   Then, on the server side we will have to catch each click on the button. Once a click is registered, the plot is supposed to be rendered again. We do this with observeEvent() which expects an event expression and a handler expression. In our case, the former is simply the id of our button, i.e. input$draw_button, and the latter is what code is to be executed when the event is observed. Therefore, we move our code for rendering the plot into this part of observeEvent(). Thus, in our server function we now have\n1 2 3 4 5 6 7 8 9  observeEvent( input$draw_button, { output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   Notice that we have wrapped our code into {}. Strictly speaking, this is not necessary because we only \u0026ldquo;do one thing\u0026rdquo; but, of course, we can easily imagine that we want to tie multiple calculations to a button click. In this case, we will need to wrap all commands into {}. In any case, our code now does what we expect it to do and on each click a new histogram is rendered using the current value of the slider input. This new app\u0026rsquo;s complete code can be found here.\nUse eventReactive() as an alternative for updating values Honestly, this part I learned just 5 minutes ago while I was writing the last section of this blog post. When I looked into the documentation of observeEvent(), I noticed that there is also a function eventReactive() which may be better suited for our current use case as it allows us to avoid manually isolating input$bins.\nThis new function works similar to observeEvent() but it creates a reactive variable instead. This, we can use for rendering. Check this out\n1 2 3 4 5 6 7 8 9  plot \u0026lt;- eventReactive( input$draw_button, { x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = input$bins + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) } ) output$distPlot \u0026lt;- renderPlot({plot()})   Notice how we do not use isolate() anymore and use the plot variable like a reactive in renderPlot(), i.e. we have to \u0026ldquo;call\u0026rdquo; its value with ().\nHowever, be aware that eventReactive() creates a reactive variable such that you cannot change, say, multiple plots at once. Nevertheless, eventReactive() can be a great way to tie a plot to an event. So, I guess it dependes on your use case and personal preference if you want to use eventReactive() rather than observeEvent(). Anyway, this version\u0026rsquo;s code can be copied from here.\nUse reactiveVal() to manually change values on click Another neat function is reactiveVal() which helps you to construct for instance counters that increase on the click of a button. We can initialize a reactive value by writing\n1  counter \u0026lt;- reactiveVal(value = 0)   within the server function. This way, our counter is set to zero and we can update it and set it to, say, one by calling counter(value = 1). The current value of the counter can be accessed through counter().\nClearly, we can tie the updating of a reactive value to an event that we observe through observeEvent(). For instance, we count how often the draw button in our small app is clicked by changing our previous observeEvent(input$draw_button, ...). Here, we would change this particular line of code to\n1 2 3 4 5 6 7 8 9 10 11 12  observeEvent( input$draw_button, { tmp \u0026lt;- counter() counter(tmp + 1) output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   Finally, we can show this information on our UI for demonstration purposes by adding a textOutput(\u0026quot;demonstration_text\u0026quot;) to our UI and setting\n1 2 3 4 5  output$demonstration_text \u0026lt;- renderText(paste( \u0026#34;You have clicked the draw button\u0026#34;, counter(), \u0026#34;times. Congrats!\u0026#34; ))   The complete app can be found here.\nUse tabsetPanel and unique plot names Often, you do not want to display all information at once. In my particular case, I wanted to show only one out of two plots based on the user\u0026rsquo;s chosen estimator (sample mean or sample variance). A great way to achieve that is to use tabsetPanel() in the UI.\nOrdinarily, you can create a UI this way by setting\n1 2 3 4 5 6 7  mainPanel( tabsetPanel( tabPanel(\u0026#34;Plot\u0026#34;, plotOutput(\u0026#34;plot\u0026#34;)), tabPanel(\u0026#34;Summary\u0026#34;, verbatimTextOutput(\u0026#34;summary\u0026#34;)), tabPanel(\u0026#34;Table\u0026#34;, tableOutput(\u0026#34;table\u0026#34;)) ) )   This was an example taken straight out of the documentation of tabsetPanel(). What you will get if you start an app containing a UI like this is a panel with three tabs (each one corresponding to a plot, text or table output) and the user can click on the tabs to switch between the views. This isn\u0026rsquo;t that surprising.\nHowever, if we also add an id to this and set type to hidden, like so\n1 2 3 4 5 6 7 8 9  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;Plot\u0026#34;, plotOutput(\u0026#34;plot\u0026#34;)), tabPanel(\u0026#34;Summary\u0026#34;, verbatimTextOutput(\u0026#34;summary\u0026#34;)), tabPanel(\u0026#34;Table\u0026#34;, tableOutput(\u0026#34;table\u0026#34;)) ) )   then, by default, the user does not have the options to change between views by clicking on tabs. Now, the view will need to change based on other interactions of the user with the UI. This change will then need to be customized within the server function. This is where the id argument comes into play because it allows ourselves to address the tabs via updateTabsetPanel().\nHere, let us take our previous example and display the same information on a different panel, i.e. at the end we will have two panels with exactly the same information in each tab. I know. This is not particularly exciting or meaningful but it serves our current purpose well.\nNaively, we might implement our user-interface like so\n1 2 3 4 5 6 7 8 9 10 11 12  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;panel1\u0026#34;, { # UI commands from before here }), tabPanel(\u0026#34;panel2\u0026#34;, { # UI commands from before here }), ) )   However, we will have to be careful! If we simply copy-and-paste our UI from before, then we won\u0026rsquo;t have unique identifiers to address e.g. the draw button or the plot output. Since this is a serious NO-NO (all caps for dramatic effect) and the app won\u0026rsquo;t work properly, let us instead write a function that draws the UI for us but creates it with different identifiers like this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  create_UI \u0026lt;- function(unique_part) { sidebarLayout( sidebarPanel( # unique label here by adding unique_part to bins sliderInput(paste(\u0026#34;bins\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Number of bins:\u0026#34;, min = 1, max = 50, value = 30), actionButton(paste(\u0026#34;draw_button\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Reevaluate!\u0026#34;, width = \u0026#34;100%\u0026#34;), actionButton(paste(\u0026#34;change_view\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;), \u0026#34;Change view\u0026#34;, width = \u0026#34;100%\u0026#34;) ), mainPanel( textOutput(paste(\u0026#34;demonstration_text\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)), # Counter text added textOutput(paste(\u0026#34;countEvaluations\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)), plotOutput(paste(\u0026#34;distPlot\u0026#34;, unique_part, sep = \u0026#34;_\u0026#34;)) ) ) }   Also, notice that I have created another button called \u0026ldquo;Change view\u0026rdquo; within the UI. Further, this button\u0026rsquo;s name is so mind-baffling that I won\u0026rsquo;t even try to elaborate what it will do. Finally, using create_UI, we can set up the UI like so\n1 2 3 4 5 6 7 8 9  mainPanel( tabsetPanel( id = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;, type = \u0026#34;hidden\u0026#34;, tabPanel(\u0026#34;panel1\u0026#34;, create_UI(\u0026#34;panel1\u0026#34;)), tabPanel(\u0026#34;panel2\u0026#34;, create_UI(\u0026#34;panel2\u0026#34;)), ) )   and address everything within the UI in a unique manner. Of course, such a functional approach only works well if the two panels look sufficiently similar such that it makes sense to design them through a single function. In my particular app that deals with the variance of estimators, this was the case because the tabs for the sample mean and sample variance were quite similar in their structure.\nNow that we have covered how the UI needs to be set up, let me show you how to change the view from one panel to the next. Shockingly, let us link this to a click on the \u0026ldquo;change view\u0026rdquo; button(s) like so\n1 2 3 4 5 6 7 8  observeEvent( input$change_view_panel1, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel2\u0026#34;) ) observeEvent( input$change_view_panel2, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;) )   Also, note that the previous code\n1 2 3 4 5 6 7 8 9 10 11 12  observeEvent( input$draw_button, { tmp \u0026lt;- counter() counter(tmp + 1) output$distPlot \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(input$bins) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) } )   won\u0026rsquo;t work anymore because the old identifiers like draw_button etc. need to be updated to draw_button_panel1 or draw_button_panel2. Clearly, this could potentially require some code duplication to implement the server-side logic for both tabs. But since we feel particularly clever today1, let us write another function that avoids a lot of code duplication.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  render_my_plot \u0026lt;- function(panel, counter, input, output) { tmp \u0026lt;- counter() # save current value of counter counter(tmp + 1) # update counter # Create identifier names bins_name \u0026lt;- paste(\u0026#34;bins\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) distplot_name \u0026lt;- paste(\u0026#34;distPlot\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) demonstration_text \u0026lt;- paste(\u0026#34;demonstration_text\u0026#34;, panel, sep = \u0026#34;_\u0026#34;) # Render Plot output[[distplot_name]] \u0026lt;- renderPlot({ x \u0026lt;- faithful[, 2] bins \u0026lt;- seq(min(x), max(x), length.out = isolate(pluck(input, bins_name)) + 1) hist(x, breaks = bins, col = \u0026#39;darkgray\u0026#39;, border = \u0026#39;white\u0026#39;) }) # Render counter text output[[demonstration_text]] \u0026lt;- renderText(paste( \u0026#34;You have clicked the draw button\u0026#34;, counter(), \u0026#34;times. Congrats!\u0026#34; )) }   Notice a few things here:\n Our function needs to know the objects counter, input and output to work. Also we need to switch to double-bracket notation for assigning new variables like distPlot_panel1 to output. Obviously, we couldn\u0026rsquo;t use $ for assignment anymore but single-bracket notation like output[var_name] is for some reason forbidden in Shiny. At least, that\u0026rsquo;s what an error message will kindly tell you when you dare to use only one bracket.  So, all in all our server-side logic looks like this now\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  server \u0026lt;- function(input, output) { # Counter initialization counter \u0026lt;- reactiveVal(value = 0) counter2 \u0026lt;- reactiveVal(value = 0) # Plot Rendering observeEvent( input$draw_button_panel1, { render_my_plot(\u0026#34;panel1\u0026#34;, counter, input, output) } ) observeEvent( input$draw_button_panel2, { render_my_plot(\u0026#34;panel2\u0026#34;, counter2, input, output) } ) # Panel Switching observeEvent( input$change_view_panel1, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel2\u0026#34;) ) observeEvent( input$change_view_panel2, updateTabsetPanel(inputId = \u0026#34;my_tabs\u0026#34;, selected = \u0026#34;panel1\u0026#34;) ) }   The complete app that we have just build can be found here.\nClosing Alright, I hope this helps you to build your own small Shiny app. In my particular case, I had to use another cool function from the shinyjs package to update the text on the UI such that it appears in red for a second (in order for the user to notice what changes). And because I have the feeling that shinyjs has way more in store for us, I will end this already quite long blog post here and save that (exciting) story for another time. Hope you will be there when I talk about shinyjs.\n And with that I really mean today. When I built my Shiny app, I actually used code duplication. But in hindsight, I feel somewhat embarrassed to leave it as it is for this blog post. Thus, I figured out how to make it work with a function.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recently built a small Shiny app where I had to search for how to do a lot of things. Here are 6 things I learned doing that. Maybe a Shiny beginner will find something useful in here.","id":0,"section":"post","tags":[],"title":"6 simple Shiny things I have learned from creating a somewhat small app","uri":"https://albert-rapp.de/post/2021-11-21-a-few-learnings-from-a-simple-shiny-app/"},{"content":"In this week\u0026rsquo;s TidyTuesday, I noticed that I am frequently not using only ggplot2 to create plots. In fact, it has become essential to me to leverage the powers of other great additional packages that align well with ggplot2. Therefore, I decided to extend my ggplot2-tips series by introducing a few packages I use quite often.\nIn this post, I want to cover how to arrange multiple plots. In particular, I will talk about the fantastic patchwork package by Thomas Lin Pedersen which helps to arrange plots quite intuitively. Further, I want to take a glance at ggforce, another package written by the same author as patchwork, because it also has a neat function for arranging plots. However, ggforce can do way more and I will demonstrate that in another installment of this series.\nSo, let us begin by creating a data set we want to fiddle with for plotting purposes. For simplicity, let us use the penguins data (without missing values) from the palmerpenguins package.\n1 2 3 4 5  library(tidyverse) theme_set(theme_light()) # All missing values can be filtered out by filtering the `sex` variable dat \u0026lt;- palmerpenguins::penguins %\u0026gt;% filter(!is.na(sex))   Arrange Plots via patchwork Often, we want to show multiple plots that tell a story when looked at together. Using patchwork, we can easily compose a single plot consisting of subplots. This is done by using the simple symbols + resp. / to display plots next to resp. on top of each other.\nFor demonstration purposes, let us generate a few simple plots.\n1 2 3 4  point_plot \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, flipper_length_mm, fill = sex)) + geom_jitter(size = 3, alpha = 0.5, shape = 21) point_plot   1 2 3 4  point_plot2 \u0026lt;- dat %\u0026gt;% ggplot(aes(bill_length_mm, bill_depth_mm, fill = sex)) + geom_jitter(size = 3, alpha = 0.5, shape = 21) point_plot2   1 2 3 4 5  # plot_plot is obviously a fun name boxplot_plot \u0026lt;- dat %\u0026gt;% ggplot(aes(x = body_mass_g, fill = sex)) + geom_boxplot() boxplot_plot   Clearly, showing each plot separately is boring and may not tell a story convincingly. Possibly, here you may want to say that the length and depth measurements give no clear distinction between male and female penguins but the weight measurements offers a better distinguishabilty between sexes. Maybe, if we see all plots together, we can tell that story without boring the reader.\n1 2 3  library(patchwork) p \u0026lt;- (point_plot + point_plot2) / boxplot_plot p   See how I have used + to put the point plots next to each other and / to plot the boxplots below the two point plots. Obviously, that was super easy and neat. But this simple arrangement leads to a doubling of the legends which is somewhat bothersome. However, this is no cause for concern. plot_layout() is there to collect those legends for you.\n1  p + plot_layout(guides = \u0026#34;collect\u0026#34;)   Of course, this leaves you with two legends which is kind of superfluous. The easy way to get rid of this is to plot no legends for the boxplots.\n1 2 3  boxplot_plot \u0026lt;- boxplot_plot + guides(fill = \u0026#34;none\u0026#34;) p \u0026lt;- (point_plot + point_plot2) / boxplot_plot p + plot_layout(guides = \u0026#34;collect\u0026#34;)   Now, what about legend positioning? Well, we already know how that usually works for a single plot (via theme() in case you forgot) and the good news is that the exact same thing works with patchwork as well. But beware to apply an additional theme() layer to the whole plot and not just to the last plot added to our composed plot. To make sure that happens, we have to add this layer via \u0026amp;.\n1  p + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;)   By the same logic, we can make additional changes to the whole plot e.g. to change the color mapping.\n1 2 3 4  p + plot_layout(guides = \u0026#34;collect\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   Next, let us control the layout a bit more and annotate the plot with plot_annotation().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  (point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3))) / boxplot_plot + plot_layout(guides = \u0026#34;collect\u0026#34;, heights = c(0.4, 0.6)) + plot_annotation( title = \u0026#34;Look at that arrangement!\u0026#34;, subtitle = \u0026#34;Wow\u0026#34;, caption = \u0026#34;Olà.\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;(\u0026#34;, tag_suffix = \u0026#34;)\u0026#34; ) \u0026amp; labs(fill = \u0026#34;Penguin sex\u0026#34;) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   We did quite a lot here, so let\u0026rsquo;s recap:\n We changed the widths of the plots in the first row by passing a vector of relative widths to widths in plot_layout(). Same thing with heights in plot_layout() to make the boxplots larger. Renamed legend label with the regular labs() function. Added a title, subtitle, caption and tags to the whole plot with plot_annotation().  Also, if you want to have the tags to only label the upper and lower row, you may want to wrap the first row together by wrap_elements(). Think of this as creating a new single unit.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  wrapped_plots \u0026lt;- wrap_elements( point_plot + point_plot2 + plot_layout(widths = c(0.7, 0.3)) ) (wrapped_plots) / boxplot_plot + plot_layout(guides = \u0026#34;collect\u0026#34;, heights = c(0.4, 0.6)) + plot_annotation( title = \u0026#34;Look at that arrangement!\u0026#34;, subtitle = \u0026#34;Wow\u0026#34;, caption = \u0026#34;Olà.\u0026#34;, tag_levels = \u0026#34;A\u0026#34;, tag_prefix = \u0026#34;(\u0026#34;, tag_suffix = \u0026#34;)\u0026#34; ) \u0026amp; theme(legend.position = \u0026#34;top\u0026#34;) \u0026amp; scale_fill_brewer(palette = \u0026#34;Dark2\u0026#34;)   Notice how the upper row reinstated the default colors and has two legends. This demonstrates how wrap_elements() made the plots \u0026ldquo;independent\u0026rdquo; from the overall theming via \u0026amp;, so to speak. On the bright side, there is no (C) tag anymore.\nUnsurprisingly, patchwork can do much more but for starters I think the previous examples will already get you quite far. They are you \u0026ldquo;80/20 leverage points\u0026rdquo;, if you will. But in order to add one more neat feature, let me finish our intro to patchwork by showing you how to create plots in plots via insets.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  # Tweak boxplots a bit for better visual fit to point_plot plt \u0026lt;- boxplot_plot + theme_minimal() + coord_flip() + theme(plot.background = element_rect(fill = \u0026#34;grey80\u0026#34;)) point_plot + coord_cartesian(xlim = c(25, 60)) + inset_element( plt, left = 0.01, right = 0.4, top = 0.99, bottom = 0.6 )   Create Subplots via ggforce I really enjoy arranging plots with patchwork because, to me, the syntax feels quite intuitive (mostly). However, as you probably noticed, I had to design each subplot and arrange them by hand. Clearly, if I want to use a grid-like arrangement to display each combination of two variables from a given set of variables, this may become tedious.\nLuckily, there is the ggforce package that has a neat faceting function to accomplish just that. As was already mentioned above, apart from that, the ggforce package offers even more cool stuff which we will look at in a future blog post.\nWith facet_matrix() it becomes quite easy to get a grid of subplots to display multiple combinations of two variables. For instance, take a look at this.\n1 2 3 4 5 6 7  library(ggforce) dat %\u0026gt;% ggplot(aes(x = .panel_x, y = .panel_y, fill = sex)) + geom_point(alpha = 0.5, size = 2, shape = 21) + facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g) )   Now, while this is not a particular beautiful plot, it gives us a quick overview of interesting variables which might be great for an exploratory analysis. Notice how we had to use .panel_x and .panel_y as placeholder for the individual variables. We could use the geom_auto*() functions to avoid typing that as they default to the correct values for x and y. Consequently, we could have written\n1 2 3 4 5 6  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g) )   With a little bit of tweaking, we can make this plot more interesting. For example. it would be neat if we had density plots on the diagonal. No problem! Add another geom_autodensity() layer and make sure that facet_matrix() understands to map only this layer to the diagonal subplots.\n1 2 3 4 5 6 7 8  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1 geom_autodensity(alpha = 0.5, position = \u0026#34;identity\u0026#34;) + # Layer 2 facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g), layer.diag = 2 )   See how layer.diag = 2 maps the diagonal elements to the second line of geom_* code. Similarly, we can manipulate the content of the upper and lower triangle in this grid by changing layer.lower or layer.upper in facet_matrix(). Let\u0026rsquo;s add another layer to see that in action.\n1 2 3 4 5 6 7 8 9 10  dat %\u0026gt;% ggplot(aes(fill = sex)) + geom_autopoint(alpha = 0.5, size = 2, shape = 21) + # Layer 1 geom_autodensity(alpha = 0.75, position = \u0026#34;identity\u0026#34;) + # Layer 2 geom_hex(aes(x = .panel_x, y = .panel_y), alpha = 0.75) + # Layer 3 facet_matrix( vars(bill_length_mm, flipper_length_mm, bill_depth_mm, body_mass_g), layer.diag = 2, layer.lower = 3 )   Last but not least, let me mention that we can also easily create what is called an \u0026ldquo;asymmetric grid\u0026rdquo; in ggforce by mapping rows and columns manually. This is great for having categorical variables on one axis and numerical variables on the other axis.\n1 2 3 4 5 6 7 8 9  dat %\u0026gt;% ggplot() + geom_boxplot( aes(x = .panel_x, y = .panel_y, group = .panel_x) ) + facet_matrix( cols = vars(sex, species), rows = vars(bill_depth_mm:body_mass_g) )   Beware that geom_boxplot() is a bit tricky as it requires the group argument to be explicitly set. Furthermore, if you want to add another aesthetic, e.g. fill, you will have to set group via interaction().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  dat %\u0026gt;% ggplot() + geom_boxplot( aes( x = .panel_x, y = .panel_y, fill = island, group = interaction(.panel_x, island) ) ) + facet_matrix( cols = vars(sex, species), rows = vars(bill_depth_mm:body_mass_g) )   This concludes our short summary of possibilities to arrange plots. In the next post of this ggplot2-tips series we will take a closer look at ggforce. I hope you enjoyed today\u0026rsquo;s blog post and I look forward to \u0026ldquo;see\u0026rdquo; you at my next blog post. In the meantime, feel free to leave a comment or a click on the applause button below.\n","description":"The patchwork and ggforce packages can be used to compose plots from multiple subplots. Let's have a look at how that works.","id":1,"section":"post","tags":["visualization"],"title":"ggplot tips: Arranging plots","uri":"https://albert-rapp.de/post/2021-10-28-extend-plot-variety/"},{"content":"Currently, I am quite curious about interactive plots which is why I am reading Scott Murray\u0026rsquo;s highly recommendable book on D3. For those of you who don\u0026rsquo;t know it, D3.js is a JavaScript library that is great for creating amazing interactive Data-Driven-Documents on the web.\nUnfortunately, compared to ggplot2, D3\u0026rsquo;s learning curve feels quite steep and I am not yet able to work with it yet. Fortunately, there are other interactive graphing libraries out there that I can use to get a first feel for creating interactive plots. For instance, there is another JavaScript library plotly.js, which is built on top of D3 and can be easily used in R through the plotly package.\nTherefore, I decided to play around with this R package in the hope of figuring out how it works. Also, I summarized what I played around with in this blog post by writing what I like to call an \u0026ldquo;exploratory introduction\u0026rdquo;.\nAs the name implies, this is not really a formal introduction to plotly and more of an experience report. Nevertheless, I suspect that this can be useful for people who have already knowledge about ggplot2 and want to get started with plotly as well.\nFurther, in case you do not want to read my informal commentary, you can also check out the video version of this blog post. Think of it as a summary of this blog post.\nAs of now, I plan on doing a similar exploratory introduction to r2d3 which is an R interface to D3. Possibly, this will then be combined with knowledge I gathered from Scott Murray\u0026rsquo;s book.\nOne Side Note before We Dive in According to Wikipedia, JavaScript \u0026ldquo;is one of the core technologies of the World Wide Web\u0026rdquo;. Surprisingly, JavaScript\u0026rsquo;s ubiquity on the interwebs also messed with this blog\u0026rsquo;s theme template such that font formatting was completely destroyed when I included an interactive plotly plot.\nTo work around this issue, I had to save the plots in an auxillary html-file and include it as a separate frame. This is why you will find that, here, all plots are saved in a variable (for later export) even if it does not really make sense to save it for plotting purposes.\nFinally, be aware that, for some reason, interactive plotly plots are quite large such that it might take some time to load them all.\nFrom ggplot2 to plotly Let us begin by transforming a ggplot to a plotly plot. Using a built-in function from the plotly package, it is straight-forward to convert a ggplot.\n1 2 3 4 5 6 7 8  library(tidyverse) library(plotly) p \u0026lt;- mpg %\u0026gt;% ggplot(aes(hwy, cty, fill = class)) + geom_jitter(shape = 21, size = 2, alpha = 0.5) plotly_p \u0026lt;- ggplotly(p) plotly_p   \rMarvel at what one can do here:\n Get additional information by hovering your cursor over a point Filter classes by clicking on the corresponding legend items Click and draw a rectangle with your cursor to zoom into the plot  But there is one thing that bothers me. If I move the cursor across the plot window, then the plotly options bar at the top of the window overlaps the \u0026ldquo;class\u0026rdquo; legend label. Can I fix this by moving the legend to the bottom?\n1 2  p \u0026lt;- p + theme(legend.position = \u0026#34;bottom\u0026#34;) ggplotly(p)   \rHmm, it appears that the legend cannot be moved by this. Do other changes in theme() get registered? Let\u0026rsquo;s check by applying a theme.\n1 2  p \u0026lt;- p + theme_light() ggplotly(p)   \rApparently, at least some changes will be conducted. Let\u0026rsquo;s see, if I can use layout() as shown in the getting started documentation of plotly to move the legend.\nThe R documentation of the layout() function is somewhat minimalistic. Basically, it refers to the online plotly reference manual. Using the table of content on that website, one can easily find the layout options.\nI believe the options xanchor and yanchor are exactly what I need as both of them can be set to options like left, bottom, etc. Though, I wonder why the value auto was not changed by ggplotly() as the initial plot p contains legend.position = \u0026quot;bottom\u0026quot;.\nIn any case, I am not a hundred percent sure what kind of syntax layout() requires. An example in the R documentaion would have been nice. Well, let\u0026rsquo;s try the straight-forward approach then.\n1 2 3 4 5 6  p %\u0026gt;% ggplotly() %\u0026gt;% layout(xanchor = \u0026#34;center\u0026#34;) ## Warning: \u0026#39;layout\u0026#39; objects don\u0026#39;t have these attributes: \u0026#39;xanchor\u0026#39; ## Valid attributes include: ## \u0026#39;font\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;uniformtext\u0026#39;, \u0026#39;autosize\u0026#39;, \u0026#39;width\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;margin\u0026#39;, \u0026#39;computed\u0026#39;, \u0026#39;paper_bgcolor\u0026#39;, \u0026#39;plot_bgcolor\u0026#39;, \u0026#39;separators\u0026#39;, \u0026#39;hidesources\u0026#39;, \u0026#39;showlegend\u0026#39;, \u0026#39;colorway\u0026#39;, \u0026#39;datarevision\u0026#39;, \u0026#39;uirevision\u0026#39;, \u0026#39;editrevision\u0026#39;, \u0026#39;selectionrevision\u0026#39;, \u0026#39;template\u0026#39;, \u0026#39;modebar\u0026#39;, \u0026#39;newshape\u0026#39;, \u0026#39;activeshape\u0026#39;, \u0026#39;meta\u0026#39;, \u0026#39;transition\u0026#39;, \u0026#39;_deprecated\u0026#39;, \u0026#39;clickmode\u0026#39;, \u0026#39;dragmode\u0026#39;, \u0026#39;hovermode\u0026#39;, \u0026#39;hoverdistance\u0026#39;, \u0026#39;spikedistance\u0026#39;, \u0026#39;hoverlabel\u0026#39;, \u0026#39;selectdirection\u0026#39;, \u0026#39;grid\u0026#39;, \u0026#39;calendar\u0026#39;, \u0026#39;xaxis\u0026#39;, \u0026#39;yaxis\u0026#39;, \u0026#39;ternary\u0026#39;, \u0026#39;scene\u0026#39;, \u0026#39;geo\u0026#39;, \u0026#39;mapbox\u0026#39;, \u0026#39;polar\u0026#39;, \u0026#39;radialaxis\u0026#39;, \u0026#39;angularaxis\u0026#39;, \u0026#39;direction\u0026#39;, \u0026#39;orientation\u0026#39;, \u0026#39;editType\u0026#39;, \u0026#39;legend\u0026#39;, \u0026#39;annotations\u0026#39;, \u0026#39;shapes\u0026#39;, \u0026#39;images\u0026#39;, \u0026#39;updatemenus\u0026#39;, \u0026#39;sliders\u0026#39;, \u0026#39;colorscale\u0026#39;, \u0026#39;coloraxis\u0026#39;, \u0026#39;metasrc\u0026#39;, \u0026#39;barmode\u0026#39;, \u0026#39;bargap\u0026#39;, \u0026#39;mapType\u0026#39;   Well, that didn\u0026rsquo;t go as expected, but the warning displays valid attributes and legend is among them. Upon closer inspection, I also realize that, according to the reference manual, xanchor and yanchor have parent layout.legend. So, I guess, I will have to use this somehow. Maybe, pass a list to legend via layout()?\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(xanchor = \u0026#34;center\u0026#34;)) p_layout   \rAha! At least this did not crash again but the resulting plot does not look as I would expect it to. Let\u0026rsquo;s see what happens when we change yanchor as well and then we\u0026rsquo;ll go from there.\n1 2 3 4 5 6 7  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list( xanchor = \u0026#34;center\u0026#34;, yanchor = \u0026#34;bottom\u0026#34; )) p_layout   \rUnsurprisingly, this did not help at all. In retrospect, I don\u0026rsquo;t know how I could think that changing yanchor as well would magically cure things. Again, referring back to the manual (I should really read the descriptions instead of relying on the possible values), it appears that xanchor only sets a reference points for the option x. Same thing for yanchor and y.\nSo, how about changing x and y instead? Possible values for both options range from -2 to 3, so my best guess is that ranges from 0 to 1 refer to the window the points are plotted in. Consequently, moving the legend to the bottom could be as easy as using a positive x value and negative y value which are both close to zero.\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(x = 0.1, y = -0.1)) p_layout   \rNow, this brings us closer to what I had in mind when I set legend.position = \u0026quot;bottom\u0026quot; in theme(). Possibly, we can change the orientation of the legend from vertical to horizontal, tweak the x and y values a bit and then we\u0026rsquo;re there. Scrolling through the manual (again), reveals that there is an option orientation which can be set to h. This sounds promising.\n1 2 3 4 5 6 7 8  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list( x = 0.1, y = -0.2, orientation = \u0026#34;h\u0026#34; )) p_layout   \rNice! Finally, I am satisfied. Out of curiosity, let us investigate what had happened, if we had set orientation to \u0026quot;h\u0026quot; from the start.\n1 2 3 4  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;)) p_layout   \rThis already looks nice enough, so our manual tweaking was not technically necessary. But then again, this does not recreate what legend.position = \u0026quot;bottom\u0026quot; usually does. Now that we understand how layout() works, we can roam the reference manual and try to tweak the legend box. Let\u0026rsquo;s try to change a couple of things. This does not have to be pretty, we only want to see how plotly works.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  p_layout \u0026lt;- p %\u0026gt;% ggplotly() %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, borderwidth = 3, bgcolor = \u0026#34;grey\u0026#34;, bordercolor = \u0026#34;red\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ), title = list( text = \u0026#34;Class\u0026#34;, side = \u0026#34;top\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ) ) ) ) p_layout   \rNote how we have used lists in lists (in lists) to customize the legend. Interestingly, our initial blunder of ignoring the \u0026ldquo;parent\u0026rdquo; resp. the hierarchy of options earlier helped to understand that as an option\u0026rsquo;s parent\u0026rsquo;s name gets longer, e.g. layout.legend.title.font, we will have to use more convoluted lists to change that option.\nCreating a Plotly Chart Manually Since we have learned how to tweak a plotly object, we might as well figure out how to create one without having to use ggplotly(). It is not that I want to avoid using ggplot altogether but, in principle, it cannot hurt if we can understand plotly\u0026rsquo;s implementation in the R package plotly.\nSo, from the book Interactive web-based data visualization with R, plotly, and shiny I gather that the plotly R package implements the JavaScript plotly.js library via a Grammar of Graphics approach. Thus, it works similar to ggplot2 in the sense that we can add layers of graphical objects to create a plot. In plotly\u0026rsquo;s case, we pass a plotly object from one add_* layer to the next (via %\u0026gt;%).\n1 2 3 4  plt \u0026lt;- mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class) plt   \rNotice the ~. These are used to ensure that the variables are mapped from the data (similar to aes() in ggplot2). Alternatively, one could also use x = mpg$hwy to create the same plot.\nBecause we can see a lot of overplotting, let us jitter the points. Unfortunately, I couldn\u0026rsquo;t find a built-in option for that. Therefore, let\u0026rsquo;s do the jittering manually.\n1 2 3 4 5 6 7 8 9 10 11 12  set.seed(123) jitter_hwy \u0026lt;- 2 jitter_cty \u0026lt;- 1 jittered_mpg \u0026lt;- mpg %\u0026gt;% mutate( hwy = hwy + runif(length(hwy), -jitter_hwy, jitter_hwy), cty = cty + runif(length(cty), -jitter_cty, jitter_cty) ) plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class) plt   \rRegarding the customization of the points aka markers, we can pass a list of options (taken from the reference manual again) to the marker argument in add_markers().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  set.seed(123) plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers( x = ~hwy, y = ~cty, color = ~class, marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) plt   \rAlternatively, and what I find surprisingly convenient, we can leave our initial plotly object as it is and pass it to the style() function. This functions works just like the layout() function we have seen before.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers( x = ~hwy, y = ~cty, color = ~class ) %\u0026gt;% style( marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) plt   Similarly, we could pass this along to layout() if we want to customize the legend box again.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  plt_layout \u0026lt;- plt %\u0026gt;% layout( legend = list( orientation = \u0026#34;h\u0026#34;, borderwidth = 3, bgcolor = \u0026#34;grey\u0026#34;, bordercolor = \u0026#34;red\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ), title = list( text = \u0026#34;Class\u0026#34;, side = \u0026#34;top\u0026#34;, font = list( color = \u0026#34;white\u0026#34;, family = \u0026#34;Gravitas One\u0026#34;, size = 15 ) ) ) ) plt_layout   \rInteresting! For some obscure reason the exact same legend command behaves differently now. Honestly, I have no clue what is going on here. If I had to hazard a guess, I would say that during the conversion of a ggplot object to a plotly object via ggplotly() some default values were implemented that cause the change but this is just a hunch. Possibly, this is connected to xanchor or yanchor.\nIn any case, we got a glimpse of how the plotly R package works which uses the JavaScript library plotly.js to create interactive plots. Also, we have learned how to convert a ggplot2 object to a plotly object and how we can customize this further.\nInterestingly, when converting from ggplot2 to plotly, the pop-up window that appears when you hover over a point is already customized compared to the default from plot_ly(). Did you notice the difference already?\nSo, in order to end our \u0026ldquo;exploratory introduction\u0026rdquo; let us adjust the hovertemplate according to the description in the reference manual. Here, we will use \\n for line breaks.\n1 2 3 4  plt \u0026lt;- plt %\u0026gt;% style(hovertemplate = \u0026#34;hwy: %{x:.2f}\\ncty: %{y:.2f}\u0026#34;) %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;, y = -0.2)) plt   \rNotice how the class labels appear outside of the box. The reference manual refers to this position as \u0026ldquo;secondary\u0026rdquo; box. To get rid of this, we simply add \u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt; to our hover template. Unfortunately, it appears as if what is not displayed in the primary box cannot be used as part of the hover template.\nThus, we cannot use %{color}. Instead, we simply map class to the text attribute of the markers as well. Then, we can use %{text}.\n1 2 3 4 5 6 7 8 9 10 11 12 13  plt \u0026lt;- jittered_mpg %\u0026gt;% plot_ly() %\u0026gt;% add_markers(x = ~hwy, y = ~cty, color = ~class, text = ~class) %\u0026gt;% style( marker = list( size = 8, opacity = 0.6, line = list(color = \u0026#34;black\u0026#34;, width = 2) ) ) %\u0026gt;% style(hovertemplate = \u0026#34;hwy: %{x:.2f}\\ncty: %{y:.2f}\\nclass: %{text} \u0026lt;extra\u0026gt;\u0026lt;/extra\u0026gt;\u0026#34;) %\u0026gt;% layout(legend = list(orientation = \u0026#34;h\u0026#34;, y = -0.2)) plt   \rAll right! That\u0026rsquo;s enough exciting plotting action for today. Hope you enjoyed this blog post and see you next time.\n","description":"We try to do a few simple things with the plotly package in order to figure out how it works.","id":2,"section":"post","tags":["visualization","exploratory-intro"],"title":"An Exploratory Introduction to the Plotly Package","uri":"https://albert-rapp.de/post/2021-10-16-exploratory-intro-plotly/"},{"content":"A bit more than two weeks ago, Germany held a federal election and, naturally, this is always reason for a lot of discussions and subjective truths. One subjective truth I encountered myself related to how fast the party CDU/CSU was able to collect and lose votes according to polls right before the election.\nAccording to the Allensbach Institute, a private polling institute based in Allensbach, Baden-Württemberg, on July 20th, approximately three months before the official election, the CDU/CSU could get 31.5% of the votes1. Almost four weeks later on the 19th of August, the Allensbach institute forecast only 27.5% for the CDU/CSU.\nAt that time, I had the subjective feeling that it was quite common to assume that the CDU/CSU is on a steep downward spiral. In the end, the CDU/CSU was able to slow its downward course and got 24.1% of the votes in the election. While this is still an abysmal outcome for this party, I was surprised that it was not worse.\nA \u0026ldquo;similar\u0026rdquo; surprising tale can be told for other parties too. For instance, the party SPD was gaining a lot of steam in the last three months of the election campaign and the party DIE GRÜNE, once surprisingly popular during the election, lost a lot of votes towards the end as well.\nAll of these ups and downs left a feeling of rapid change for some. For example, last weekend I had an interesting discussion about whether voters no longer cast their votes according to \u0026ldquo;belief\u0026rdquo; but are much more influenced by the spur of the moment and flip-flop back and forth between parties depending on who is making the headlines at that time. Consequently, I decided that this might be something worth looking at with data.\nThus, this blog post tries to look at historic data from election polls to see if this year\u0026rsquo;s change before the election is indeed something unprecedented. If this is so, then that might support that voters become more impulsive. So, this is why I scraped election polls2 since 1998 from the Allensbach institute and the Kantar (Emnid) institute whose election polls can be found publicly here3.\nData Warning I believe it is worth pointing out that the polling results will have to be taken with a grain of salt. Especially the fact that people might judge their current preference differently in non-election years compared to election years has to be taken into account. Obviously, I suspect that the polling institutes considered this as part of their forecast but nevertheless it cannot hurt to mention potential caveats.\nPopularity Over Time This being said, let\u0026rsquo;s take a look at the six (currently) largest parties and their popularity over time. Here, I will only look at the data from the Allensbach institute as these are already quite a lot of data points and the picture might get messy otherwise. As this is an election-focused blog post, I took the liberty of labeling only the election years on the x-axis4.\nSome recent trends are detectable but in this particular figure, I don\u0026rsquo;t see anything that points to an increased volatility in recent times.\nWhat Happens Close to an Election? Instead of looking at the overall fluctuations, we could look at the last three months before an election. Since an election takes places in September, in the next plot I have depicted only the polling results in the months July, August and September in an election year. To make trends more visible, I have added a regression line for each party and each election.\nInterestingly, the most recent election seems to have had more volatile last three months compared to previous elections. Indeed, this could indicate more impulsive voteing behavior but I am not entirely convinced yet.\nThree-Month Volatility To see if the change in the last three months of the most recent election is truly something out of the ordinary, we need context. In order to get this context, let us consult our historical data again and compute the average share of votes for each party in every quarter of every year. Then, hopefully, these computed mean percentages represent the mood of the majority of people in a given quarter and we can see how much these means change from quarter to quarter.\nTaking the fluctuations over time into account, the quarterly change right before this year\u0026rsquo;s election looks less extreme. In fact, most of the parties have had more extreme or similar changes on a quarterly basis in the past.\nClearly, the fact that we aggregate the share of votes over a period of three months could potentially obscure fluctuations. But as the next plot shows, if we do the same thing but aggregate on a monthly basis, then the overall impression of the new monthly plot is the same as with the original quarterly plot.\nCoefficient of Variation Before we try to make sense of what all that we have seen could mean in terms of voters' impulsiveness, let us take one more stab at trying to measure the volatility. This time, let us compute the coefficient of variation (CV)5 of the mean weekly share of votes for each quarter and each party and display this over time.\nExcept for the AfD and SPD, no profound trend in the CV can be detected as most regression lines appear to have a slope that is close to zero. Further, the AfD\u0026rsquo;s decrease in its monthly CV might be explained by the fact that it is a comparatively new party which means that might not have had a solid voter base in the beginning.\nSince we are interested in more recent voter behavior, let us try to take only the data since the beginning of 2013 into account. This should give us an impression about the three most recent elections.\nOverall, as the confidence bands of the regression lines indicate that the real slope of the lines might as well be close to zero, I find it hard to argue either way about a more impulsive voting behavior.\nConclusion So, we have observed something odd here. In previous elections, a party\u0026rsquo;s changes in popularity in the last three months were not as profound as the last quarter\u0026rsquo;s changes that we witnessed this year. Overall, however, the degree of how much changed right before the election is nothing that could not be witnessed in the past at other non-election times.\nIf I had to guess, I would say that this might indicate that over the last 20 years, people\u0026rsquo;s willingness to vote for a different party remained somewhat similar. But it appears like the moment, when voters eventually decide for a party, has been moved closer to the election itself. Thus, one might argue that it has become harder to pinpoint which party one feels most connected to and this leads to an indecision right until the end. This indecision could indeed lead to a flipping back and forth between two or three parties one feels similarly connected to. But this, I would argue, is a sign of people lacking a strong connection to one single party and not one of impulsiveness.\nWhat do you think? Is there more that we can extract from this kind of data? As I am quite new to this type of analysis, I am always glad about suggestions about how to improve. If you want to share your ideas, feel free to send me an e-mail or leave a message in the comment section. As always, if you liked this blog post, I would appreciate a hit on the applause button below.\n The data from the Allensbach institute can be found online here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n The resulting data can be found here and the script I used to extract the data is here\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n There are a couple of more polling institutes available online but I decided to not scrape all of their results to save time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n For better legibility, I have tried to use the colors the parties are usually associated with. In some instances, using only one (primary) color resulted in a hard to read plot. Thus, whenever possible, I have consulted online party guidelines to find colors they use in their own publications.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n This quantity is defined as the sample standard deviation divided by the sample mean. See also Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"We try to find out if voters in Germany became more impulsive over time.","id":3,"section":"post","tags":[],"title":"Did German Voters Become More Impulsive?","uri":"https://albert-rapp.de/post/2021-10-03-sonntagsfrage/"},{"content":"This week, I had to deal with two very similar tasks on two very similar but not identical data sets that required me to write a function that is versatile enough to deal with both data sets despite their subtle differences. The differences that had to be accounted for mainly related to using functions in the two cases that relied on differently many arguments. Also, some of the column names were different which meant that I could not hard-code the column names into the function I was creating.\nConsequently, I had to use a few non-standard concepts (at least not standard to me) that enabled me to create the function which did everything I asked it to do. Since these concepts seemed interesting to me, I decided to implement a small example resulting in this blog post. Actually, I was even motivated to create a video for this blog post. You can find it on YouTube.\nWhat We Want To Achieve The aim of this example is to write a function that can create two tibbles that are conceptually similar but do not necessarily use the same column names or compute the existing columns in the same way. For this blog post, I have already set up two dummy data sets like that so that we can see what we want to do.\nLet\u0026rsquo;s take a look at these data sets I creatively called dat_A and dat_B.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  library(tidyverse) dat_A %\u0026gt;% head(3) ## # A tibble: 3 x 3 ## mu sigma dat  ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;  ## 1 -1 1 \u0026lt;tibble [5 x 2]\u0026gt; ## 2 -1 1.5 \u0026lt;tibble [5 x 2]\u0026gt; ## 3 -1 2 \u0026lt;tibble [5 x 2]\u0026gt; dat_B %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## lambda dat  ## \u0026lt;dbl\u0026gt; \u0026lt;list\u0026gt;  ## 1 0.5 \u0026lt;tibble [5 x 2]\u0026gt; ## 2 0.7 \u0026lt;tibble [5 x 2]\u0026gt; ## 3 0.9 \u0026lt;tibble [5 x 2]\u0026gt;   As you can see, each tibble contains a column dat. This column consists of tibbles with multiple summarized stochastic processes which were simulated using parameters that are given by the remaining columns of dat_A and dat_B.\nYou probably have already noticed that the stochastic processes must have been simulated using differently many parameters since tibble A contains additional columns mu and sigma whereas tibble B can offer only one additional column lambda. However, even if differently many and differently named parameters are used, the logic of the generating function needs to be the same:\n Take parameters. Simulate stochastic processes with these parameters. Summarize processes  Thus, in step 1 the generating function which we want to code, needs to be versatile enough to handle different argument names and amounts. Next, let\u0026rsquo;s see what the dat column has in store for us.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  dat_A %\u0026gt;% pluck(\u0026#34;dat\u0026#34;, 1) %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## n proc_mean ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -1.01  ## 2 2 -0.958 ## 3 3 -0.968 dat_B %\u0026gt;% pluck(\u0026#34;dat\u0026#34;, 1) %\u0026gt;% head(3) ## # A tibble: 3 x 2 ## n proc_variance ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 5.27 ## 2 2 2.66 ## 3 3 3.08   First of all, notice that I accessed the first tibble in the dat column using the super neat pluck() function. In my opinion, this function is preferable to the clunky base R usage of $ and [[, e.g. like dat_A$dat[[1]].\nAs you can see, the tibbles that are saved in dat contain columns n and proc_mean resp. proc_variance. As hinted at before, each row is supposed to represent a summary of the n-th realization of a stochastic process.\nHowever, notice that the summary statistics in use are not the same! The different column names proc_mean and proc_variance indicate that in tibble A the sample mean was used whereas tibble B contains sample variances. Again, our function that generates tib_A and tib_B should be flexible enough to create differently named and differently computed columns.\nHelpful Concepts Now that we know what we want to create, let us begin by learning how to handle differently many arguments and their varying names.\ndot-dot-dot For these kinds of purposes, R offers the ...-operator (pronounced dot-dot-dot). Basically, it serves as a placeholder for everything you do not want to evaluate immediately.\nFor instance, have you ever wondered how dplyr\u0026rsquo;s select() function is able to select the correct column?1 If you\u0026rsquo;re thinking \u0026ldquo;No, but what\u0026rsquo;s so special about this?\u0026rdquo;, then you may want to notice that it is actually not that simple to define your own select() function even with the help of the dplyr function.\nThis is because defining an appropriate function to select two columns from, say, the iris data set cannot be done like this:\n1  my_select \u0026lt;- function(x, y) {select(iris, x, y)}   Now, if you want to use the function the same way you would use dplyr::select(), i.e. simply passing, say, Sepal.Width, Sepal.Length (notice no \u0026quot;\u0026quot;) to your new function, it would look like this\n1 2  my_select(Sepal.Width, Sepal.Length) #\u0026gt; Error: object \u0026#39;Sepal.Width\u0026#39; not found   This error appears because at some point, R will try to evaluate the arguments as variables from your current environment. But of course this variable is not present in your environment and only present within the iris data set. Therefore, what dplyr::select() accomplishes is that it lets R know to evaluate the input argument only later on, i.e. when the variable from the data set is \u0026ldquo;available\u0026rdquo;.\nThis is where ... comes into play. It is not by chance that select() only has arguments .data and .... Here, select() uses that everything which is thrown into ..., will be passed along to be evaluated later. This can save our my_select() function, too.\n1 2 3 4 5 6  my_select \u0026lt;- function(...) {select(iris, ...)} my_select(Sepal.Width, Sepal.Length) %\u0026gt;% head(3) ## Sepal.Width Sepal.Length ## 1 3.5 5.1 ## 2 3.0 4.9 ## 3 3.2 4.7   Works like a charm! This will help us to define a function that is flexible enough for our purposes. Before we start with that, let us learn about another ingredient we will use.\ncurly-curly If we were to only select a single column from iris using our my_select() function, we could have also written the function using {{ }} (pronounced curly-curly). It operators similar to ... in the sense that it allows for later evaluation but applies this concept to specific variable. Check out how that can be used here.\n1 2 3 4 5 6  my_select \u0026lt;- function(x) {select(iris, {{x}})} my_select(Sepal.Width) %\u0026gt;% head(3) ## Sepal.Width ## 1 3.5 ## 2 3.0 ## 3 3.2   What\u0026rsquo;s more the curly-curly variables - curly-curlied variables (?) - can also be used later on for stuff like naming a new column. For example, let us modify our previous function to demonstrate how that can be used.\n1 2 3 4 5 6 7 8 9 10  select_and_add \u0026lt;- function(x, y) { select(iris, {{x}}) %\u0026gt;% mutate({{y}} := 5) # 5 can be replaced by some meaningful calculation } select_and_add(\u0026#34;Sepal.Width\u0026#34;, \u0026#34;variable_y\u0026#34;) %\u0026gt;% head(3) ## Sepal.Width variable_y ## 1 3.5 5 ## 2 3.0 5 ## 3 3.2 5   Mind the colon! Here, if you want to use y as column name later on you cannot use the standard mutate() syntax but have to use := instead.\nFunctional Programming One last thing that we will use, is the fact that R supports functional programming. Thus, we can use functions as arguments of other functions. For instance, take a look at this super simple, yet somewhat useless wrapper function for illustration purposes.\n1 2 3 4 5 6  my_simulate \u0026lt;- function(n, func) { func(n) } set.seed(564) my_simulate(5, rnorm) ## [1] 0.4605501 -0.7750968 -0.7159321 0.6882645 -2.0544591   As you just witnessed, I simply passed rnorm (without a call using ()) to my_simulate as the func argument such that rnorm is used whenever func is called. In our use case, this functionality can be used to simulate different stochastic processes (that may depend on different parameters).\nThe Implementation Alright, we have assembled everything we need in order to create our simulate_and_summarize_proc() function. In this example, the simulation of the stochastic processes will consist of simply calling rnorm() or rexp() but, of course, these functions can be substituted with arbitrarily complex simulation functions.\nWe will use n_simus as the amount of realizations that are supposed to be simulated and each realization will be of length TMax. Further, we will use ... to handle an arbitrary amount of parameters that are supposed to be passed to simulation_func. So, let\u0026rsquo;s implement the simulation part first (detailed explanations below).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  simulate_and_summarize_proc \u0026lt;- function(..., TMax, n_simus, simulation_func) { argslist \u0026lt;- list(n = TMax, ...) %\u0026gt;% map(~rep(., n_simus)) tibble( t = list(1:TMax), n = 1:n_simus, value = pmap(argslist, simulation_func) ) } set.seed(457) simulate_and_summarize_proc( mean = 1, sd = 2, TMax = 200, n_simus = 3, simulation_func = rnorm # arguments -\u0026gt; n, mean, sd ) ## # A tibble: 3 x 3 ## t n value  ## \u0026lt;list\u0026gt; \u0026lt;int\u0026gt; \u0026lt;list\u0026gt;  ## 1 \u0026lt;int [200]\u0026gt; 1 \u0026lt;dbl [200]\u0026gt; ## 2 \u0026lt;int [200]\u0026gt; 2 \u0026lt;dbl [200]\u0026gt; ## 3 \u0026lt;int [200]\u0026gt; 3 \u0026lt;dbl [200]\u0026gt;   As you can see, this created three (simple) stochastic processes of length 200 using the parameters mean = 1 and sd = 2. We can validate that the correct parameters were used once we implement the summary functions.\nFirst, let us address the tricky part in this function. In order to pass a list of arguments to pmap() that are then used with simulation_func, we first need to rearrange the lists a bit. After the first step, by simply putting everything from ... into the list we have a list like this:\n1 2 3 4 5  list(n = 100, mean = 1, sd = 2) %\u0026gt;% str() ## List of 3 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2   However, we will need to have each variable in the list repeated n_simus time in order to simulate more than one realization. Thus, we use map() to replicate:\n1 2 3 4 5 6 7  list(n = 200, mean = 1, sd = 2) %\u0026gt;% map(~rep(., 3)) %\u0026gt;% str() ## List of 3 ## $ n : num [1:3] 200 200 200 ## $ mean: num [1:3] 1 1 1 ## $ sd : num [1:3] 2 2 2   Note that calling rep() without map() does not cause an error but does not deliver the appropriate format:\n1 2 3 4 5 6 7 8 9 10 11 12 13  list(n = 100, mean = 1, sd = 2) %\u0026gt;% rep(3) %\u0026gt;% str() ## List of 9 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2 ## $ n : num 100 ## $ mean: num 1 ## $ sd : num 2   Next, let us take the current output and implement the summary. To do so, we will add another variables summary_name and summary_func to the function in order to choose a column name resp. a summary statistic.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  simulate_and_summarize_proc \u0026lt;- function(..., TMax, n_simus, simulation_func, summary_name, summary_func) { argslist \u0026lt;- list(n = TMax, ...) %\u0026gt;% map(~rep(., n_simus)) tibble( t = list(1:TMax), n = 1:n_simus, value = pmap(argslist, simulation_func) ) %\u0026gt;% # this part is added unnest(c(t, value)) %\u0026gt;% group_by(n) %\u0026gt;% summarise({{summary_name}} := summary_func(value)) } set.seed(457) simulate_and_summarize_proc( mean = 1, sd = 2, TMax = 200, n_simus = 5, simulation_func = rnorm, summary_name = \u0026#34;mega_awesome_mean\u0026#34;, summary_func = mean ) ## # A tibble: 5 x 2 ## n mega_awesome_mean ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 0.955 ## 2 2 0.932 ## 3 3 0.987 ## 4 4 1.07  ## 5 5 1.15   Finally, we can use our super versatile function in combination with map() to create dat_A and dat_B.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34  dat_A \u0026lt;- expand_grid( mu = seq(-1, 1, 0.25), sigma = seq(1, 3, 0.5) ) %\u0026gt;% mutate(dat = map2( mu, sigma, ~simulate_and_summarize_proc( mean = .x, sd = .y, TMax = 200, n_simus = 3, simulation_func = rnorm, summary_name = \u0026#34;proc_mean\u0026#34;, summary_func = mean ) )) dat_B \u0026lt;- expand_grid( lambda = seq(0.5, 1.5, 0.2) ) %\u0026gt;% mutate(dat = map( lambda, ~simulate_and_summarize_proc( rate = ., TMax = 200, n_simus = 3, simulation_func = rexp, summary_name = \u0026#34;proc_variance\u0026#34;, summary_func = var ) ))   Conclusion So, we have seen that we can combine {{ }}, ... and functional programming to create highly versatile functions. Of course, as always one might be tempted to say that one could have just programmed two different functions for our particular example.\nHowever, this would cause a lot of code duplication because a lot of steps are essentially the same which is hard to debug and maintain. Also, creating numerous functions does not scale well if we need to cover way more than two cases.\nWith that being said, I hope that you found this blog post helpful and if so, feel free to hit the comments or push the applause button below. See you next time.\n If you have read my YARDS lecture notes and this sounds familiar to you, you are absolutely right. I have reused and adapted a part of the \u0026ldquo;Choose Your Own Data Science Adventure\u0026rdquo;-chapter here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"Using concepts like dot-dot-dot and curly-curly we create functions that are more versatile and can be used in multiple settings.","id":4,"section":"post","tags":[],"title":"Writing Versatile Functions with R","uri":"https://albert-rapp.de/post/2021-09-16-similar-data-and-list-like-columns/"},{"content":"For a long time I have wondered why some people would use position_stack() for position alignment instead of the simpler version position = \u0026quot;stack\u0026quot;. Recently, though, I learned the purpose of the former approach when I tried to add data labels to a stacked bar chart for better legibility.\nFurther, I decided that this knowledge is a good addition to this ggplot2-tips series, so let\u0026rsquo;s see what position_stack() can do. To achieve this, let us create a small dummy data set.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  library(tidyverse) dummy_dat \u0026lt;- tibble( group = c(rep(\u0026#34;A\u0026#34;, 3), rep(\u0026#34;B\u0026#34;, 3)), category = factor( c(rep(c(\u0026#34;low\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;high\u0026#34;), 2)), levels = rev(c(\u0026#34;low\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;high\u0026#34;)), ), percent = c(0.41, 0.16, 1 - 0.41 - 0.16, 0.26, 1 - 0.26 - 0.36, 0.36) ) dummy_dat ## # A tibble: 6 x 3 ## group category percent ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 A low 0.41 ## 2 A medium 0.16 ## 3 A high 0.43 ## 4 B low 0.26 ## 5 B medium 0.38 ## 6 B high 0.36   Next, take a look at the corresponding stacked bar chart. Since we created a dataset that contains percentages, I took the liberty of appropriately transforming the y-axis via scale_y_continuous().\n1 2 3 4  dummy_dat %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + scale_y_continuous(labels = scales::percent_format())   I believe that this visualization could be improved by adding text labels to each part of the stacked bar chart in order for the reader to immediately detect how large each portion of the bars is. Let\u0026rsquo;s try this via simply converting the values to strings and adding geom_text() to the plot.\n1 2 3 4 5 6  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text(aes(label = percent_labels)) + scale_y_continuous(labels = scales::percent_format())   Clearly, this did not work as intended because geom_text() uses position = \u0026quot;identity\u0026quot; by default which is why the y-position of the labels is simply determined by its value. Now, here is where I would usually change the positioning via position = \u0026quot;stack\u0026quot;. However, the result this approach delivers is somewhat less than perfect.\n1 2 3 4 5 6  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text(aes(label = percent_labels), position = \u0026#34;stack\u0026#34;) + scale_y_continuous(labels = scales::percent_format())   Ideally, I would like the labels to appear in the middle of each colored block. We could try to use vjust to move the labels which is not a great idea since every label will be moved by the same amount and the blocks are of different height. Similarly, we could compute the block middle points by hand and use that as separate y-aesthetic in geom_text().\nClearly, this involves a tedious additional computation and we should avoid this, if possible. This is precisely where position_stack() comes in. Conveniently, using position = position_stack() stacks the bars just like position = \u0026quot;stack\u0026quot; does but the function position_stack() has another argument vjust by which we can move the labels individually.\nHere, the possible values of vjust range from 0 (bottom of the designated height) to 1 (top of the designated height). Therefore, moving the labels to the middle of each bar is as easy as setting vjust = 0.5.\n1 2 3 4 5 6 7 8 9  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text( aes(label = percent_labels), position = position_stack(vjust = 0.5) ) + scale_y_continuous(labels = scales::percent_format())   Finally, one may - and this is definitely a matter of taste - tweak this plot further by changing the color and text formatting. Personally, I like darker colors combined with a white, bold label. In this case, this would look like this.\n1 2 3 4 5 6 7 8 9 10 11 12  dummy_dat %\u0026gt;% mutate(percent_labels = scales::percent(percent)) %\u0026gt;% ggplot(aes(x = group, y = percent, fill = category)) + geom_col() + geom_text( aes(label = percent_labels), position = position_stack(vjust = 0.5), col = \u0026#34;white\u0026#34;, fontface = \u0026#34;bold\u0026#34; ) + scale_y_continuous(labels = scales::percent_format()) + scale_fill_brewer(palette = \u0026#34;Set1\u0026#34;)   In summary, we have seen that using position = position_stack() is a more powerful alternative to position = \u0026quot;stack\u0026quot; that allows individual positioning. Nevertheless, as long as the additional arguments of position_stack() are not needed I still find the latter version simpler.\n","description":"We take a look at the differences between position = 'stack' and position = position_stack().","id":5,"section":"post","tags":["visualization"],"title":"ggplot tips: Using position_stack() for Individual Positioning","uri":"https://albert-rapp.de/post/2021-09-11-position-adjustment/"},{"content":"This blog post is part of a series I am creating where I collect tips I found useful when I first learned to work with ggplot2. All posts which are part of this series can be found here. In this post I want to deal with how to manually or automatically create labels for some aesthetic.\nManually Assigning Labels Assigning labels by hand, e.g. via col = \u0026quot;some label\u0026quot;, can be a great idea in some instances. For example, when you use two different smoothing methods, a hand-written label to differentiate between the two methods helps a lot. For instance, take a look the relationship between city mileage cty and highway mileage hwy of cars in the mpg data set from the ggplot2 package.\n1 2 3  library(tidyverse) ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5)   If one suspects a linear relationship between those two variables, one might want to use geom_smooth(method = 'lm') to check that hypothesis by drawing a straight line through the points. Similarly, one may be inclined to see what geom_smooth() would return if a linear model is not enforced. Adding both smoothing methods to the plot (and removing the confidence bands) yields:\n1 2 3 4  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, se = F, size = 1.5)   Obviously, differently colored lines should be used here to differentiate between the two smoothing methods. We have two approaches to do this. Either, we can manually assign a color (without using aes()):\n1 2 3 4  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(se = F, size = 1.5, col = \u0026#39;red\u0026#39;) + geom_smooth(method = \u0026#39;lm\u0026#39;, se = F, size = 1.5, col = \u0026#39;blue\u0026#39;)   Or we can use aes() and assign labels instead and let ggplot2 handle the colors on its own.\n1 2 3 4 5  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(aes(col = \u0026#39;auto\u0026#39;), se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, aes(col = \u0026#39;lm\u0026#39;), se = F, size = 1.5) + labs(col = \u0026#39;Smoothing\u0026#39;)   Personally, I prefer the latter approach because it has a couple of small advantages\n A legend is automatically generated with the corresponding labels such that even without looking at the code it becomes more obvious how each line was generated. Also, creating labels for an aesthetic is kind of the point of this post. I do not have to bother about the specific color names. For me, this is something that could take up a lot of time if I want to change the appearance of the plot later on because I might spend way too much time on finding colors that \u0026ldquo;work\u0026rdquo; together. Here, if I want to change the colors, I could simply use a Brewer color palette and hope that the creators of that palette had good reasons to arrange the palette the way they did.  1 2 3 4 5 6  ggplot(data = mpg, aes(hwy, cty)) + geom_jitter(alpha = 0.5) + geom_smooth(aes(col = \u0026#39;auto\u0026#39;), se = F, size = 1.5) + geom_smooth(method = \u0026#39;lm\u0026#39;, aes(col = \u0026#39;lm\u0026#39;), se = F, size = 1.5) + labs(col = \u0026#39;Smoothing\u0026#39;) + scale_color_brewer(palette = \u0026#39;Set1\u0026#39;)   Automatically Assigning Labels via Pivoting Sometimes, manually coloring aspects of your data can also be a bad idea. Especially, if you find yourself using the exact same geom_* multiple times on different variables of a data set, you may want to think about using a different approach. One such approach can be to rearrange the data first. For example, take a look at the following two time series that were simulated and collected in a tibble as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  set.seed(123) x1 \u0026lt;- rnorm(10) x2 \u0026lt;- rnorm(10) tib \u0026lt;- tibble( t = seq_along(x1), ts1 = cumsum(x1), ts2 = cumsum(x2) ) tib ## # A tibble: 10 x 3 ## t ts1 ts2 ## \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 -0.560 1.22 ## 2 2 -0.791 1.58 ## 3 3 0.768 1.98 ## 4 4 0.839 2.10 ## 5 5 0.968 1.54 ## 6 6 2.68 3.33 ## 7 7 3.14 3.82 ## 8 8 1.88 1.86 ## 9 9 1.19 2.56 ## 10 10 0.746 2.09   Now, it is possible to plot both times series using geom_line() and use different colors for each line. To do so, one might be tempted (as I often was when I first learned ggplot2) to write code similar to the one we wrote earlier:\n1 2 3 4  tib %\u0026gt;% ggplot(aes(x = t)) + geom_line(aes(y = ts1, col = \u0026#34;Time series 1\u0026#34;)) + geom_line(aes(y = ts2, col = \u0026#34;Time series 2\u0026#34;))   Here, we basically used geom_line() twice for more or less the same plot but with only one aesthetic slightly changed. However, this may not be the best approach. This is especially true if we were to do this for, say, 100 time series as it would involve a lot of code duplication.\nInstead, let\u0026rsquo;s try to rearrange the data via pivot_longer() before even beginning to plot anything1. This way, we might even plot way more than 2 time series with only a single geom_line():\n1 2 3 4 5 6 7 8 9 10 11  set.seed(123) # Create multiple time series tib \u0026lt;- map_dfc(1:6, ~cumsum(rnorm(10))) %\u0026gt;% rename_with(~glue::glue(\u0026#34;label{1:6}\u0026#34;)) %\u0026gt;% bind_cols(t = 1:10, .) # Pivot and plot tib %\u0026gt;% pivot_longer(cols = -1, names_to = \u0026#34;ts\u0026#34;) %\u0026gt;% ggplot(aes(t, value, col = ts)) + geom_line()   As you just saw, it is also possible to, if necessary, relabel the column names in bulk before rearranging the data in order to label the aesthetic the way we want.\nSame Procedure, Different Aesthetic For the sake of an additional example, let us use the same ideas but with geom_boxplot() instead of geom_line(). Therefore, we will generate a couple of \u0026ldquo;data sets\u0026rdquo; and plot a box plot for each one:\n1 2 3 4 5 6  set.seed(123) map_dfc(1:6, rnorm, n = 100) %\u0026gt;% rename_with(~(1:6)) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026#34;ds\u0026#34;) %\u0026gt;% ggplot(aes(col = ds, y = value)) + geom_boxplot()   Here, I have used col again but as I have recently come to realize, using fill instead of col creates the \u0026ldquo;prettier\u0026rdquo; box plots so let\u0026rsquo;s use that instead.\n1 2 3 4 5 6  set.seed(123) map_dfc(1:6, rnorm, n = 100) %\u0026gt;% rename_with(~(1:6)) %\u0026gt;% pivot_longer(cols = everything(), names_to = \u0026#34;ds\u0026#34;) %\u0026gt;% ggplot(aes(fill = ds, y = value)) + geom_boxplot()   So, as you just witnessed, what I have described so far does not only work with the color aesthetic. In fact, we can pretty much use the same approaches for all other aesthetics.\nThus, we have seen how to easily create labels for an aesthetic of our choice by either manually assigning labels or rearranging the data first in order to use the previous column names to assign labels automatically. Let me know what you think in the comments or if you liked this post, simply hit the applause button below.\n I am not describing how pivot_longer() works in detail here because I want to keep this post short by only \u0026ldquo;connecting the dots\u0026rdquo;. If you are unfamiliar with pivoting, you may check out the tidy data chapter from my YARDS lecture notes which was of course inspired by the infamous R for Data Science book. For an animation that demonstrates what pivot_longer() and pivot_wider() do, see gadenbuie/tidyexplain on GitHub.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"We talk about how to easily create labels for an aesthetic.","id":6,"section":"post","tags":["visualization"],"title":"ggplot tips: Assigning Labels to an Aesthetic","uri":"https://albert-rapp.de/post/2021-08-19-aesthetic-labels/"},{"content":"It is not that long ago when I first encountered ggplot2 and decided to learn how to use it1. By no means do I think that I have sufficiently mastered this package yet but as time has passed I have certainly picked up a few tips on my journey to get better at creating more meaningful visualizations. So, in order to remind myself of and share what I learned, I decided to create a sort of series containing tips that enhanced my visualization skills.\nHowever, this is not supposed to be an intro to ggplot2 though. I have already done that and you can find it in the data exploration chapter of my \u0026ldquo;Yet Again: R + Data Science\u0026rdquo; lecture notes (see YARDS). Currently, I plan to make each installment of the series somewhat short to keep it simple and all further posts in this series will be collected under the ggplot2-tips series tag which you can also access from this blog\u0026rsquo;s main page. So, without further ado, let us begin.\nUsing log-transforms Often, one finds variables in a data set that resemble heavy-tailed distributions and you can detect it by a simple histogram in a lot of cases. For instance, take a look at the variable sale_price of the ames dataset from the modeldata package. This variable contains the sale price of 2930 properties in Ames, Iowa and its histogram looks like this:\n1 2 3 4 5 6 7 8 9  library(tidyverse) library(modeldata) data(ames) # I like to clean names s.t. no capital letters are used in the variable names ames \u0026lt;- ames %\u0026gt;% janitor::clean_names() ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram()   As you can see, the distribution looks skewed in the sense that most of the sale prices fall within one range but there are also sale prices that are comparatively high. In effect, the histogram depicts a \u0026ldquo;long tail\u0026rdquo; and the highly priced sales are not that easily discernible in the histogram as the column heights may become really small and there may be large \u0026ldquo;gaps\u0026rdquo; between columns as seen above.\nOne common way to deal with this is to apply a logarithm to the variable of interest. It does not really matter which logarithm you use but since we like to work in a decimal system, a logarithm with base 10 is often used. Let\u0026rsquo;s see how this changes the picture.\n1 2 3  ames %\u0026gt;% ggplot(aes(x = log(sale_price, 10))) + geom_histogram()   Admittedly, we have a gap in the histogram on the left hand side now but overall the histogram looks way less skewed. In fact, this somewhat resembles what a histogram of a normally distributed random variable could look like. This is nice because Gaussian variables are something which a lot of statistical techniques can work with best.\nThus, working with a logarithmized variable might be helpful in the long run. Note that sometimes a variable benefits from being logarithmized but also contains values that are zero. To apply the logarithm anyway, often one then offsets the variable by shifting the variable by 1.\nUnfortunately, it may be nice that logarithmized variables are beneficial for statistical techniques and that the histograms are less skewed but the way we achieved that in the above example let\u0026rsquo;s the audience of the visualization somewhat clueless as to what the actual sale prices were. Sure, if in doubt, one could simply use a calculator to compute \\(10^{4}\\) and \\(10^{6}\\) to get a feeling for the range of the sale prices but of course no one will want to do that. This brings me to my next point.\nUse scale_*_log10() Honestly, I don\u0026rsquo;t know why but for a long time I have logarithmized data for visualization purposes as above because using scale_x_log10() felt somewhat frightening because I did not understand what was going on there. Take a look what happens if I add this particular layer to our initial plot instead of logarithmizing manually.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10()   Notice that the overall impression of the picture is the same as with the manually logarithmized plot. However, the x-axis is now logarithmized as opposed to being linear. So, manual logarithmization of the variable leads to just that: A transformation of the data but the axis in the plot remains linear which is why the labels on that x-axis showed values that needed to be retransformed to its original values.\nIn contrast, using scale_x_log10() leaves the data as it is but transforms the x-axis. In this case, this new axis is used for binning and counting to compute the histogram. Therefore, we can easily see that the majority of the sale prices lie between 100,000 and 300,000. Of course, things would be even simpler if the axis labels were not given in scientific notation. Luckily, we can easily change that.\nAdjust labels using the scales package As its name says, the scales package works really well in conjunction with the scale_* layers from ggplot2. In fact, this can make it somewhat comfortable to quickly adjust axis labels by simply passing a function (mind the ()) from the scales package to the scale_* layer\u0026rsquo;s argument labels. Here, we may simply use label_number() to get rid of the scientific notation.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_number())   Even better, scales has a lot of functions that are useful for specific units such as dollar or week, month, year (in case you are working with time data whose labels can be a special kind of pain).\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_dollar())   Of course, the same thing works not only for the x-axis scale but for all kinds of other scales too. For instance, if you want to plot the same histogram but oriented vertically, you can simply change the x-aesthetic to be the y-aesthetic which means that you will need to adjust the y scale then.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(y = sale_price)) + geom_histogram() + scale_y_log10(labels = scales::label_dollar())   In retrospect, it is really easy to adjust the axis with scale_* layers and the scales package and I really do not know why I hesitated in the past to use these functions. I guess adding another layer to the plot felt somewhat harder and slower than brute-forcing my way through the transformation. But believe me, in the long run this takes up way more of your time (especially if you want to interpret your plot later on).\nI hope that you enjoyed this post and if you did, feel free to hit the applause button below. In any case, I look forward to see you in the next installment of this series.\n Fun fact: Actually I somehow disliked R at first (to be fair I was not a frequent user of R back then anyway) but ggplot2 changed that and motivated me to do more in R.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"This is the beginning of a series about a few ggplot2 tricks I picked up along the way. In this first installment we talk about how logarithmizing scales can be beneficial.","id":7,"section":"post","tags":["visualization"],"title":"Beginning a ggplot2 Series: Logarithmize Your Scales","uri":"https://albert-rapp.de/post/2021-08-07-a-few-ggplot-tips/"},{"content":"Recently, I decided to try out a few stretches in an effort to stay in shape during long stretches of working from home and not leaving the house. Also, to distract me from my inflexible body I thought I would watch a video on YouTube simultaneously and as luck would have it, I saw an interesting video on Veritasium\u0026rsquo;s YouTube Channel called \u0026ldquo;Is Success Luck or Hard Work\u0026rdquo;. Personally, I agree with a lot of things being said in that video and I recommend that you check out the video if you want to get a perspective on the role of luck compared to skill (or keep on reading for my own take on this topic).\nBut more importantly, in the video Derek Muller - the guy behind Veritasium - describes a simulation he ran in order to hint at whether luck played a role in the selection of 11 out of 18300 applicants in 2017 for the NASA astronaut training program. The underlying model that is simulated in the video is described as assuming that astronauts are selected mostly based on their skill. However, 5% of the decision is also based on luck.\nThis sounds a little bit vague, so Muller elaborates: First, each applicant is assigned a random skill score (out of 100) and a luck score (out of 100). Then, the weighted sum of the two scores result in an applicant\u0026rsquo;s overall score (the weights being of course 95% for skill and 5% for luck). Finally, the top 11 applicants according to this overall score will then go on to become astronauts.\nNow, based on a thousand runs of this simulation what Veritasium finds is that it was the very lucky applicants who were selected. More precisely, the average luck score of the picked applicants was 94.7. Similarly, on average out of the 11 picked astronauts only 1.6 applicants would have been the same had the selection process been based on skill alone.\nSo, as I was watching this video, I noticed two things. One, I am really inflexible and I need to stretch more and two, this simulation sounds pretty cool and I bet I could recreate this simulation quite easily. Thus, an idea for a new blog post was born.\nThe Original Approach Later on, I want to tweak the above approach a bit but for now let us simulate the process as described above. First, we will need a function to generate the applicants' scores. Here, I want to generate the luck score according to a uniform distribution on \\((0, 100)\\) because I assume that we are all lucky in a similar way regardless of what the position we apply for is.\nHowever, I think it is conceivable that for highly specialized jobs (such as astronauts) only really skilled applicants show up whereas jobs that fall more in the \u0026ldquo;jack of all trades\u0026rdquo; category may attract applicants from all kinds of skill ranges. This will, of course, affect the distribution of skill scores and I wonder if this has a significant effect on the overall results. Therefore, I will make sure that the score generating function has the ability to use different skill distributions. Finally, let us add an option to change the skill-luck ration which we will first set to its default value of 5%.1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  library(tidyverse) simulate_applicants \u0026lt;- function(n, dist, luck_ratio = 0.05) { tibble( skill = dist(n), luck = runif(n, min = 0, max = 100), overall = (1 - luck_ratio) * skill + luck_ratio * luck ) } set.seed(123) simulate_applicants(5, function(x) rnorm(x, 50, 1)) ## # A tibble: 5 x 3 ## skill luck overall ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 49.4 95.7 51.8 ## 2 49.8 45.3 49.5 ## 3 51.6 67.8 52.4 ## 4 50.1 57.3 50.4 ## 5 50.1 10.3 48.1   Obviously, each column represents the respective score for each applicant. Regarding the skill scores I propose a couple of different distributions:\n Uniform distribution: We assume that applicants come from all kinds of skill ranges and all skill levels are equally likely. Normal distribution: We assume that most people fall within a medium skill range which we model by a normal distribution with mean 50 and standard deviation \\(50/4\\) so that for our 18000 astronauts chances are very slim that one of them falls outside the range (due to the empirical rule of the normal distribution). High specialization: To cover the scenario of only highly skilled applicants, let us use a beta distribution \\(X \\sim \\text{Beta}(1.2, 10)\\) and use the transformed variable \\(100(1-X)\\) as skill distribution.2 Low Specialization: Similarly, let us use \\(100X\\) where \\(X \\sim \\text{Beta}(1.2, 10)\\) to simulate a scenario in which mostly applicants with a low skill score occur.  The corresponding functions that realize the distributions for us are given as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  normal_skills \u0026lt;- function(n) { rnorm(n, mean = 50, sd = 50 / 4) %\u0026gt;% # Make sure that score stays in bounds pmax(0) %\u0026gt;% pmin(100) } uniform_skills \u0026lt;- function(n) { runif(n, min = 0, max = 100) } high_skills \u0026lt;- function(n) { 100 * (1 - rbeta(n, 1.2, 10)) } low_skills \u0026lt;- function(n) { 100 * rbeta(n, 1.2, 10) }   To illustrate the high and low skill distribution, let us take a look at the densities of the beta distributions in question.\nNow, let us write a function that runs a single iteration of the selection process and marks applicants as either selected or not. We will denote the number of applicants to be picked via m. In our astronaut example it holds that \\(m = 11\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  pick_applicants \u0026lt;- function(n, dist, m = 11) { applicants \u0026lt;- simulate_applicants(n, dist) applicants %\u0026gt;% arrange(desc(overall)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, n - m))) } n \u0026lt;- 18300 pick_applicants(n, normal_skills) ## # A tibble: 18,300 x 4 ## skill luck overall selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  ## 1 98.1 11.3 93.8 yes  ## 2 97.4 5.56 92.8 yes  ## 3 93.1 72.9 92.1 yes  ## 4 93.2 60.5 91.6 yes  ## 5 90.5 94.4 90.7 yes  ## 6 94.3 15.4 90.4 yes  ## 7 89.1 88.6 89.1 yes  ## 8 89.4 76.1 88.7 yes  ## 9 87.7 83.8 87.5 yes  ## 10 88.0 75.2 87.3 yes  ## # ... with 18,290 more rows   Using this function, we can easily simulate the whole application process a couple of times Monte Carlo style.\n1 2 3 4 5 6 7 8 9  set.seed(123) N \u0026lt;- 1000 simus \u0026lt;- expand_grid( dist = c(\u0026#34;uniform_skills\u0026#34;, \u0026#34;normal_skills\u0026#34;, \u0026#34;low_skills\u0026#34;, \u0026#34;high_skills\u0026#34;), simuID = 1:N ) %\u0026gt;% mutate(applicants = map(dist, ~pick_applicants(n, get(.)))) %\u0026gt;% unnest(applicants) %\u0026gt;% filter(selected == \u0026#39;yes\u0026#39;)   Having created a tibble simus that contains information on the skill and luck scores of success candidates for each simulation run, we can visualize the distribution of the luck scores of the successful candidates using boxplots. The idea behind that is that if luck is important, then the boxplots should differ from that of a standard uniform distribution on \\((0, 100)\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  library(grid) library(gridExtra) library(gtable) avgs \u0026lt;- simus %\u0026gt;% group_by(dist) %\u0026gt;% summarize(avg_luck = mean(luck)) %\u0026gt;% arrange(desc(avg_luck)) %\u0026gt;% mutate( avg_luck = scales::comma(avg_luck, accuracy = .01), dist = case_when( dist == \u0026#34;high_skills\u0026#34; ~ \u0026#34;High skills\u0026#34;, dist == \u0026#34;low_skills\u0026#34; ~ \u0026#34;Low skills\u0026#34;, dist == \u0026#34;uniform_skills\u0026#34; ~ \u0026#34;Uniform skills\u0026#34;, dist == \u0026#34;normal_skills\u0026#34; ~ \u0026#34;Normal skills\u0026#34; ) ) colorScale \u0026lt;- glue::glue(\u0026#34;dodgerblue{1:4}\u0026#34;) avgTable \u0026lt;- tableGrob( avgs, rows = NULL, cols = NULL, theme = ttheme_minimal( core=list(fg_params = list(col = colorScale, fontface = 2)) ) ) simus %\u0026gt;% mutate(dist = fct_reorder(dist, luck)) %\u0026gt;% ggplot(aes(y = dist, x = luck, col = dist)) + geom_boxplot(show.legend = F) + theme_classic() + labs( x = \u0026#34;Luck\u0026#34;, y = element_blank(), title = \u0026#34;Luck Distribution Among Successful Applicants\u0026#34;, subtitle = \u0026#34;(Average luck score depicted in the table)\u0026#34; ) + annotation_custom(avgTable, xmin = 20, xmax = 60, ymin = 2, ymax = 5) + scale_y_discrete(breaks = NULL) + scale_color_manual(values = rev(colorScale)) + theme(axis.line.y = element_blank(), plot.subtitle = element_text(size = 10))   Here, we added a table of the average luck score of the successful applicants to the boxplots that summarize the distribution of their luck scores. As it turns out, we can clearly see differences between the luck distribution of the successful candidates across the different skill distributions. Interestingly, the uniform skill distribution comes quite close to the average value Veritasium finds in his video, so I guess we can assume that he probably used that skill distribution.\nI would say that the key takeaway of this picture is that the more specialized your area of expertise is, the luckier you have to be if you have many similarly skilled competitors. In a way, this makes sense. If you and your competition is basically at the top of the game and there is not much room to differentiate candidates w.r.t. skill, then luck may just be the deciding factor.\nInterestingly, the same holds true when skills are uniformly distributed among the whole range. Further, when the skill distribution of you and your competition looks more normally distributed or if the skill scores of all applicants are rather low, then you do not need to be extremely lucky (at least not as much as before). But still, you have to be more lucky than the average applicant (recall that luck is modeled via a uniform distribution here with mean 50).\nSo, in a sense this might mean that if chances are good that there are a couple of applicants who are as good as it gets, i.e. at the maximum of the skill range (which is the case for the high and uniform skills), then the successful candidates are indeed really lucky. Similarly, if chances are low that some applicants are the best of the best (normal and low skills), then successful applicants are luckier than the average Joe but in a less extreme way than in the previous example. In total, it looks like luck plays a role in either case.\nFinally, to break this scenario down to something more realistic with a higher available spots to applicants ratio I ran the same simulation but with only one available position and 100 applications.3 The results look similar but as expected if their are less applicants for one position, then luck plays a lesser but still important role.\nOvercoming Threshold Approach There is this old joke where a recruiter in an HR department gets a large stack of applications for a specific position within the company. The recruiter immediately begins to work on the applications by simply taking half of the applications and throwing them into the trash because he \u0026ldquo;does not hire unlucky people\u0026rdquo;.\nThis may be an extreme action but then again I have heard that some companies immediately reject an application if they see only a single typo in the documents. To me, this is similar to what the recruiter from the joke is doing because a typo can happen by accident to even the most careful person regardless of their skill. So, let us use this as a way to construct another hypothetical scenario.\nIn this scenario, an application goes through two stages. In the first stage, the application is either is moved to the second stage or is rejected due to some arbitrary small reason. By this we mean some small event based on luck which we will model here with our luck score from above. In the second stage, everything is judged purely on skill alone, i.e. the person with the highest skill score gets the job.\nLet us write a new function for this second hypothetical scenario. Notice that this new function ranks applicants according to their skill score, i.e. the most skilled applicant is ranked as 1, the second most skilled applicant is ranked as 2 and so on.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  pick_applicants2 \u0026lt;- function(n, dist, m = 11, luck_thresh = 50) { applicants \u0026lt;- simulate_applicants(n, dist) applicants \u0026lt;- applicants %\u0026gt;% mutate(skill_rank = min_rank(desc(skill))) %\u0026gt;% filter(luck \u0026gt; luck_thresh) applicants %\u0026gt;% arrange(desc(skill)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, nrow(applicants) - m))) } pick_applicants2(n, high_skills) ## # A tibble: 49 x 5 ## skill luck overall skill_rank selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  ## 1 99.9 87.4 99.3 1 yes  ## 2 99.1 54.6 96.9 3 yes  ## 3 98.8 54.5 96.6 5 yes  ## 4 98.8 97.5 98.7 6 yes  ## 5 98.2 90.6 97.8 10 yes  ## 6 97.6 60.8 95.8 12 yes  ## 7 97.0 87.0 96.5 13 yes  ## 8 96.8 71.9 95.5 14 yes  ## 9 96.7 97.0 96.8 16 yes  ## 10 96.6 55.1 94.5 18 yes  ## # ... with 39 more rows   Afterwards, we can run a similar simulation as before. However, this time we will take a look at the distribution of the ranks of the successful candidates. Here are the results for our astronaut scenario, i.e. 18300 applicants with 11 open positions.\nAs you can see, the distribution of the skill ranks of the successful applicants do not vary much across the skill distributions. Also, it looks like the average ranks are simply the amount of positions plus 1. This is somewhat unsurprising since we throw out 50% of the applicants randomly in the first stage of the process.\nNevertheless, this reinforces the idea that not necessarily the most skilled person gets hired to do the job. What is even more surprising to me is the fact that in the current scenario there is still a chance of around 8% that a successful applicant has a skill rank above 22 (recall that there are 11 open positions). Now, running the same analysis for 100 applicants for 1 job again yields similar results.\nIn these 1000 simulations, the chances that a successful applicant has a skill score of 5 or above are similarly high but depend on the skill distribution.\n## # A tibble: 4 x 3\r## # Groups: dist [4]\r## dist n prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;\r## 1 Normal skills 82 8.2% ## 2 Uniform skills 69 6.9% ## 3 High skills 65 6.5% ## 4 Low skills 58 5.8%\rSummary So, in our simulations we have seen that luck plays a role in whether an applicant gets a job. Further, this actually depended on the skill distribution of the applicants and the impact of luck was most strongly pronounced in the cases when there was a good chance that highly skilled people are present among the applicants. Probably, this is the most relevant scenario anyway since we all like to believe that we are skilled but are still somewhat more proficient than other skilled people.\nOf course, in terms of real world implications, this simulation can only give anecdotal evidence towards the hypothesis that luck plays a large role in success. Also, we made a lot of assumptions in our simulations that might be debatable. But, personally, I like to believe that luck cannot be neglected in the end and even the most skilled person may want to be grateful when he gets accepted for a job. If you wish to share your opinion on that, feel free to leave a comment. Finally, if you thought this post was interesting, I would also appreciate it if you clicked the applause button (just so that it feels less like I am talking into a void).\n When I first started to write this blog post, I wanted to investigate how results change for different ratios but this post is already long enough. Thus, I decided to simply leave that functionality unused but someone interested in recreating parts of this simulation may tweak that parameter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Here, the parameters of the beta distribution were picked so that the densities somewhat fit the desired description as will be seen in the plot a few lines ahead. Of course, one could potentially change the parameters but here I decided to go with this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I tried to research what is a realistic number of applicants for a given positions but results were varying and I found claims of less than 100 applicants on average and way more than 100 applicants. So, here I decided to go with something in between.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recreate a simulation study on the influence of luck on success compared to the influence of skill.","id":8,"section":"post","tags":["simulation"],"title":"Is Success Luck or Hard Work?","uri":"https://albert-rapp.de/post/2021-07-26-luck-vs-skill/"},{"content":"For my first post on this blog I decided to create an animation using the animation package. To give this animation some purpose let me demonstrate how kernel density estimators work with the help of an animation.\nIn general, kernel density estimators are, as is kind of obvious by the name, used to estimate the underlying density of a random sample. For instance, imagine that we have a sample drawn from an some unknown distribution.\n1 2  n \u0026lt;- 100 sample \u0026lt;- rexp(n)   Then, assuming that we do not actually know that the current sample was drawn from an exponential distribution, we might want to estimate the density and see if the estimate fits to some well-known parametric family of distributions. With the help of ggplot() and geom_density() this is straightforward.\n1 2 3  library(tidyverse) ggplot() + geom_density(aes(x = sample))   The underlying procedure to generate the plot it to use a kernel density estimator \\(\\hat{f}_h\\) in order to estimate the true underlying density \\(f\\) by using the formula $$ \\hat{f}_h(x) = \\frac{1}{nh} \\sum_{k = 1}^{n} K\\Big(\\frac{x - x_k}{h}\\Big) $$ for all \\(x \\in \\mathbb{R}\\) where \\(n\\) is the sample lengh and \\(h \u0026gt; 0\\) is a smoothing parameter that needs to be chosen and \\(K\\) is a \u0026ldquo;suitable\u0026rdquo; function. Usually, this parameter \\(h\\) is called bandwidth and \\(K\\) is called a kernel function which is often to be the density of a probability distribution.\nThe Bandwidth In geom_density(), the default kernel function is the Gaussian density and the bandwidth can be tweaked through the bw argument.\n1 2 3 4 5 6 7 8 9  h \u0026lt;- 1 ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) )   Of course, I could now create multiple plots and change the value if h each time to demonstrate the effect of the bandwidth but the point of this post was to create an animation. So let\u0026rsquo;s do that instead.\nNevertheless, to create an animation, we need to be able to create multiple plots. Therefore, let us use the previous code and wrap a function depending on h around that. This function will be our plot generator depending on the bandwidth.\n1 2 3 4 5 6 7 8 9 10 11 12  plot_gen \u0026lt;- function(h) { g \u0026lt;- ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) ) print(g) # For the animation we need this to be printed }   Now that we have that, define a function that creates all the plots we want to see in our animation, i.e. we create the animation frame by frame. This function can then be passed to saveGIF() from the animation package which then renders the animation for us. Creating a gif in R is as simple as that.\n1 2 3 4 5 6 7  all_the_plots \u0026lt;- function() { map(seq(0.05, 0.5, 0.05), plot_gen) } library(animation) saveGIF(all_the_plots()) ## Output at: animation.gif ## [1] TRUE   As you can see, the bandwidth really is a smoothing parameter. Of course, too much smoothing may not yield great results so the parameter needs to be chosen with care but let us not worry about this in this blog post.\nThe Actual Estimation Procedure Let us create another animation to visualize how kernel density estimation works on a more basic level, i.e. . In order to do so, notice that if the kernel \\(K\\) is a continuous density function of a random variable \\(X\\) (e.g. a Gaussian random variable), then the density function of the random variable \\((X + x_0)h\\) where \\(x_0 \\in \\mathbb{R}\\) and \\(h \u0026gt; 0\\) is given by \\(K((X - x_0)/h)/h.\\)\nConsequently, in the case of standard Gaussian random variables \\(X\\), the kernel density estimator is nothing but the average of the densities of \\(n\\) Gaussian random variables with individual means \\(x_k\\), \\(k = 1, \\ldots, n,\\) and common standard deviation \\(h.\\)\nTherefore, for a given sample you can run a kernel density estimation by taking the following steps:\n Check where each data point \\(x_k\\) is located on the x-axis For each data point \\(x_k\\) draw a Gaussian density \\(f_k\\) with standard deviation \\(h\\) and mean \\(x_k\\) For each \\(x \\in \\mathbb{R}\\) check what are the values of \\(f_k\\), \\(k = 1, \\ldots, n,\\) at \\(x\\) and average these.  So to create a visualization of the kernel density estimation principle, we simple create a function that plots each of those steps for us. Finally, we execute all of these functions and send them to saveGIF().\nWe begin by computing the data we need to create the plots later on, i.e. we simulate a sample and compute the values of the densities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  compute_density \u0026lt;- function(x_0, h, K = dnorm) { xx \u0026lt;- seq(-6, 6, 0.001) * h tibble( x = xx, density = K((x - x_0) / h) / h ) } set.seed(123) # For the sake of demonstration we use a  # small uniformly distributed sample here x_sample \u0026lt;- runif(5, -5, 5) h \u0026lt;- 1 tib \u0026lt;- tibble( k = seq_along(x_sample), density = map(x_sample, compute_density, h = h) ) %\u0026gt;% unnest(density)   Then, it becomes time for our first step, i.e. check where the sample values are located.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  draw_axis \u0026lt;- function(x_sample, tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) p \u0026lt;- ggplot(data = NULL, aes(x = x_sample)) + theme_minimal() + theme( axis.line.x = element_line(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank()) p } p \u0026lt;- draw_axis(x_sample, tib) p   Once we have that, we draw the kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  draw_kernel \u0026lt;- function(p, tib) { p \u0026lt;- p + geom_line(data = tib, aes(x, density, group = k), size = 1) + geom_segment( aes( x = x_sample, xend = x_sample, y = 0, yend = dnorm(0) ), linetype = 2 ) + labs(y = element_blank()) p } draw_kernel(p, tib)   Next, average the densities at an arbitrary position \\(x_0.\\) If we can do that, then we can iterate through different values of \\(x_0.\\)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  plot_until_x0 \u0026lt;- function(tib, x0) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% filter(x \u0026lt;= x0) %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- ggplot() + geom_line( data = tib, aes(x, density, group = k), alpha = 0.5, size = 1 ) + geom_point( data = filter(tib, x == x0), aes(x, density), #col = anim_col, alpha = 0.75, size = 3 ) + geom_vline(xintercept = x0, col = anim_col, alpha = 0.5) + geom_point( data = slice_tail(tib_x0, n = 1), aes(x, est), col = anim_col, size = 3 ) + geom_line( data = tib_x0, aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } x0 \u0026lt;- (0) plot_until_x0(tib, x0)   Last but not least, we may want to display the estimated density without the underlying kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  final_plot \u0026lt;- function(tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- tib_x0 %\u0026gt;% ggplot() + geom_line( aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } final_plot(tib)   Finally, we have all the ingredients to create the animation by collecting all of these functions in a wrapper function and using it in conjunction with saveGIF().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  gif \u0026lt;- function(x_sample, tib) { p \u0026lt;- draw_axis(x_sample, tib) map(1:3, ~print(p)) p \u0026lt;- draw_kernel(p, tib) map(1:5, ~print(p)) map(seq(min(tib$x), max(tib$x), 0.5), ~plot_until_x0(tib, .)) map(1:15, ~final_plot(tib)) } saveGIF(gif(x_sample, tib), interval = 0.4, # animation speed ani.width = 720, ani.height = 405, movie.name = \u0026#34;kernelAnimation.gif\u0026#34;)   Thus, we have created a short animation that illustrates the kernel density estimation procedure. Probably, there is some room for improving the animation by fine tuning the plots, tweaking with the animation speed or the number of frames. For now, though, let us leave everything as it is.\nBut feel free to let me know what could be improved in the comments. Similarly, if you want to leave any other form of feedback, feel free to roam the comment section too. Finally, if you enjoyed what you have seen here but do not want to bother writing a comment, you may simply hit the applause button instead.\n","description":"For my first post I create an animation using the animate package.","id":9,"section":"post","tags":["statistics","visualization"],"title":"Animating kernel density estimators","uri":"https://albert-rapp.de/post/visualize-kernel-density-estimation/"},{"content":"Welcome! My Name is Albert Rapp and I am currently a PhD student in mathematics at Ulm University. As part of my obligations as a PhD student I had to teach an applied course on statistical techniques using the statistical software R. Interestingly, I noticed that I actually enjoy using R which is why I went the extra mile and decided to write a set of lecture notes on my own and made them accessible online. In fact, I realized that the process of writing felt (mostly) fun and my retention rate of the stuff I had to learn in order to write about it was actually quite high.\nConsequently, I decided to make a habit out of writing about things I encounter in statistics and/or R. So, what you see here in this blog is the result of that decision. In the end, this is nothing but a personal project and is as much about helping me retain stuff as I learn new things as it is about making a contribution to the R community along the lines of many other blogs and tutorials that helped me learn R. Hopefully, what can be found here will be of benefit to someone other than me.\nCurrently, I aim at posting a new blog post on a biweekly schedule and regarding the content, I will write about whatever I find most interesting at the time of writing. On a long term basis though, I want to cover parts of my R/statistics bucket list which I described in the final chapter of my YARDS lecture notes. If you want to be notified when there is a new post online, you may want to check out the RSS feed which you can access here or via the RSS symbol at the bottom of the \u0026ldquo;Posts\u0026rdquo; Page.\nThis blog is build with blogdown and Hugo and all blog posts are released under a CC-BY-NC 4.0 license.\n","description":"About Me","id":10,"section":"","tags":null,"title":"About","uri":"https://albert-rapp.de/about/"}]