[{"content":"It is not that long ago when I first encountered ggplot2 and decided to learn how to use it1. By no means do I think that I have sufficiently mastered this package yet but as time has passed I have certainly picked up a few tips on my journey to get better at creating more meaningful visualizations. So, in order to remind myself of and share what I learned, I decided to create a sort of series containing tips that enhanced my visualization skills.\nHowever, this is not supposed to be an intro to ggplot2 though. I have already done that and you can find it in the data exploration chapter of my \u0026ldquo;Yet Again: R + Data Science\u0026rdquo; lecture notes (see YARDS). Currently, I plan to make each installment of the series somewhat short to keep it simple and all further posts in this series will be collected under the ggplot2-tips series tag which you can also access from this blog\u0026rsquo;s main page. So, without further ado, let us begin.\nUsing log-transforms Often, one finds variables in a data set that resemble heavy-tailed distributions and you can detect it by a simple histogram in a lot of cases. For instance, take a look at the variable sale_price of the ames dataset from the modeldata package. This variable contains the sale price of 2930 properties in Ames, Iowa and its histogram looks like this:\n1 2 3 4 5 6 7 8 9  library(tidyverse) library(modeldata) data(ames) # I like to clean names s.t. no capital letters are used in the variable names ames \u0026lt;- ames %\u0026gt;% janitor::clean_names() ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram()   As you can see, the distribution looks skewed in the sense that most of the sale prices fall within one range but there are also sale prices that are comparatively high. In effect, the histogram depicts a \u0026ldquo;long tail\u0026rdquo; and the highly priced sales are not that easily discernible in the histogram as the column heights may become really small and there may be large \u0026ldquo;gaps\u0026rdquo; between columns as seen above.\nOne common way to deal with this is to apply a logarithm to the variable of interest. It does not really matter which logarithm you use but since we like to work in a decimal system, a logarithm with base 10 is often used. Let\u0026rsquo;s see how this changes the picture.\n1 2 3  ames %\u0026gt;% ggplot(aes(x = log(sale_price, 10))) + geom_histogram()   Admittedly, we have a gap in the histogram on the left hand side now but overall the histogram looks way less skewed. In fact, this somewhat resembles what a histogram of a normally distributed random variable could look like. This is nice because Gaussian variables are something which a lot of statistical techniques can work with best.\nThus, working with a logarithmized variable might be helpful in the long run. Note that sometimes a variable benefits from being logarithmized but also contains values that are zero. To apply the logarithm anyway, often one then offsets the variable by shifting the variable by 1.\nUnfortunately, it may be nice that logarithmized variables are beneficial for statistical techniques and that the histograms are less skewed but the way we achieved that in the above example let\u0026rsquo;s the audience of the visualization somewhat clueless as to what the actual sale prices were. Sure, if in doubt, one could simply use a calculator to compute \\(10^{4}\\) and \\(10^{6}\\) to get a feeling for the range of the sale prices but of course no one will want to do that. This brings me to my next point.\nUse scale_*_log10() Honestly, I don\u0026rsquo;t know why but for a long time I have logarithmized data for visualization purposes as above because using scale_x_log10() felt somewhat frightening because I did not understand what was going on there. Take a look what happens if I add this particular layer to our initial plot instead of logarithmizing manually.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10()   Notice that the overall impression of the picture is the same as with the manually logarithmized plot. However, the x-axis is now logarithmized as opposed to being linear. So, manual logarithmization of the variable leads to just that: A transformation of the data but the axis in the plot remains linear which is why the labels on that x-axis showed values that needed to be retransformed to its original values.\nIn contrast, using scale_x_log10() leaves the data as it is but transforms the x-axis. In this case, this new axis is used for binning and counting to compute the histogram. Therefore, we can easily see that the majority of the sale prices lie between 100,000 and 300,000. Of course, things would be even simpler if the axis labels were not given in scientific notation. Luckily, we can easily change that.\nAdjust labels using the scales package As its name says, the scales package works really well in conjunction with the scale_* layers from ggplot2. In fact, this can make it somewhat comfortable to quickly adjust axis labels by simply passing a function (mind the ()) from the scales package to the scale_* layer\u0026rsquo;s argument labels. Here, we may simply use label_number() to get rid of the scientific notation.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_number())   Even better, scales has a lot of functions that are useful for specific units such as dollar or week, month, year (in case you are working with time data whose labels can be a special kind of pain).\n1 2 3 4  ames %\u0026gt;% ggplot(aes(x = sale_price)) + geom_histogram() + scale_x_log10(labels = scales::label_dollar())   Of course, the same thing works not only for the x-axis scale but for all kinds of other scales too. For instance, if you want to plot the same histogram but oriented vertically, you can simply change the x-aesthetic to be the y-aesthetic which means that you will need to adjust the y scale then.\n1 2 3 4  ames %\u0026gt;% ggplot(aes(y = sale_price)) + geom_histogram() + scale_y_log10(labels = scales::label_dollar())   In retrospect, it is really easy to adjust the axis with scale_* layers and the scales package and I really do not know why I hesitated in the past to use these functions. I guess adding another layer to the plot felt somewhat harder and slower than brute-forcing my way through the transformation. But believe me, in the long run this takes up way more of your time (especially if you want to interpret your plot later on).\nI hope that you enjoyed this post and if you did, feel free to hit the applause button below. In any case, I look forward to see you in the next installment of this series.\n Fun fact: Actually I somehow disliked R at first (to be fair I was not a frequent user of R back then anyway) but ggplot2 changed that and motivated me to do more in R.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"This is the beginning of a series about a few ggplot2 tricks I picked up along the way. In this first installment we talk about how logarithmizing scales can be beneficial.","id":0,"section":"post","tags":["visualization"],"title":"Beginning a ggplot2 Series: Logarithmize Your Scales","uri":"https://albert-rapp.de/post/2021-08-07-a-few-ggplot-tips/"},{"content":"Recently, I decided to try out a few stretches in an effort to stay in shape during long stretches of working from home and not leaving the house. Also, to distract me from my inflexible body I thought I would watch a video on YouTube simultaneously and as luck would have it, I saw an interesting video on Veritasium\u0026rsquo;s YouTube Channel called \u0026ldquo;Is Success Luck or Hard Work\u0026rdquo;. Personally, I agree with a lot of things being said in that video and I recommend that you check out the video if you want to get a perspective on the role of luck compared to skill (or keep on reading for my own take on this topic).\nBut more importantly, in the video Derek Muller - the guy behind Veritasium - describes a simulation he ran in order to hint at whether luck played a role in the selection of 11 out of 18300 applicants in 2017 for the NASA astronaut training program. The underlying model that is simulated in the video is described as assuming that astronauts are selected mostly based on their skill. However, 5% of the decision is also based on luck.\nThis sounds a little bit vague, so Muller elaborates: First, each applicant is assigned a random skill score (out of 100) and a luck score (out of 100). Then, the weighted sum of the two scores result in an applicant\u0026rsquo;s overall score (the weights being of course 95% for skill and 5% for luck). Finally, the top 11 applicants according to this overall score will then go on to become astronauts.\nNow, based on a thousand runs of this simulation what Veritasium finds is that it was the very lucky applicants who were selected. More precisely, the average luck score of the picked applicants was 94.7. Similarly, on average out of the 11 picked astronauts only 1.6 applicants would have been the same had the selection process been based on skill alone.\nSo, as I was watching this video, I noticed two things. One, I am really inflexible and I need to stretch more and two, this simulation sounds pretty cool and I bet I could recreate this simulation quite easily. Thus, an idea for a new blog post was born.\nThe Original Approach Later on, I want to tweak the above approach a bit but for now let us simulate the process as described above. First, we will need a function to generate the applicants' scores. Here, I want to generate the luck score according to a uniform distribution on \\((0, 100)\\) because I assume that we are all lucky in a similar way regardless of what the position we apply for is.\nHowever, I think it is conceivable that for highly specialized jobs (such as astronauts) only really skilled applicants show up whereas jobs that fall more in the \u0026ldquo;jack of all trades\u0026rdquo; category may attract applicants from all kinds of skill ranges. This will, of course, affect the distribution of skill scores and I wonder if this has a significant effect on the overall results. Therefore, I will make sure that the score generating function has the ability to use different skill distributions. Finally, let us add an option to change the skill-luck ration which we will first set to its default value of 5%.1\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  library(tidyverse) simulate_applicants \u0026lt;- function(n, dist, luck_ratio = 0.05) { tibble( skill = dist(n), luck = runif(n, min = 0, max = 100), overall = (1 - luck_ratio) * skill + luck_ratio * luck ) } set.seed(123) simulate_applicants(5, function(x) rnorm(x, 50, 1)) ## # A tibble: 5 x 3 ## skill luck overall ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 49.4 95.7 51.8 ## 2 49.8 45.3 49.5 ## 3 51.6 67.8 52.4 ## 4 50.1 57.3 50.4 ## 5 50.1 10.3 48.1   Obviously, each column represents the respective score for each applicant. Regarding the skill scores I propose a couple of different distributions:\n Uniform distribution: We assume that applicants come from all kinds of skill ranges and all skill levels are equally likely. Normal distribution: We assume that most people fall within a medium skill range which we model by a normal distribution with mean 50 and standard deviation \\(50/4\\) so that for our 18000 astronauts chances are very slim that one of them falls outside the range (due to the empirical rule of the normal distribution). High specialization: To cover the scenario of only highly skilled applicants, let us use a beta distribution \\(X \\sim \\text{Beta}(1.2, 10)\\) and use the transformed variable \\(100(1-X)\\) as skill distribution.2 Low Specialization: Similarly, let us use \\(100X\\) where \\(X \\sim \\text{Beta}(1.2, 10)\\) to simulate a scenario in which mostly applicants with a low skill score occur.  The corresponding functions that realize the distributions for us are given as follows:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  normal_skills \u0026lt;- function(n) { rnorm(n, mean = 50, sd = 50 / 4) %\u0026gt;% # Make sure that score stays in bounds pmax(0) %\u0026gt;% pmin(100) } uniform_skills \u0026lt;- function(n) { runif(n, min = 0, max = 100) } high_skills \u0026lt;- function(n) { 100 * (1 - rbeta(n, 1.2, 10)) } low_skills \u0026lt;- function(n) { 100 * rbeta(n, 1.2, 10) }   To illustrate the high and low skill distribution, let us take a look at the densities of the beta distributions in question.\nNow, let us write a function that runs a single iteration of the selection process and marks applicants as either selected or not. We will denote the number of applicants to be picked via m. In our astronaut example it holds that \\(m = 11\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  pick_applicants \u0026lt;- function(n, dist, m = 11) { applicants \u0026lt;- simulate_applicants(n, dist) applicants %\u0026gt;% arrange(desc(overall)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, n - m))) } n \u0026lt;- 18300 pick_applicants(n, normal_skills) ## # A tibble: 18,300 x 4 ## skill luck overall selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;  ## 1 98.1 11.3 93.8 yes  ## 2 97.4 5.56 92.8 yes  ## 3 93.1 72.9 92.1 yes  ## 4 93.2 60.5 91.6 yes  ## 5 90.5 94.4 90.7 yes  ## 6 94.3 15.4 90.4 yes  ## 7 89.1 88.6 89.1 yes  ## 8 89.4 76.1 88.7 yes  ## 9 87.7 83.8 87.5 yes  ## 10 88.0 75.2 87.3 yes  ## # ... with 18,290 more rows   Using this function, we can easily simulate the whole application process a couple of times Monte Carlo style.\n1 2 3 4 5 6 7 8 9  set.seed(123) N \u0026lt;- 1000 simus \u0026lt;- expand_grid( dist = c(\u0026#34;uniform_skills\u0026#34;, \u0026#34;normal_skills\u0026#34;, \u0026#34;low_skills\u0026#34;, \u0026#34;high_skills\u0026#34;), simuID = 1:N ) %\u0026gt;% mutate(applicants = map(dist, ~pick_applicants(n, get(.)))) %\u0026gt;% unnest(applicants) %\u0026gt;% filter(selected == \u0026#39;yes\u0026#39;)   Having created a tibble simus that contains information on the skill and luck scores of success candidates for each simulation run, we can visualize the distribution of the luck scores of the successful candidates using boxplots. The idea behind that is that if luck is important, then the boxplots should differ from that of a standard uniform distribution on \\((0, 100)\\).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44  library(grid) library(gridExtra) library(gtable) avgs \u0026lt;- simus %\u0026gt;% group_by(dist) %\u0026gt;% summarize(avg_luck = mean(luck)) %\u0026gt;% arrange(desc(avg_luck)) %\u0026gt;% mutate( avg_luck = scales::comma(avg_luck, accuracy = .01), dist = case_when( dist == \u0026#34;high_skills\u0026#34; ~ \u0026#34;High skills\u0026#34;, dist == \u0026#34;low_skills\u0026#34; ~ \u0026#34;Low skills\u0026#34;, dist == \u0026#34;uniform_skills\u0026#34; ~ \u0026#34;Uniform skills\u0026#34;, dist == \u0026#34;normal_skills\u0026#34; ~ \u0026#34;Normal skills\u0026#34; ) ) colorScale \u0026lt;- glue::glue(\u0026#34;dodgerblue{1:4}\u0026#34;) avgTable \u0026lt;- tableGrob( avgs, rows = NULL, cols = NULL, theme = ttheme_minimal( core=list(fg_params = list(col = colorScale, fontface = 2)) ) ) simus %\u0026gt;% mutate(dist = fct_reorder(dist, luck)) %\u0026gt;% ggplot(aes(y = dist, x = luck, col = dist)) + geom_boxplot(show.legend = F) + theme_classic() + labs( x = \u0026#34;Luck\u0026#34;, y = element_blank(), title = \u0026#34;Luck Distribution Among Successful Applicants\u0026#34;, subtitle = \u0026#34;(Average luck score depicted in the table)\u0026#34; ) + annotation_custom(avgTable, xmin = 20, xmax = 60, ymin = 2, ymax = 5) + scale_y_discrete(breaks = NULL) + scale_color_manual(values = rev(colorScale)) + theme(axis.line.y = element_blank(), plot.subtitle = element_text(size = 10))   Here, we added a table of the average luck score of the successful applicants to the boxplots that summarize the distribution of their luck scores. As it turns out, we can clearly see differences between the luck distribution of the successful candidates across the different skill distributions. Interestingly, the uniform skill distribution comes quite close to the average value Veritasium finds in his video, so I guess we can assume that he probably used that skill distribution.\nI would say that the key takeaway of this picture is that the more specialized your area of expertise is, the luckier you have to be if you have many similarly skilled competitors. In a way, this makes sense. If you and your competition is basically at the top of the game and there is not much room to differentiate candidates w.r.t. skill, then luck may just be the deciding factor.\nInterestingly, the same holds true when skills are uniformly distributed among the whole range. Further, when the skill distribution of you and your competition looks more normally distributed or if the skill scores of all applicants are rather low, then you do not need to be extremely lucky (at least not as much as before). But still, you have to be more lucky than the average applicant (recall that luck is modeled via a uniform distribution here with mean 50).\nSo, in a sense this might mean that if chances are good that there are a couple of applicants who are as good as it gets, i.e. at the maximum of the skill range (which is the case for the high and uniform skills), then the successful candidates are indeed really lucky. Similarly, if chances are low that some applicants are the best of the best (normal and low skills), then successful applicants are luckier than the average Joe but in a less extreme way than in the previous example. In total, it looks like luck plays a role in either case.\nFinally, to break this scenario down to something more realistic with a higher available spots to applicants ratio I ran the same simulation but with only one available position and 100 applications.3 The results look similar but as expected if their are less applicants for one position, then luck plays a lesser but still important role.\nOvercoming Threshold Approach There is this old joke where a recruiter in an HR department gets a large stack of applications for a specific position within the company. The recruiter immediately begins to work on the applications by simply taking half of the applications and throwing them into the trash because he \u0026ldquo;does not hire unlucky people\u0026rdquo;.\nThis may be an extreme action but then again I have heard that some companies immediately reject an application if they see only a single typo in the documents. To me, this is similar to what the recruiter from the joke is doing because a typo can happen by accident to even the most careful person regardless of their skill. So, let us use this as a way to construct another hypothetical scenario.\nIn this scenario, an application goes through two stages. In the first stage, the application is either is moved to the second stage or is rejected due to some arbitrary small reason. By this we mean some small event based on luck which we will model here with our luck score from above. In the second stage, everything is judged purely on skill alone, i.e. the person with the highest skill score gets the job.\nLet us write a new function for this second hypothetical scenario. Notice that this new function ranks applicants according to their skill score, i.e. the most skilled applicant is ranked as 1, the second most skilled applicant is ranked as 2 and so on.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  pick_applicants2 \u0026lt;- function(n, dist, m = 11, luck_thresh = 50) { applicants \u0026lt;- simulate_applicants(n, dist) applicants \u0026lt;- applicants %\u0026gt;% mutate(skill_rank = min_rank(desc(skill))) %\u0026gt;% filter(luck \u0026gt; luck_thresh) applicants %\u0026gt;% arrange(desc(skill)) %\u0026gt;% mutate(selected = c(rep(\u0026#39;yes\u0026#39;, m), rep(\u0026#39;no\u0026#39;, nrow(applicants) - m))) } pick_applicants2(n, high_skills) ## # A tibble: 49 x 5 ## skill luck overall skill_rank selected ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;  ## 1 99.9 87.4 99.3 1 yes  ## 2 99.1 54.6 96.9 3 yes  ## 3 98.8 54.5 96.6 5 yes  ## 4 98.8 97.5 98.7 6 yes  ## 5 98.2 90.6 97.8 10 yes  ## 6 97.6 60.8 95.8 12 yes  ## 7 97.0 87.0 96.5 13 yes  ## 8 96.8 71.9 95.5 14 yes  ## 9 96.7 97.0 96.8 16 yes  ## 10 96.6 55.1 94.5 18 yes  ## # ... with 39 more rows   Afterwards, we can run a similar simulation as before. However, this time we will take a look at the distribution of the ranks of the successful candidates. Here are the results for our astronaut scenario, i.e. 18300 applicants with 11 open positions.\nAs you can see, the distribution of the skill ranks of the successful applicants do not vary much across the skill distributions. Also, it looks like the average ranks are simply the amount of positions plus 1. This is somewhat unsurprising since we throw out 50% of the applicants randomly in the first stage of the process.\nNevertheless, this reinforces the idea that not necessarily the most skilled person gets hired to do the job. What is even more surprising to me is the fact that in the current scenario there is still a chance of around 8% that a successful applicant has a skill rank above 22 (recall that there are 11 open positions). Now, running the same analysis for 100 applicants for 1 job again yields similar results.\nIn these 1000 simulations, the chances that a successful applicant has a skill score of 5 or above are similarly high but depend on the skill distribution.\n## # A tibble: 4 x 3\r## # Groups: dist [4]\r## dist n prop ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt;\r## 1 Normal skills 82 8.2% ## 2 Uniform skills 69 6.9% ## 3 High skills 65 6.5% ## 4 Low skills 58 5.8%\rSummary So, in our simulations we have seen that luck plays a role in whether an applicant gets a job. Further, this actually depended on the skill distribution of the applicants and the impact of luck was most strongly pronounced in the cases when there was a good chance that highly skilled people are present among the applicants. Probably, this is the most relevant scenario anyway since we all like to believe that we are skilled but are still somewhat more proficient than other skilled people.\nOf course, in terms of real world implications, this simulation can only give anecdotal evidence towards the hypothesis that luck plays a large role in success. Also, we made a lot of assumptions in our simulations that might be debatable. But, personally, I like to believe that luck cannot be neglected in the end and even the most skilled person may want to be grateful when he gets accepted for a job. If you wish to share your opinion on that, feel free to leave a comment. Finally, if you thought this post was interesting, I would also appreciate it if you clicked the applause button (just so that it feels less like I am talking into a void).\n When I first started to write this blog post, I wanted to investigate how results change for different ratios but this post is already long enough. Thus, I decided to simply leave that functionality unused but someone interested in recreating parts of this simulation may tweak that parameter.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Here, the parameters of the beta distribution were picked so that the densities somewhat fit the desired description as will be seen in the plot a few lines ahead. Of course, one could potentially change the parameters but here I decided to go with this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n I tried to research what is a realistic number of applicants for a given positions but results were varying and I found claims of less than 100 applicants on average and way more than 100 applicants. So, here I decided to go with something in between.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","description":"I recreate a simulation study on the influence of luck on success compared to the influence of skill.","id":1,"section":"post","tags":["simulation"],"title":"Is Success Luck or Hard Work?","uri":"https://albert-rapp.de/post/2021-07-26-luck-vs-skill/"},{"content":"For my first post on this blog I decided to create an animation using the animation package. To give this animation some purpose let me demonstrate how kernel density estimators work with the help of an animation.\nIn general, kernel density estimators are, as is kind of obvious by the name, used to estimate the underlying density of a random sample. For instance, imagine that we have a sample drawn from an some unknown distribution.\n1 2  n \u0026lt;- 100 sample \u0026lt;- rexp(n)   Then, assuming that we do not actually know that the current sample was drawn from an exponential distribution, we might want to estimate the density and see if the estimate fits to some well-known parametric family of distributions. With the help of ggplot() and geom_density() this is straightforward.\n1 2 3  library(tidyverse) ggplot() + geom_density(aes(x = sample))   The underlying procedure to generate the plot it to use a kernel density estimator \\(\\hat{f}_h\\) in order to estimate the true underlying density \\(f\\) by using the formula $$ \\hat{f}_h(x) = \\frac{1}{nh} \\sum_{k = 1}^{n} K\\Big(\\frac{x - x_k}{h}\\Big) $$ for all \\(x \\in \\mathbb{R}\\) where \\(n\\) is the sample lengh and \\(h \u0026gt; 0\\) is a smoothing parameter that needs to be chosen and \\(K\\) is a \u0026ldquo;suitable\u0026rdquo; function. Usually, this parameter \\(h\\) is called bandwidth and \\(K\\) is called a kernel function which is often to be the density of a probability distribution.\nThe Bandwidth In geom_density(), the default kernel function is the Gaussian density and the bandwidth can be tweaked through the bw argument.\n1 2 3 4 5 6 7 8 9  h \u0026lt;- 1 ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) )   Of course, I could now create multiple plots and change the value if h each time to demonstrate the effect of the bandwidth but the point of this post was to create an animation. So let\u0026rsquo;s do that instead.\nNevertheless, to create an animation, we need to be able to create multiple plots. Therefore, let us use the previous code and wrap a function depending on h around that. This function will be our plot generator depending on the bandwidth.\n1 2 3 4 5 6 7 8 9 10 11 12  plot_gen \u0026lt;- function(h) { g \u0026lt;- ggplot() + geom_density(aes(x = sample), bw = h) + annotate( \u0026#34;label\u0026#34;, x = 0.5, y = 0.1, label = glue::glue(\u0026#34;bw = {h}\u0026#34;) ) print(g) # For the animation we need this to be printed }   Now that we have that, define a function that creates all the plots we want to see in our animation, i.e. we create the animation frame by frame. This function can then be passed to saveGIF() from the animation package which then renders the animation for us. Creating a gif in R is as simple as that.\n1 2 3 4 5 6 7  all_the_plots \u0026lt;- function() { map(seq(0.05, 0.5, 0.05), plot_gen) } library(animation) saveGIF(all_the_plots()) ## Output at: animation.gif ## [1] TRUE   As you can see, the bandwidth really is a smoothing parameter. Of course, too much smoothing may not yield great results so the parameter needs to be chosen with care but let us not worry about this in this blog post.\nThe Actual Estimation Procedure Let us create another animation to visualize how kernel density estimation works on a more basic level, i.e. . In order to do so, notice that if the kernel \\(K\\) is a continuous density function of a random variable \\(X\\) (e.g. a Gaussian random variable), then the density function of the random variable \\((X + x_0)h\\) where \\(x_0 \\in \\mathbb{R}\\) and \\(h \u0026gt; 0\\) is given by \\(K((X - x_0)/h)/h.\\)\nConsequently, in the case of standard Gaussian random variables \\(X\\), the kernel density estimator is nothing but the average of the densities of \\(n\\) Gaussian random variables with individual means \\(x_k\\), \\(k = 1, \\ldots, n,\\) and common standard deviation \\(h.\\)\nTherefore, for a given sample you can run a kernel density estimation by taking the following steps:\n Check where each data point \\(x_k\\) is located on the x-axis For each data point \\(x_k\\) draw a Gaussian density \\(f_k\\) with standard deviation \\(h\\) and mean \\(x_k\\) For each \\(x \\in \\mathbb{R}\\) check what are the values of \\(f_k\\), \\(k = 1, \\ldots, n,\\) at \\(x\\) and average these.  So to create a visualization of the kernel density estimation principle, we simple create a function that plots each of those steps for us. Finally, we execute all of these functions and send them to saveGIF().\nWe begin by computing the data we need to create the plots later on, i.e. we simulate a sample and compute the values of the densities.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  compute_density \u0026lt;- function(x_0, h, K = dnorm) { xx \u0026lt;- seq(-6, 6, 0.001) * h tibble( x = xx, density = K((x - x_0) / h) / h ) } set.seed(123) # For the sake of demonstration we use a  # small uniformly distributed sample here x_sample \u0026lt;- runif(5, -5, 5) h \u0026lt;- 1 tib \u0026lt;- tibble( k = seq_along(x_sample), density = map(x_sample, compute_density, h = h) ) %\u0026gt;% unnest(density)   Then, it becomes time for our first step, i.e. check where the sample values are located.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  draw_axis \u0026lt;- function(x_sample, tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) p \u0026lt;- ggplot(data = NULL, aes(x = x_sample)) + theme_minimal() + theme( axis.line.x = element_line(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank()) p } p \u0026lt;- draw_axis(x_sample, tib) p   Once we have that, we draw the kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  draw_kernel \u0026lt;- function(p, tib) { p \u0026lt;- p + geom_line(data = tib, aes(x, density, group = k), size = 1) + geom_segment( aes( x = x_sample, xend = x_sample, y = 0, yend = dnorm(0) ), linetype = 2 ) + labs(y = element_blank()) p } draw_kernel(p, tib)   Next, average the densities at an arbitrary position \\(x_0.\\) If we can do that, then we can iterate through different values of \\(x_0.\\)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65  plot_until_x0 \u0026lt;- function(tib, x0) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% filter(x \u0026lt;= x0) %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- ggplot() + geom_line( data = tib, aes(x, density, group = k), alpha = 0.5, size = 1 ) + geom_point( data = filter(tib, x == x0), aes(x, density), #col = anim_col, alpha = 0.75, size = 3 ) + geom_vline(xintercept = x0, col = anim_col, alpha = 0.5) + geom_point( data = slice_tail(tib_x0, n = 1), aes(x, est), col = anim_col, size = 3 ) + geom_line( data = tib_x0, aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } x0 \u0026lt;- (0) plot_until_x0(tib, x0)   Last but not least, we may want to display the estimated density without the underlying kernels.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  final_plot \u0026lt;- function(tib) { labs \u0026lt;- glue::glue(\u0026#34;$x_{seq_along(x_sample)}$\u0026#34;) labs \u0026lt;- latex2exp::TeX(labs) tib_x0 \u0026lt;- tib %\u0026gt;% group_by(x) %\u0026gt;% summarise(est = mean(density), .groups = \u0026#34;drop\u0026#34;) anim_col \u0026lt;- \u0026#39;firebrick3\u0026#39; g \u0026lt;- tib_x0 %\u0026gt;% ggplot() + geom_line( aes(x, est), col = anim_col, size = 1 ) + theme_classic() + theme( axis.line.x = element_line(), axis.line.y = element_blank(), panel.grid = element_blank(), axis.ticks = element_line(size = 1), axis.text = element_text(size = 14) ) + scale_x_continuous( limits = c(min(tib$x), max(tib$x)), breaks = sort(x_sample), minor_breaks = NULL, labels = labs ) + scale_y_continuous( breaks = NULL, limits = c(0, max(tib$density) + 0.025) ) + labs(x = element_blank(), y = element_blank()) print(g) } final_plot(tib)   Finally, we have all the ingredients to create the animation by collecting all of these functions in a wrapper function and using it in conjunction with saveGIF().\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  gif \u0026lt;- function(x_sample, tib) { p \u0026lt;- draw_axis(x_sample, tib) map(1:3, ~print(p)) p \u0026lt;- draw_kernel(p, tib) map(1:5, ~print(p)) map(seq(min(tib$x), max(tib$x), 0.5), ~plot_until_x0(tib, .)) map(1:15, ~final_plot(tib)) } saveGIF(gif(x_sample, tib), interval = 0.4, # animation speed ani.width = 720, ani.height = 405, movie.name = \u0026#34;kernelAnimation.gif\u0026#34;)   Thus, we have created a short animation that illustrates the kernel density estimation procedure. Probably, there is some room for improving the animation by fine tuning the plots, tweaking with the animation speed or the number of frames. For now, though, let us leave everything as it is.\nBut feel free to let me know what could be improved in the comments. Similarly, if you want to leave any other form of feedback, feel free to roam the comment section too. Finally, if you enjoyed what you have seen here but do not want to bother writing a comment, you may simply hit the applause button instead.\n","description":"For my first post I create an animation using the animate package.","id":2,"section":"post","tags":["statistics","visualization"],"title":"Animating kernel density estimators","uri":"https://albert-rapp.de/post/visualize-kernel-density-estimation/"},{"content":"Welcome! My Name is Albert Rapp and I am currently a PhD student in mathematics at Ulm University. As part of my obligations as a PhD student I had to teach an applied course on statistical techniques using the statistical software R. Interestingly, I noticed that I actually enjoy using R which is why I went the extra mile and decided to write a set of lecture notes on my own and made them accessible online. In fact, I realized that the process of writing felt (mostly) fun and my retention rate of the stuff I had to learn in order to write about it was actually quite high.\nConsequently, I decided to make a habit out of writing about things I encounter in statistics and/or R. So, what you see here in this blog is the result of that decision. In the end, this is nothing but a personal project and is as much about helping me retain stuff as I learn new things as it is about making a contribution to the R community along the lines of many other blogs and tutorials that helped me learn R. Hopefully, what can be found here will be of benefit to someone other than me.\nCurrently, I aim at posting a new blog post on a biweekly schedule and regarding the content, I will write about whatever I find most interesting at the time of writing. On a long term basis though, I want to cover parts of my R/statistics bucket list which I described in the final chapter of my YARDS lecture notes. If you want to be notified when there is a new post online, you may want to check out the RSS feed which you can access here or via the RSS symbol at the bottom of the \u0026ldquo;Posts\u0026rdquo; Page.\nThis blog is build with blogdown and Hugo and all blog posts are released under a CC-BY-NC 4.0 license.\n","description":"About Me","id":3,"section":"","tags":null,"title":"About","uri":"https://albert-rapp.de/about/"}]